{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"LociSimiles","text":"<p>A Python package for finding intertextual links in Latin literature using pre-trained language models.</p> <p>LociSimiles helps researchers discover textual similarities and allusions between Latin texts using modern NLP techniques.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Semantic Retrieval: Find similar passages using sentence embeddings</li> <li>Classification Pipeline: Classify text pairs using transformer models</li> <li>Two-Stage Pipeline: Combine retrieval and classification for optimal results</li> <li>Evaluation Tools: Comprehensive metrics for assessing intertextual detection</li> <li>Command-Line Interface: Easy-to-use CLI for common workflows</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<pre><code>pip install locisimiles\n</code></pre> <p>Or install with development dependencies:</p> <pre><code>pip install \"locisimiles[dev]\"\n</code></pre>"},{"location":"#basic-usage","title":"Basic Usage","text":"<pre><code>from locisimiles import Document, RetrievalPipeline\n\n# Load your documents\nsource = Document.from_csv(\"source_texts.csv\")\ntarget = Document.from_csv(\"target_texts.csv\")\n\n# Create retrieval pipeline\npipeline = RetrievalPipeline()\n\n# Find similar passages\nresults = pipeline.retrieve(source, target, top_k=5)\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>Getting Started - Installation and first steps</li> <li>CLI Reference - Command-line interface documentation</li> <li>API Reference - Complete API documentation</li> <li>Development - Contributing and development setup</li> </ul>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License.</p>"},{"location":"cli/","title":"CLI Reference","text":"<p>LociSimiles provides a command-line interface for common workflows.</p>"},{"location":"cli/#installation","title":"Installation","text":"<p>The CLI is installed automatically with the package:</p> <pre><code>pip install locisimiles\n</code></pre>"},{"location":"cli/#commands","title":"Commands","text":""},{"location":"cli/#locisimiles-run","title":"<code>locisimiles run</code>","text":"<p>Run the intertextual detection pipeline on source and target documents.</p> <pre><code>locisimiles run SOURCE TARGET [OPTIONS]\n</code></pre>"},{"location":"cli/#arguments","title":"Arguments","text":"Argument Description <code>SOURCE</code> Path to the source CSV file <code>TARGET</code> Path to the target CSV file"},{"location":"cli/#options","title":"Options","text":"Option Default Description <code>--output</code>, <code>-o</code> <code>results.csv</code> Output file path <code>--model</code>, <code>-m</code> <code>sentence-transformers/all-MiniLM-L6-v2</code> Model name or path <code>--top-k</code>, <code>-k</code> <code>10</code> Number of candidates to retrieve <code>--threshold</code>, <code>-t</code> <code>0.5</code> Classification threshold <code>--batch-size</code>, <code>-b</code> <code>32</code> Batch size for processing"},{"location":"cli/#examples","title":"Examples","text":"<p>Basic usage:</p> <pre><code>locisimiles run source.csv target.csv\n</code></pre> <p>With custom output and model:</p> <pre><code>locisimiles run source.csv target.csv \\\n    --output results.csv \\\n    --model bert-base-multilingual-cased \\\n    --top-k 20\n</code></pre>"},{"location":"cli/#locisimiles-evaluate","title":"<code>locisimiles evaluate</code>","text":"<p>Evaluate detection results against ground truth.</p> <pre><code>locisimiles evaluate PREDICTIONS GROUND_TRUTH [OPTIONS]\n</code></pre>"},{"location":"cli/#arguments_1","title":"Arguments","text":"Argument Description <code>PREDICTIONS</code> Path to predictions CSV file <code>GROUND_TRUTH</code> Path to ground truth CSV file"},{"location":"cli/#options_1","title":"Options","text":"Option Default Description <code>--output</code>, <code>-o</code> <code>None</code> Output file for metrics (prints to stdout if not specified)"},{"location":"cli/#examples_1","title":"Examples","text":"<pre><code>locisimiles evaluate results.csv ground_truth.csv\n</code></pre> <p>Save metrics to file:</p> <pre><code>locisimiles evaluate results.csv ground_truth.csv -o metrics.json\n</code></pre>"},{"location":"cli/#input-file-formats","title":"Input File Formats","text":""},{"location":"cli/#sourcetarget-csv","title":"Source/Target CSV","text":"<p>CSV files should contain at minimum an ID column and a text column:</p> <pre><code>id,text\n1,\"Arma virumque cano Troiae qui primus ab oris\"\n2,\"Italiam fato profugus Laviniaque venit\"\n</code></pre>"},{"location":"cli/#ground-truth-csv","title":"Ground Truth CSV","text":"<p>Ground truth files should contain query-reference pairs with labels:</p> <pre><code>query_id,reference_id,label\n1,42,1\n2,15,0\n</code></pre> <p>Where <code>label</code> is <code>1</code> for true matches and <code>0</code> for non-matches.</p>"},{"location":"cli/#output-format","title":"Output Format","text":"<p>The pipeline outputs a CSV with the following columns:</p> Column Description <code>query_id</code> ID of the source text segment <code>reference_id</code> ID of the matched target segment <code>score</code> Similarity/classification score <code>above_threshold</code> Whether the score exceeds the threshold"},{"location":"cli/#environment-variables","title":"Environment Variables","text":"Variable Description <code>LOCISIMILES_CACHE_DIR</code> Directory for model caching <code>LOCISIMILES_DEVICE</code> Device for computation (<code>cpu</code>, <code>cuda</code>, <code>mps</code>)"},{"location":"development/","title":"Development Guide","text":"<p>This guide covers setting up a development environment and contributing to LociSimiles.</p>"},{"location":"development/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10 or higher</li> <li>Git</li> <li>pip package manager</li> </ul>"},{"location":"development/#setup","title":"Setup","text":""},{"location":"development/#clone-the-repository","title":"Clone the Repository","text":"<pre><code>git clone https://github.com/julian-schelb/locisimiles.git\ncd locisimiles\n</code></pre>"},{"location":"development/#create-virtual-environment","title":"Create Virtual Environment","text":"<pre><code>python -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n</code></pre>"},{"location":"development/#install-development-dependencies","title":"Install Development Dependencies","text":"<pre><code>pip install -e \".[dev]\"\n</code></pre> <p>This installs the package in editable mode along with development tools:</p> <ul> <li>pytest - Testing framework</li> <li>pytest-cov - Coverage reporting</li> <li>poethepoet - Task runner</li> <li>mkdocs - Documentation</li> <li>mkdocs-material - Documentation theme</li> </ul>"},{"location":"development/#running-tests","title":"Running Tests","text":""},{"location":"development/#using-poe-recommended","title":"Using Poe (Recommended)","text":"<pre><code># Run all tests\npoe test\n\n# Run tests with coverage report\npoe test-cov\n</code></pre>"},{"location":"development/#using-pytest-directly","title":"Using Pytest Directly","text":"<pre><code># Run all tests\npytest\n\n# Run with verbose output\npytest -v\n\n# Run specific test file\npytest tests/test_document.py\n\n# Run specific test\npytest tests/test_document.py::TestDocument::test_from_csv\n\n# Run with coverage\npytest --cov=locisimiles --cov-report=term-missing\n\n# Generate HTML coverage report\npytest --cov=locisimiles --cov-report=html\n</code></pre>"},{"location":"development/#documentation","title":"Documentation","text":""},{"location":"development/#serve-locally","title":"Serve Locally","text":"<pre><code>poe docs\n</code></pre> <p>This starts a local server at <code>http://127.0.0.1:8000</code> with live reload.</p>"},{"location":"development/#build-static-site","title":"Build Static Site","text":"<pre><code>poe docs-build\n</code></pre> <p>Output is written to the <code>site/</code> directory.</p>"},{"location":"development/#deploy-to-github-pages","title":"Deploy to GitHub Pages","text":"<pre><code>poe docs-deploy\n</code></pre> <p>This builds and pushes to the <code>gh-pages</code> branch.</p>"},{"location":"development/#project-structure","title":"Project Structure","text":"<pre><code>locisimiles/\n\u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 locisimiles/\n\u2502       \u251c\u2500\u2500 __init__.py          # Package exports\n\u2502       \u251c\u2500\u2500 cli.py               # Command-line interface\n\u2502       \u251c\u2500\u2500 document.py          # Document and TextSegment classes\n\u2502       \u251c\u2500\u2500 evaluator.py         # Evaluation metrics\n\u2502       \u2514\u2500\u2500 pipeline/\n\u2502           \u251c\u2500\u2500 __init__.py      # Pipeline exports\n\u2502           \u251c\u2500\u2500 _types.py        # Type definitions\n\u2502           \u251c\u2500\u2500 classification.py # Classification pipeline\n\u2502           \u251c\u2500\u2500 retrieval.py     # Retrieval pipeline\n\u2502           \u2514\u2500\u2500 two_stage.py     # Combined pipeline\n\u251c\u2500\u2500 tests/\n\u2502   \u251c\u2500\u2500 conftest.py              # Shared test fixtures\n\u2502   \u251c\u2500\u2500 test_document.py         # Document tests\n\u2502   \u251c\u2500\u2500 test_evaluator.py        # Evaluator tests\n\u2502   \u251c\u2500\u2500 test_cli.py              # CLI tests\n\u2502   \u251c\u2500\u2500 test_types.py            # Type definition tests\n\u2502   \u251c\u2500\u2500 test_retrieval.py        # Retrieval pipeline tests\n\u2502   \u251c\u2500\u2500 test_classification.py   # Classification pipeline tests\n\u2502   \u2514\u2500\u2500 test_two_stage.py        # Two-stage pipeline tests\n\u251c\u2500\u2500 docs/                        # Documentation source\n\u251c\u2500\u2500 examples/                    # Example scripts and notebooks\n\u251c\u2500\u2500 pyproject.toml               # Project configuration\n\u2514\u2500\u2500 mkdocs.yml                   # Documentation configuration\n</code></pre>"},{"location":"development/#code-style","title":"Code Style","text":""},{"location":"development/#general-guidelines","title":"General Guidelines","text":"<ul> <li>Follow PEP 8 style guidelines</li> <li>Use type hints for function signatures</li> <li>Write docstrings for public APIs</li> <li>Keep functions focused and small</li> </ul>"},{"location":"development/#example","title":"Example","text":"<pre><code>def compute_similarity(\n    text_a: str,\n    text_b: str,\n    model: Optional[str] = None\n) -&gt; float:\n    \"\"\"Compute semantic similarity between two texts.\n\n    Args:\n        text_a: First text string.\n        text_b: Second text string.\n        model: Optional model name. Defaults to MiniLM.\n\n    Returns:\n        Similarity score between 0 and 1.\n\n    Raises:\n        ValueError: If either text is empty.\n    \"\"\"\n    if not text_a or not text_b:\n        raise ValueError(\"Texts cannot be empty\")\n    ...\n</code></pre>"},{"location":"development/#testing-guidelines","title":"Testing Guidelines","text":""},{"location":"development/#writing-tests","title":"Writing Tests","text":"<ul> <li>Place tests in the <code>tests/</code> directory</li> <li>Name test files <code>test_&lt;module&gt;.py</code></li> <li>Name test classes <code>Test&lt;ClassName&gt;</code></li> <li>Name test methods <code>test_&lt;behavior&gt;</code></li> </ul>"},{"location":"development/#using-fixtures","title":"Using Fixtures","text":"<p>Shared fixtures are defined in <code>conftest.py</code>:</p> <pre><code>def test_document_loading(sample_csv_file):\n    \"\"\"Test loading document from CSV.\"\"\"\n    doc = Document.from_csv(sample_csv_file)\n    assert len(doc) &gt; 0\n</code></pre>"},{"location":"development/#mocking-external-dependencies","title":"Mocking External Dependencies","text":"<p>Use unittest.mock for ML models:</p> <pre><code>from unittest.mock import MagicMock, patch\n\ndef test_retrieval_with_mock(mock_embedder):\n    \"\"\"Test retrieval with mocked embedding model.\"\"\"\n    pipeline = RetrievalPipeline()\n    pipeline.model = mock_embedder\n    # Test without actual model loading\n</code></pre>"},{"location":"development/#contributing","title":"Contributing","text":""},{"location":"development/#workflow","title":"Workflow","text":"<ol> <li>Fork the repository</li> <li>Create a feature branch: <code>git checkout -b feature/my-feature</code></li> <li>Make changes and add tests</li> <li>Ensure tests pass: <code>poe test</code></li> <li>Commit changes: <code>git commit -m \"Add my feature\"</code></li> <li>Push to fork: <code>git push origin feature/my-feature</code></li> <li>Open a Pull Request</li> </ol>"},{"location":"development/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<ul> <li>Include tests for new functionality</li> <li>Update documentation as needed</li> <li>Keep commits focused and atomic</li> <li>Write clear commit messages</li> </ul>"},{"location":"development/#available-poe-tasks","title":"Available Poe Tasks","text":"Task Command Description <code>test</code> <code>poe test</code> Run all tests <code>test-cov</code> <code>poe test-cov</code> Run tests with coverage <code>docs</code> <code>poe docs</code> Serve documentation locally <code>docs-build</code> <code>poe docs-build</code> Build documentation <code>docs-deploy</code> <code>poe docs-deploy</code> Deploy to GitHub Pages"},{"location":"getting-started/","title":"Getting Started","text":"<p>This guide will help you get started with LociSimiles for finding intertextual links in Latin literature.</p>"},{"location":"getting-started/#installation","title":"Installation","text":""},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10 or higher</li> <li>pip package manager</li> </ul>"},{"location":"getting-started/#installing-from-pypi","title":"Installing from PyPI","text":"<pre><code>pip install locisimiles\n</code></pre>"},{"location":"getting-started/#installing-from-source","title":"Installing from Source","text":"<pre><code>git clone https://github.com/julian-schelb/locisimiles.git\ncd locisimiles\npip install -e \".[dev]\"\n</code></pre>"},{"location":"getting-started/#basic-concepts","title":"Basic Concepts","text":""},{"location":"getting-started/#documents-and-segments","title":"Documents and Segments","text":"<p>LociSimiles works with Documents containing TextSegments. Each segment represents a unit of text (e.g., a verse, sentence, or passage).</p> <pre><code>from locisimiles import Document, TextSegment\n\n# Create segments manually\nsegments = [\n    TextSegment(id=\"1\", text=\"Arma virumque cano\"),\n    TextSegment(id=\"2\", text=\"Troiae qui primus ab oris\"),\n]\n\n# Create a document\ndoc = Document(segments=segments)\n</code></pre>"},{"location":"getting-started/#loading-from-csv","title":"Loading from CSV","text":"<p>Documents are typically loaded from CSV files:</p> <pre><code>doc = Document.from_csv(\"texts.csv\")\n</code></pre> <p>The CSV should have columns for <code>id</code> and <code>text</code> (column names are configurable).</p>"},{"location":"getting-started/#pipelines","title":"Pipelines","text":"<p>LociSimiles provides three main pipeline types:</p>"},{"location":"getting-started/#1-retrieval-pipeline","title":"1. Retrieval Pipeline","text":"<p>Uses semantic embeddings to find similar passages:</p> <pre><code>from locisimiles import RetrievalPipeline\n\npipeline = RetrievalPipeline(\n    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n)\n\nresults = pipeline.retrieve(source_doc, target_doc, top_k=10)\n</code></pre>"},{"location":"getting-started/#2-classification-pipeline","title":"2. Classification Pipeline","text":"<p>Uses a transformer model to classify text pairs:</p> <pre><code>from locisimiles import ClassificationPipeline\n\npipeline = ClassificationPipeline(\n    model_name=\"bert-base-uncased\"\n)\n\nresults = pipeline.classify(pairs)\n</code></pre>"},{"location":"getting-started/#3-two-stage-pipeline","title":"3. Two-Stage Pipeline","text":"<p>Combines retrieval and classification for best results:</p> <pre><code>from locisimiles import TwoStagePipeline\n\npipeline = TwoStagePipeline(\n    retrieval_model=\"sentence-transformers/all-MiniLM-L6-v2\",\n    classification_model=\"bert-base-uncased\"\n)\n\nresults = pipeline.run(source_doc, target_doc)\n</code></pre>"},{"location":"getting-started/#evaluation","title":"Evaluation","text":"<p>Use the <code>IntertextEvaluator</code> to assess detection quality:</p> <pre><code>from locisimiles import IntertextEvaluator\n\nevaluator = IntertextEvaluator(\n    predictions=predictions,\n    ground_truth=ground_truth\n)\n\nmetrics = evaluator.evaluate()\nprint(f\"Precision: {metrics['precision']:.3f}\")\nprint(f\"Recall: {metrics['recall']:.3f}\")\nprint(f\"F1: {metrics['f1']:.3f}\")\n</code></pre>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<ul> <li>See the CLI Reference for command-line usage</li> <li>Explore the API Reference for detailed documentation</li> <li>Check out the examples for complete workflows</li> </ul>"},{"location":"api/","title":"API Reference","text":"<p>This section provides detailed documentation for the LociSimiles Python API.</p>"},{"location":"api/#core-modules","title":"Core Modules","text":""},{"location":"api/#document-module","title":"Document Module","text":"<p>The <code>document</code> module provides classes for representing and loading text collections.</p> <ul> <li><code>Document</code> - Container for text segments</li> <li><code>TextSegment</code> - Individual text unit with ID and content</li> </ul>"},{"location":"api/#pipeline-module","title":"Pipeline Module","text":"<p>The <code>pipeline</code> module provides the main processing pipelines.</p> <ul> <li><code>RetrievalPipeline</code> - Semantic similarity retrieval</li> <li><code>ClassificationPipeline</code> - Text pair classification</li> <li><code>TwoStagePipeline</code> - Combined retrieval and classification</li> </ul>"},{"location":"api/#evaluator-module","title":"Evaluator Module","text":"<p>The <code>evaluator</code> module provides tools for assessing detection quality.</p> <ul> <li><code>IntertextEvaluator</code> - Main evaluation class</li> <li>Metric Functions - Precision, recall, F1, etc.</li> </ul>"},{"location":"api/#quick-reference","title":"Quick Reference","text":""},{"location":"api/#loading-documents","title":"Loading Documents","text":"<pre><code>from locisimiles import Document\n\n# From CSV file\ndoc = Document.from_csv(\"texts.csv\")\n\n# From list of dictionaries\ndoc = Document.from_records([\n    {\"id\": \"1\", \"text\": \"First passage\"},\n    {\"id\": \"2\", \"text\": \"Second passage\"},\n])\n</code></pre>"},{"location":"api/#running-pipelines","title":"Running Pipelines","text":"<pre><code>from locisimiles import TwoStagePipeline\n\npipeline = TwoStagePipeline()\nresults = pipeline.run(source, target, top_k=10, threshold=0.5)\n</code></pre>"},{"location":"api/#evaluating-results","title":"Evaluating Results","text":"<pre><code>from locisimiles import IntertextEvaluator\n\nevaluator = IntertextEvaluator(predictions, ground_truth)\nmetrics = evaluator.evaluate()\n</code></pre>"},{"location":"api/#type-definitions","title":"Type Definitions","text":"<p>The <code>pipeline._types</code> module defines common types used throughout the library:</p> <ul> <li><code>TextPair</code> - Tuple of (source_id, target_id, source_text, target_text)</li> <li><code>RetrievalResult</code> - Result from retrieval pipeline</li> <li><code>ClassificationResult</code> - Result from classification pipeline</li> <li><code>PipelineResult</code> - Combined result from two-stage pipeline</li> </ul>"},{"location":"api/document/","title":"Document API","text":"<p>The document module provides classes for representing text collections.</p>"},{"location":"api/document/#textsegment","title":"TextSegment","text":"<p>A <code>TextSegment</code> represents an individual unit of text with an identifier.</p>"},{"location":"api/document/#constructor","title":"Constructor","text":"<pre><code>TextSegment(id: str, text: str, metadata: Optional[Dict[str, Any]] = None)\n</code></pre>"},{"location":"api/document/#parameters","title":"Parameters","text":"Parameter Type Description <code>id</code> <code>str</code> Unique identifier for the segment <code>text</code> <code>str</code> The text content <code>metadata</code> <code>Dict[str, Any]</code> Optional metadata dictionary"},{"location":"api/document/#properties","title":"Properties","text":"Property Type Description <code>id</code> <code>str</code> The segment identifier <code>text</code> <code>str</code> The text content <code>metadata</code> <code>Dict[str, Any]</code> Metadata dictionary (defaults to empty dict)"},{"location":"api/document/#methods","title":"Methods","text":""},{"location":"api/document/#to_dict","title":"<code>to_dict()</code>","text":"<p>Convert the segment to a dictionary.</p> <pre><code>def to_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Returns: Dictionary with <code>id</code>, <code>text</code>, and <code>metadata</code> keys.</p>"},{"location":"api/document/#example","title":"Example","text":"<pre><code>from locisimiles import TextSegment\n\nsegment = TextSegment(\n    id=\"aen.1.1\",\n    text=\"Arma virumque cano\",\n    metadata={\"book\": 1, \"line\": 1}\n)\n\nprint(segment.id)       # \"aen.1.1\"\nprint(segment.text)     # \"Arma virumque cano\"\nprint(segment.metadata) # {\"book\": 1, \"line\": 1}\n</code></pre>"},{"location":"api/document/#document","title":"Document","text":"<p>A <code>Document</code> is a collection of <code>TextSegment</code> objects.</p>"},{"location":"api/document/#constructor_1","title":"Constructor","text":"<pre><code>Document(segments: List[TextSegment])\n</code></pre>"},{"location":"api/document/#parameters_1","title":"Parameters","text":"Parameter Type Description <code>segments</code> <code>List[TextSegment]</code> List of text segments"},{"location":"api/document/#properties_1","title":"Properties","text":"Property Type Description <code>segments</code> <code>List[TextSegment]</code> The list of segments"},{"location":"api/document/#methods_1","title":"Methods","text":""},{"location":"api/document/#from_csv","title":"<code>from_csv()</code>","text":"<p>Load a document from a CSV file.</p> <pre><code>@classmethod\ndef from_csv(\n    path: str,\n    id_column: str = \"id\",\n    text_column: str = \"text\",\n    metadata_columns: Optional[List[str]] = None,\n    delimiter: str = \",\"\n) -&gt; Document\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>path</code> <code>str</code> - Path to the CSV file <code>id_column</code> <code>str</code> <code>\"id\"</code> Name of the ID column <code>text_column</code> <code>str</code> <code>\"text\"</code> Name of the text column <code>metadata_columns</code> <code>List[str]</code> <code>None</code> Additional columns to include as metadata <code>delimiter</code> <code>str</code> <code>\",\"</code> CSV delimiter character <p>Returns: <code>Document</code> instance.</p> <p>Example:</p> <pre><code>doc = Document.from_csv(\n    \"vergil.csv\",\n    id_column=\"verse_id\",\n    text_column=\"latin_text\",\n    metadata_columns=[\"book\", \"line\"]\n)\n</code></pre>"},{"location":"api/document/#from_records","title":"<code>from_records()</code>","text":"<p>Create a document from a list of dictionaries.</p> <pre><code>@classmethod\ndef from_records(\n    records: List[Dict[str, Any]],\n    id_key: str = \"id\",\n    text_key: str = \"text\"\n) -&gt; Document\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>records</code> <code>List[Dict]</code> - List of record dictionaries <code>id_key</code> <code>str</code> <code>\"id\"</code> Key for the ID field <code>text_key</code> <code>str</code> <code>\"text\"</code> Key for the text field <p>Returns: <code>Document</code> instance.</p> <p>Example:</p> <pre><code>records = [\n    {\"id\": \"1\", \"text\": \"First verse\", \"author\": \"Vergil\"},\n    {\"id\": \"2\", \"text\": \"Second verse\", \"author\": \"Vergil\"},\n]\ndoc = Document.from_records(records)\n</code></pre>"},{"location":"api/document/#get_by_id","title":"<code>get_by_id()</code>","text":"<p>Retrieve a segment by its ID.</p> <pre><code>def get_by_id(segment_id: str) -&gt; Optional[TextSegment]\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>segment_id</code> <code>str</code> The segment ID to look up <p>Returns: <code>TextSegment</code> if found, <code>None</code> otherwise.</p>"},{"location":"api/document/#texts","title":"<code>texts()</code>","text":"<p>Get a list of all text content.</p> <pre><code>def texts() -&gt; List[str]\n</code></pre> <p>Returns: List of text strings from all segments.</p>"},{"location":"api/document/#ids","title":"<code>ids()</code>","text":"<p>Get a list of all segment IDs.</p> <pre><code>def ids() -&gt; List[str]\n</code></pre> <p>Returns: List of ID strings from all segments.</p>"},{"location":"api/document/#__len__","title":"<code>__len__()</code>","text":"<p>Get the number of segments.</p> <pre><code>len(document)  # Returns number of segments\n</code></pre>"},{"location":"api/document/#__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate over segments.</p> <pre><code>for segment in document:\n    print(segment.text)\n</code></pre>"},{"location":"api/document/#example_1","title":"Example","text":"<pre><code>from locisimiles import Document, TextSegment\n\n# Create from segments\ndoc = Document([\n    TextSegment(\"1\", \"Arma virumque cano\"),\n    TextSegment(\"2\", \"Troiae qui primus ab oris\"),\n])\n\n# Access properties\nprint(len(doc))          # 2\nprint(doc.texts())       # [\"Arma virumque cano\", \"Troiae qui primus ab oris\"]\nprint(doc.ids())         # [\"1\", \"2\"]\n\n# Lookup by ID\nsegment = doc.get_by_id(\"1\")\nprint(segment.text)      # \"Arma virumque cano\"\n</code></pre>"},{"location":"api/evaluator/","title":"Evaluator API","text":"<p>The evaluator module provides tools for assessing the quality of intertextual detection.</p>"},{"location":"api/evaluator/#intertextevaluator","title":"IntertextEvaluator","text":"<p>The main class for evaluating detection results against ground truth.</p>"},{"location":"api/evaluator/#constructor","title":"Constructor","text":"<pre><code>IntertextEvaluator(\n    predictions: List[Dict[str, Any]],\n    ground_truth: List[Dict[str, Any]],\n    query_id_key: str = \"query_id\",\n    reference_id_key: str = \"reference_id\",\n    label_key: str = \"label\",\n    score_key: str = \"score\"\n)\n</code></pre>"},{"location":"api/evaluator/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>predictions</code> <code>List[Dict]</code> - List of prediction dictionaries <code>ground_truth</code> <code>List[Dict]</code> - List of ground truth dictionaries <code>query_id_key</code> <code>str</code> <code>\"query_id\"</code> Key for query ID in dictionaries <code>reference_id_key</code> <code>str</code> <code>\"reference_id\"</code> Key for reference ID <code>label_key</code> <code>str</code> <code>\"label\"</code> Key for ground truth labels <code>score_key</code> <code>str</code> <code>\"score\"</code> Key for prediction scores"},{"location":"api/evaluator/#methods","title":"Methods","text":""},{"location":"api/evaluator/#evaluate","title":"<code>evaluate()</code>","text":"<p>Compute all evaluation metrics.</p> <pre><code>def evaluate(threshold: float = 0.5) -&gt; Dict[str, float]\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>threshold</code> <code>float</code> <code>0.5</code> Classification threshold <p>Returns: Dictionary with all computed metrics.</p> <p>Example:</p> <pre><code>metrics = evaluator.evaluate(threshold=0.6)\nprint(metrics)\n# {\n#     'precision': 0.85,\n#     'recall': 0.72,\n#     'f1': 0.78,\n#     'accuracy': 0.89,\n#     'mrr': 0.65,\n#     'map': 0.58,\n#     ...\n# }\n</code></pre>"},{"location":"api/evaluator/#precision","title":"<code>precision()</code>","text":"<p>Compute precision at a given threshold.</p> <pre><code>def precision(threshold: float = 0.5) -&gt; float\n</code></pre>"},{"location":"api/evaluator/#recall","title":"<code>recall()</code>","text":"<p>Compute recall at a given threshold.</p> <pre><code>def recall(threshold: float = 0.5) -&gt; float\n</code></pre>"},{"location":"api/evaluator/#f1_score","title":"<code>f1_score()</code>","text":"<p>Compute F1 score at a given threshold.</p> <pre><code>def f1_score(threshold: float = 0.5) -&gt; float\n</code></pre>"},{"location":"api/evaluator/#mrr","title":"<code>mrr()</code>","text":"<p>Compute Mean Reciprocal Rank.</p> <pre><code>def mrr() -&gt; float\n</code></pre>"},{"location":"api/evaluator/#map_score","title":"<code>map_score()</code>","text":"<p>Compute Mean Average Precision.</p> <pre><code>def map_score() -&gt; float\n</code></pre>"},{"location":"api/evaluator/#precision_at_k","title":"<code>precision_at_k()</code>","text":"<p>Compute Precision@K.</p> <pre><code>def precision_at_k(k: int) -&gt; float\n</code></pre>"},{"location":"api/evaluator/#recall_at_k","title":"<code>recall_at_k()</code>","text":"<p>Compute Recall@K.</p> <pre><code>def recall_at_k(k: int) -&gt; float\n</code></pre>"},{"location":"api/evaluator/#example","title":"Example","text":"<pre><code>from locisimiles import IntertextEvaluator\n\npredictions = [\n    {\"query_id\": \"1\", \"reference_id\": \"a\", \"score\": 0.9},\n    {\"query_id\": \"1\", \"reference_id\": \"b\", \"score\": 0.7},\n    {\"query_id\": \"2\", \"reference_id\": \"c\", \"score\": 0.85},\n]\n\nground_truth = [\n    {\"query_id\": \"1\", \"reference_id\": \"a\", \"label\": 1},\n    {\"query_id\": \"1\", \"reference_id\": \"b\", \"label\": 0},\n    {\"query_id\": \"2\", \"reference_id\": \"c\", \"label\": 1},\n]\n\nevaluator = IntertextEvaluator(predictions, ground_truth)\n\n# Get all metrics\nmetrics = evaluator.evaluate()\n\n# Or compute individual metrics\nprint(f\"Precision: {evaluator.precision():.3f}\")\nprint(f\"Recall: {evaluator.recall():.3f}\")\nprint(f\"F1: {evaluator.f1_score():.3f}\")\nprint(f\"MRR: {evaluator.mrr():.3f}\")\n</code></pre>"},{"location":"api/evaluator/#metric-functions","title":"Metric Functions","text":"<p>Standalone functions for computing evaluation metrics.</p>"},{"location":"api/evaluator/#compute_precision","title":"<code>compute_precision()</code>","text":"<pre><code>def compute_precision(\n    y_true: List[int],\n    y_pred: List[int]\n) -&gt; float\n</code></pre> <p>Compute precision from true and predicted labels.</p>"},{"location":"api/evaluator/#compute_recall","title":"<code>compute_recall()</code>","text":"<pre><code>def compute_recall(\n    y_true: List[int],\n    y_pred: List[int]\n) -&gt; float\n</code></pre> <p>Compute recall from true and predicted labels.</p>"},{"location":"api/evaluator/#compute_f1","title":"<code>compute_f1()</code>","text":"<pre><code>def compute_f1(\n    y_true: List[int],\n    y_pred: List[int]\n) -&gt; float\n</code></pre> <p>Compute F1 score from true and predicted labels.</p>"},{"location":"api/evaluator/#compute_mrr","title":"<code>compute_mrr()</code>","text":"<pre><code>def compute_mrr(\n    rankings: List[List[Tuple[str, float]]],\n    relevant: Dict[str, Set[str]]\n) -&gt; float\n</code></pre> <p>Compute Mean Reciprocal Rank.</p> <p>Parameters:</p> Parameter Type Description <code>rankings</code> <code>List[List[Tuple]]</code> List of ranked results per query <code>relevant</code> <code>Dict[str, Set[str]]</code> Mapping of query IDs to relevant reference IDs"},{"location":"api/evaluator/#compute_map","title":"<code>compute_map()</code>","text":"<pre><code>def compute_map(\n    rankings: List[List[Tuple[str, float]]],\n    relevant: Dict[str, Set[str]]\n) -&gt; float\n</code></pre> <p>Compute Mean Average Precision.</p>"},{"location":"api/evaluator/#example_1","title":"Example","text":"<pre><code>from locisimiles.evaluator import compute_precision, compute_recall, compute_f1\n\ny_true = [1, 0, 1, 1, 0]\ny_pred = [1, 0, 0, 1, 0]\n\nprecision = compute_precision(y_true, y_pred)  # 1.0\nrecall = compute_recall(y_true, y_pred)        # 0.667\nf1 = compute_f1(y_true, y_pred)                # 0.8\n</code></pre>"},{"location":"api/evaluator/#ground-truth-format","title":"Ground Truth Format","text":"<p>Ground truth data should be provided as a list of dictionaries with the following structure:</p> <pre><code>[\n    {\n        \"query_id\": \"verse_id_1\",\n        \"reference_id\": \"target_verse_42\",\n        \"label\": 1  # 1 for match, 0 for non-match\n    },\n    ...\n]\n</code></pre> <p>Or loaded from a CSV file:</p> <pre><code>query_id,reference_id,label\nverse_id_1,target_verse_42,1\nverse_id_1,target_verse_15,0\nverse_id_2,target_verse_8,1\n</code></pre> <p>The evaluator handles both positive and negative examples, allowing for comprehensive evaluation of both precision and recall.</p>"},{"location":"api/pipelines/","title":"Pipelines API","text":"<p>The pipeline module provides the main processing pipelines for intertextual detection.</p>"},{"location":"api/pipelines/#retrievalpipeline","title":"RetrievalPipeline","text":"<p>The <code>RetrievalPipeline</code> uses semantic embeddings to find similar text passages.</p>"},{"location":"api/pipelines/#constructor","title":"Constructor","text":"<pre><code>RetrievalPipeline(\n    model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n    device: Optional[str] = None,\n    batch_size: int = 32\n)\n</code></pre>"},{"location":"api/pipelines/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>model_name</code> <code>str</code> <code>\"sentence-transformers/all-MiniLM-L6-v2\"</code> Name or path of the embedding model <code>device</code> <code>str</code> <code>None</code> Device for computation (<code>cpu</code>, <code>cuda</code>, <code>mps</code>). Auto-detected if <code>None</code> <code>batch_size</code> <code>int</code> <code>32</code> Batch size for encoding"},{"location":"api/pipelines/#methods","title":"Methods","text":""},{"location":"api/pipelines/#retrieve","title":"<code>retrieve()</code>","text":"<p>Find similar passages from target document for each source segment.</p> <pre><code>def retrieve(\n    source: Document,\n    target: Document,\n    top_k: int = 10\n) -&gt; List[RetrievalResult]\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>source</code> <code>Document</code> Source document (queries) <code>target</code> <code>Document</code> Target document (candidates) <code>top_k</code> <code>int</code> Number of candidates to retrieve per query <p>Returns: List of <code>RetrievalResult</code> objects.</p>"},{"location":"api/pipelines/#encode","title":"<code>encode()</code>","text":"<p>Encode texts to embeddings.</p> <pre><code>def encode(texts: List[str]) -&gt; np.ndarray\n</code></pre> <p>Returns: NumPy array of embeddings.</p>"},{"location":"api/pipelines/#example","title":"Example","text":"<pre><code>from locisimiles import Document, RetrievalPipeline\n\nsource = Document.from_csv(\"hieronymus.csv\")\ntarget = Document.from_csv(\"vergil.csv\")\n\npipeline = RetrievalPipeline(\n    model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n)\n\nresults = pipeline.retrieve(source, target, top_k=5)\n\nfor result in results:\n    print(f\"Query: {result.query_id}\")\n    for match in result.matches:\n        print(f\"  - {match.reference_id}: {match.score:.3f}\")\n</code></pre>"},{"location":"api/pipelines/#classificationpipeline","title":"ClassificationPipeline","text":"<p>The <code>ClassificationPipeline</code> uses a transformer model to classify text pairs.</p>"},{"location":"api/pipelines/#constructor_1","title":"Constructor","text":"<pre><code>ClassificationPipeline(\n    model_name: str = \"bert-base-uncased\",\n    device: Optional[str] = None,\n    batch_size: int = 32,\n    max_length: int = 512\n)\n</code></pre>"},{"location":"api/pipelines/#parameters_1","title":"Parameters","text":"Parameter Type Default Description <code>model_name</code> <code>str</code> <code>\"bert-base-uncased\"</code> Name or path of the classification model <code>device</code> <code>str</code> <code>None</code> Device for computation <code>batch_size</code> <code>int</code> <code>32</code> Batch size for inference <code>max_length</code> <code>int</code> <code>512</code> Maximum sequence length"},{"location":"api/pipelines/#methods_1","title":"Methods","text":""},{"location":"api/pipelines/#classify","title":"<code>classify()</code>","text":"<p>Classify text pairs.</p> <pre><code>def classify(\n    pairs: List[TextPair],\n    threshold: float = 0.5\n) -&gt; List[ClassificationResult]\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>pairs</code> <code>List[TextPair]</code> List of text pairs to classify <code>threshold</code> <code>float</code> Classification threshold <p>Returns: List of <code>ClassificationResult</code> objects.</p>"},{"location":"api/pipelines/#example_1","title":"Example","text":"<pre><code>from locisimiles import ClassificationPipeline\n\npipeline = ClassificationPipeline(\n    model_name=\"bert-base-multilingual-cased\"\n)\n\npairs = [\n    (\"q1\", \"r1\", \"Source text 1\", \"Target text 1\"),\n    (\"q2\", \"r2\", \"Source text 2\", \"Target text 2\"),\n]\n\nresults = pipeline.classify(pairs, threshold=0.7)\n\nfor result in results:\n    print(f\"{result.query_id} -&gt; {result.reference_id}: {result.score:.3f}\")\n</code></pre>"},{"location":"api/pipelines/#twostagepipeline","title":"TwoStagePipeline","text":"<p>The <code>TwoStagePipeline</code> combines retrieval and classification for optimal results.</p>"},{"location":"api/pipelines/#constructor_2","title":"Constructor","text":"<pre><code>TwoStagePipeline(\n    retrieval_model: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n    classification_model: str = \"bert-base-uncased\",\n    device: Optional[str] = None,\n    batch_size: int = 32\n)\n</code></pre>"},{"location":"api/pipelines/#parameters_2","title":"Parameters","text":"Parameter Type Default Description <code>retrieval_model</code> <code>str</code> <code>\"sentence-transformers/all-MiniLM-L6-v2\"</code> Model for retrieval stage <code>classification_model</code> <code>str</code> <code>\"bert-base-uncased\"</code> Model for classification stage <code>device</code> <code>str</code> <code>None</code> Device for computation <code>batch_size</code> <code>int</code> <code>32</code> Batch size for processing"},{"location":"api/pipelines/#methods_2","title":"Methods","text":""},{"location":"api/pipelines/#run","title":"<code>run()</code>","text":"<p>Run the full two-stage pipeline.</p> <pre><code>def run(\n    source: Document,\n    target: Document,\n    top_k: int = 10,\n    threshold: float = 0.5\n) -&gt; List[PipelineResult]\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>source</code> <code>Document</code> Source document <code>target</code> <code>Document</code> Target document <code>top_k</code> <code>int</code> Candidates per query from retrieval <code>threshold</code> <code>float</code> Classification threshold <p>Returns: List of <code>PipelineResult</code> objects.</p>"},{"location":"api/pipelines/#example_2","title":"Example","text":"<pre><code>from locisimiles import Document, TwoStagePipeline\n\nsource = Document.from_csv(\"hieronymus.csv\")\ntarget = Document.from_csv(\"vergil.csv\")\n\npipeline = TwoStagePipeline(\n    retrieval_model=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n    classification_model=\"bert-base-multilingual-cased\"\n)\n\nresults = pipeline.run(source, target, top_k=20, threshold=0.6)\n\nfor result in results:\n    if result.above_threshold:\n        print(f\"{result.query_id} -&gt; {result.reference_id}: {result.score:.3f}\")\n</code></pre>"},{"location":"api/pipelines/#type-definitions","title":"Type Definitions","text":""},{"location":"api/pipelines/#textpair","title":"TextPair","text":"<pre><code>TextPair = Tuple[str, str, str, str]  # (query_id, reference_id, query_text, reference_text)\n</code></pre>"},{"location":"api/pipelines/#retrievalresult","title":"RetrievalResult","text":"<pre><code>@dataclass\nclass RetrievalResult:\n    query_id: str\n    matches: List[Match]\n\n@dataclass\nclass Match:\n    reference_id: str\n    score: float\n</code></pre>"},{"location":"api/pipelines/#classificationresult","title":"ClassificationResult","text":"<pre><code>@dataclass\nclass ClassificationResult:\n    query_id: str\n    reference_id: str\n    score: float\n    above_threshold: bool\n</code></pre>"},{"location":"api/pipelines/#pipelineresult","title":"PipelineResult","text":"<pre><code>@dataclass\nclass PipelineResult:\n    query_id: str\n    reference_id: str\n    retrieval_score: float\n    classification_score: float\n    score: float  # Combined/final score\n    above_threshold: bool\n</code></pre>"}]}