{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"LociSimiles","text":"<p>A Python package for finding intertextual links in Latin literature using pre-trained language models.</p> <p>LociSimiles helps researchers discover textual similarities and allusions between Latin texts using modern NLP techniques.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Semantic Retrieval: Find similar passages using sentence embeddings</li> <li>Classification Pipeline: Classify text pairs using transformer models</li> <li>Two-Stage Pipeline: Combine retrieval and classification for optimal results</li> <li>Evaluation Tools: Comprehensive metrics for assessing intertextual detection</li> <li>Command-Line Interface: Easy-to-use CLI for common workflows</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<pre><code>pip install locisimiles\n</code></pre> <p>Or install with development dependencies:</p> <pre><code>pip install \"locisimiles[dev]\"\n</code></pre>"},{"location":"#basic-usage","title":"Basic Usage","text":"<pre><code>from locisimiles import Document, RetrievalPipeline\n\n# Load your documents\nsource = Document.from_csv(\"source_texts.csv\")\ntarget = Document.from_csv(\"target_texts.csv\")\n\n# Create retrieval pipeline\npipeline = RetrievalPipeline()\n\n# Find similar passages\nresults = pipeline.retrieve(source, target, top_k=5)\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>Getting Started - Installation and first steps</li> <li>CLI Reference - Command-line interface documentation</li> <li>API Reference - Complete API documentation</li> <li>Development - Contributing and development setup</li> </ul>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License.</p>"},{"location":"cli/","title":"CLI Reference","text":"<p>LociSimiles provides a command-line interface for common workflows.</p>"},{"location":"cli/#installation","title":"Installation","text":"<p>The CLI is installed automatically with the package:</p> <pre><code>pip install locisimiles\n</code></pre>"},{"location":"cli/#commands","title":"Commands","text":""},{"location":"cli/#locisimiles-run","title":"<code>locisimiles run</code>","text":"<p>Run the intertextual detection pipeline on source and target documents.</p> <pre><code>locisimiles run SOURCE TARGET [OPTIONS]\n</code></pre>"},{"location":"cli/#arguments","title":"Arguments","text":"Argument Description <code>SOURCE</code> Path to the source CSV file <code>TARGET</code> Path to the target CSV file"},{"location":"cli/#options","title":"Options","text":"Option Default Description <code>--output</code>, <code>-o</code> <code>results.csv</code> Output file path <code>--model</code>, <code>-m</code> <code>sentence-transformers/all-MiniLM-L6-v2</code> Model name or path <code>--top-k</code>, <code>-k</code> <code>10</code> Number of candidates to retrieve <code>--threshold</code>, <code>-t</code> <code>0.5</code> Classification threshold <code>--batch-size</code>, <code>-b</code> <code>32</code> Batch size for processing"},{"location":"cli/#examples","title":"Examples","text":"<p>Basic usage:</p> <pre><code>locisimiles run source.csv target.csv\n</code></pre> <p>With custom output and model:</p> <pre><code>locisimiles run source.csv target.csv \\\n    --output results.csv \\\n    --model bert-base-multilingual-cased \\\n    --top-k 20\n</code></pre>"},{"location":"cli/#locisimiles-evaluate","title":"<code>locisimiles evaluate</code>","text":"<p>Evaluate detection results against ground truth.</p> <pre><code>locisimiles evaluate PREDICTIONS GROUND_TRUTH [OPTIONS]\n</code></pre>"},{"location":"cli/#arguments_1","title":"Arguments","text":"Argument Description <code>PREDICTIONS</code> Path to predictions CSV file <code>GROUND_TRUTH</code> Path to ground truth CSV file"},{"location":"cli/#options_1","title":"Options","text":"Option Default Description <code>--output</code>, <code>-o</code> <code>None</code> Output file for metrics (prints to stdout if not specified)"},{"location":"cli/#examples_1","title":"Examples","text":"<pre><code>locisimiles evaluate results.csv ground_truth.csv\n</code></pre> <p>Save metrics to file:</p> <pre><code>locisimiles evaluate results.csv ground_truth.csv -o metrics.json\n</code></pre>"},{"location":"cli/#input-file-formats","title":"Input File Formats","text":""},{"location":"cli/#sourcetarget-csv","title":"Source/Target CSV","text":"<p>CSV files should contain at minimum an ID column and a text column:</p> <pre><code>id,text\n1,\"Arma virumque cano Troiae qui primus ab oris\"\n2,\"Italiam fato profugus Laviniaque venit\"\n</code></pre>"},{"location":"cli/#ground-truth-csv","title":"Ground Truth CSV","text":"<p>Ground truth files should contain query-reference pairs with labels:</p> <pre><code>query_id,reference_id,label\n1,42,1\n2,15,0\n</code></pre> <p>Where <code>label</code> is <code>1</code> for true matches and <code>0</code> for non-matches.</p>"},{"location":"cli/#output-format","title":"Output Format","text":"<p>The pipeline outputs a CSV with the following columns:</p> Column Description <code>query_id</code> ID of the source text segment <code>reference_id</code> ID of the matched target segment <code>score</code> Similarity/classification score <code>above_threshold</code> Whether the score exceeds the threshold"},{"location":"cli/#environment-variables","title":"Environment Variables","text":"Variable Description <code>LOCISIMILES_CACHE_DIR</code> Directory for model caching <code>LOCISIMILES_DEVICE</code> Device for computation (<code>cpu</code>, <code>cuda</code>, <code>mps</code>)"},{"location":"development/","title":"Development Guide","text":"<p>This guide covers setting up a development environment and contributing to LociSimiles.</p>"},{"location":"development/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10 or higher</li> <li>Git</li> <li>pip package manager</li> </ul>"},{"location":"development/#setup","title":"Setup","text":""},{"location":"development/#clone-the-repository","title":"Clone the Repository","text":"<pre><code>git clone https://github.com/julian-schelb/locisimiles.git\ncd locisimiles\n</code></pre>"},{"location":"development/#create-virtual-environment","title":"Create Virtual Environment","text":"<pre><code>python -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n</code></pre>"},{"location":"development/#install-development-dependencies","title":"Install Development Dependencies","text":"<pre><code>pip install -e \".[dev]\"\n</code></pre> <p>This installs the package in editable mode along with development tools:</p> <ul> <li>pytest - Testing framework</li> <li>pytest-cov - Coverage reporting</li> <li>poethepoet - Task runner</li> <li>mkdocs - Documentation</li> <li>mkdocs-material - Documentation theme</li> </ul>"},{"location":"development/#running-tests","title":"Running Tests","text":""},{"location":"development/#using-poe-recommended","title":"Using Poe (Recommended)","text":"<pre><code># Run all tests\npoe test\n\n# Run tests with coverage report\npoe test-cov\n</code></pre>"},{"location":"development/#using-pytest-directly","title":"Using Pytest Directly","text":"<pre><code># Run all tests\npytest\n\n# Run with verbose output\npytest -v\n\n# Run specific test file\npytest tests/test_document.py\n\n# Run specific test\npytest tests/test_document.py::TestDocument::test_from_csv\n\n# Run with coverage\npytest --cov=locisimiles --cov-report=term-missing\n\n# Generate HTML coverage report\npytest --cov=locisimiles --cov-report=html\n</code></pre>"},{"location":"development/#documentation","title":"Documentation","text":""},{"location":"development/#serve-locally","title":"Serve Locally","text":"<pre><code>poe docs\n</code></pre> <p>This starts a local server at <code>http://127.0.0.1:8000</code> with live reload.</p>"},{"location":"development/#build-static-site","title":"Build Static Site","text":"<pre><code>poe docs-build\n</code></pre> <p>Output is written to the <code>site/</code> directory.</p>"},{"location":"development/#deploy-to-github-pages","title":"Deploy to GitHub Pages","text":"<pre><code>poe docs-deploy\n</code></pre> <p>This builds and pushes to the <code>gh-pages</code> branch.</p>"},{"location":"development/#project-structure","title":"Project Structure","text":"<pre><code>locisimiles/\n\u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 locisimiles/\n\u2502       \u251c\u2500\u2500 __init__.py          # Package exports\n\u2502       \u251c\u2500\u2500 cli.py               # Command-line interface\n\u2502       \u251c\u2500\u2500 document.py          # Document and TextSegment classes\n\u2502       \u251c\u2500\u2500 evaluator.py         # Evaluation metrics\n\u2502       \u2514\u2500\u2500 pipeline/\n\u2502           \u251c\u2500\u2500 __init__.py      # Pipeline exports\n\u2502           \u251c\u2500\u2500 _types.py        # Type definitions\n\u2502           \u251c\u2500\u2500 classification.py # Classification pipeline\n\u2502           \u251c\u2500\u2500 retrieval.py     # Retrieval pipeline\n\u2502           \u2514\u2500\u2500 two_stage.py     # Combined pipeline\n\u251c\u2500\u2500 tests/\n\u2502   \u251c\u2500\u2500 conftest.py              # Shared test fixtures\n\u2502   \u251c\u2500\u2500 test_document.py         # Document tests\n\u2502   \u251c\u2500\u2500 test_evaluator.py        # Evaluator tests\n\u2502   \u251c\u2500\u2500 test_cli.py              # CLI tests\n\u2502   \u251c\u2500\u2500 test_types.py            # Type definition tests\n\u2502   \u251c\u2500\u2500 test_retrieval.py        # Retrieval pipeline tests\n\u2502   \u251c\u2500\u2500 test_classification.py   # Classification pipeline tests\n\u2502   \u2514\u2500\u2500 test_two_stage.py        # Two-stage pipeline tests\n\u251c\u2500\u2500 docs/                        # Documentation source\n\u251c\u2500\u2500 examples/                    # Example scripts and notebooks\n\u251c\u2500\u2500 pyproject.toml               # Project configuration\n\u2514\u2500\u2500 mkdocs.yml                   # Documentation configuration\n</code></pre>"},{"location":"development/#code-style","title":"Code Style","text":""},{"location":"development/#general-guidelines","title":"General Guidelines","text":"<ul> <li>Follow PEP 8 style guidelines</li> <li>Use type hints for function signatures</li> <li>Write docstrings for public APIs</li> <li>Keep functions focused and small</li> </ul>"},{"location":"development/#example","title":"Example","text":"<pre><code>def compute_similarity(\n    text_a: str,\n    text_b: str,\n    model: Optional[str] = None\n) -&gt; float:\n    \"\"\"Compute semantic similarity between two texts.\n\n    Args:\n        text_a: First text string.\n        text_b: Second text string.\n        model: Optional model name. Defaults to MiniLM.\n\n    Returns:\n        Similarity score between 0 and 1.\n\n    Raises:\n        ValueError: If either text is empty.\n    \"\"\"\n    if not text_a or not text_b:\n        raise ValueError(\"Texts cannot be empty\")\n    ...\n</code></pre>"},{"location":"development/#testing-guidelines","title":"Testing Guidelines","text":""},{"location":"development/#writing-tests","title":"Writing Tests","text":"<ul> <li>Place tests in the <code>tests/</code> directory</li> <li>Name test files <code>test_&lt;module&gt;.py</code></li> <li>Name test classes <code>Test&lt;ClassName&gt;</code></li> <li>Name test methods <code>test_&lt;behavior&gt;</code></li> </ul>"},{"location":"development/#using-fixtures","title":"Using Fixtures","text":"<p>Shared fixtures are defined in <code>conftest.py</code>:</p> <pre><code>def test_document_loading(sample_csv_file):\n    \"\"\"Test loading document from CSV.\"\"\"\n    doc = Document.from_csv(sample_csv_file)\n    assert len(doc) &gt; 0\n</code></pre>"},{"location":"development/#mocking-external-dependencies","title":"Mocking External Dependencies","text":"<p>Use unittest.mock for ML models:</p> <pre><code>from unittest.mock import MagicMock, patch\n\ndef test_retrieval_with_mock(mock_embedder):\n    \"\"\"Test retrieval with mocked embedding model.\"\"\"\n    pipeline = RetrievalPipeline()\n    pipeline.model = mock_embedder\n    # Test without actual model loading\n</code></pre>"},{"location":"development/#contributing","title":"Contributing","text":""},{"location":"development/#workflow","title":"Workflow","text":"<ol> <li>Fork the repository</li> <li>Create a feature branch: <code>git checkout -b feature/my-feature</code></li> <li>Make changes and add tests</li> <li>Ensure tests pass: <code>poe test</code></li> <li>Commit changes: <code>git commit -m \"Add my feature\"</code></li> <li>Push to fork: <code>git push origin feature/my-feature</code></li> <li>Open a Pull Request</li> </ol>"},{"location":"development/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<ul> <li>Include tests for new functionality</li> <li>Update documentation as needed</li> <li>Keep commits focused and atomic</li> <li>Write clear commit messages</li> </ul>"},{"location":"development/#available-poe-tasks","title":"Available Poe Tasks","text":"Task Command Description <code>test</code> <code>poe test</code> Run all tests <code>test-cov</code> <code>poe test-cov</code> Run tests with coverage <code>docs</code> <code>poe docs</code> Serve documentation locally <code>docs-build</code> <code>poe docs-build</code> Build documentation <code>docs-deploy</code> <code>poe docs-deploy</code> Deploy to GitHub Pages"},{"location":"getting-started/","title":"Getting Started","text":"<p>This guide will help you get started with LociSimiles for finding intertextual links in Latin literature.</p>"},{"location":"getting-started/#installation","title":"Installation","text":""},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10 or higher</li> <li>pip package manager</li> </ul>"},{"location":"getting-started/#installing-from-pypi","title":"Installing from PyPI","text":"<pre><code>pip install locisimiles\n</code></pre>"},{"location":"getting-started/#installing-from-source","title":"Installing from Source","text":"<pre><code>git clone https://github.com/julian-schelb/locisimiles.git\ncd locisimiles\npip install -e \".[dev]\"\n</code></pre>"},{"location":"getting-started/#basic-concepts","title":"Basic Concepts","text":""},{"location":"getting-started/#documents-and-segments","title":"Documents and Segments","text":"<p>LociSimiles works with Documents containing TextSegments. Each segment represents a unit of text (e.g., a verse, sentence, or passage).</p> <pre><code>from locisimiles import Document, TextSegment\n\n# Create segments manually\nsegments = [\n    TextSegment(id=\"1\", text=\"Arma virumque cano\"),\n    TextSegment(id=\"2\", text=\"Troiae qui primus ab oris\"),\n]\n\n# Create a document\ndoc = Document(segments=segments)\n</code></pre>"},{"location":"getting-started/#loading-from-csv","title":"Loading from CSV","text":"<p>Documents are typically loaded from CSV files:</p> <pre><code>doc = Document.from_csv(\"texts.csv\")\n</code></pre> <p>The CSV should have columns for <code>id</code> and <code>text</code> (column names are configurable).</p>"},{"location":"getting-started/#pipelines","title":"Pipelines","text":"<p>LociSimiles provides three main pipeline types:</p>"},{"location":"getting-started/#1-retrieval-pipeline","title":"1. Retrieval Pipeline","text":"<p>Uses semantic embeddings to find similar passages:</p> <pre><code>from locisimiles import RetrievalPipeline\n\npipeline = RetrievalPipeline(\n    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n)\n\nresults = pipeline.retrieve(source_doc, target_doc, top_k=10)\n</code></pre>"},{"location":"getting-started/#2-classification-pipeline","title":"2. Classification Pipeline","text":"<p>Uses a transformer model to classify text pairs:</p> <pre><code>from locisimiles import ClassificationPipeline\n\npipeline = ClassificationPipeline(\n    model_name=\"bert-base-uncased\"\n)\n\nresults = pipeline.classify(pairs)\n</code></pre>"},{"location":"getting-started/#3-two-stage-pipeline","title":"3. Two-Stage Pipeline","text":"<p>Combines retrieval and classification for best results:</p> <pre><code>from locisimiles import TwoStagePipeline\n\npipeline = TwoStagePipeline(\n    retrieval_model=\"sentence-transformers/all-MiniLM-L6-v2\",\n    classification_model=\"bert-base-uncased\"\n)\n\nresults = pipeline.run(source_doc, target_doc)\n</code></pre>"},{"location":"getting-started/#evaluation","title":"Evaluation","text":"<p>Use the <code>IntertextEvaluator</code> to assess detection quality:</p> <pre><code>from locisimiles import IntertextEvaluator\n\nevaluator = IntertextEvaluator(\n    predictions=predictions,\n    ground_truth=ground_truth\n)\n\nmetrics = evaluator.evaluate()\nprint(f\"Precision: {metrics['precision']:.3f}\")\nprint(f\"Recall: {metrics['recall']:.3f}\")\nprint(f\"F1: {metrics['f1']:.3f}\")\n</code></pre>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<ul> <li>See the CLI Reference for command-line usage</li> <li>Explore the API Reference for detailed documentation</li> <li>Check out the examples for complete workflows</li> </ul>"},{"location":"api/","title":"API Reference","text":"<p>This section provides detailed documentation for the LociSimiles Python API, auto-generated from source code docstrings.</p>"},{"location":"api/#core-modules","title":"Core Modules","text":""},{"location":"api/#document-module","title":"Document Module","text":"<p>The Document module provides classes for representing and loading text collections:</p> <ul> <li><code>TextSegment</code> - Individual text unit with ID and content</li> <li><code>Document</code> - Container for text segments</li> </ul>"},{"location":"api/#pipeline-module","title":"Pipeline Module","text":"<p>The Pipelines module provides the main processing pipelines:</p> <ul> <li><code>RetrievalPipeline</code> - Semantic similarity retrieval</li> <li><code>ClassificationPipeline</code> - Text pair classification</li> <li><code>TwoStagePipeline</code> - Combined retrieval and classification</li> </ul>"},{"location":"api/#evaluator-module","title":"Evaluator Module","text":"<p>The Evaluator module provides tools for assessing detection quality:</p> <ul> <li><code>IntertextEvaluator</code> - Main evaluation class</li> <li>Metric functions for precision, recall, F1, MRR, MAP</li> </ul>"},{"location":"api/#quick-reference","title":"Quick Reference","text":""},{"location":"api/#loading-documents","title":"Loading Documents","text":"<pre><code>from locisimiles import Document\n\n# From CSV file\ndoc = Document.from_csv(\"tedoc = Document.from_csv(\"tedoc =naries\ndoc = Document.from_records([\n    {\"id\": \"1\", \"text\": \"First passage\"},\n    {\"id\": \"2\", \"text\": \"Second passage\"},\n])\n</code></pre>"},{"location":"api/#running-runnes","title":"Running### Runnes","text":"<p><code>py</code>py<code>py``cisimiles</code>py<code>py</code>py<code>cisimiles```py```py```py</code>cisimiles<code>py</code>py<code>py``cisimiles</code>py<code>py</code>py, top_k<code>py</code>py<code>py``cisimiles</code>py<code>py</code>py<code>cisimiles</code>thon <code>py</code>py<code>py``cisimiles</code>py<code>`py</code>ator</p> <p><code>py```py```py</code>cext<code>py```py```py</code>cext<code>py```py```py</code>cext<code>py```py```py</code>cext``py<code></code></p>"},{"location":"api/document/","title":"Document API","text":"<p>The document module provides classes for representing text collections.</p>"},{"location":"api/document/#locisimiles.document.Document","title":"<code>Document(path, *, author=None, meta=None, segment_delimiter='\\n')</code>","text":"<p>Collection of text segments, representing a document.</p> Source code in <code>src/locisimiles/document.py</code> <pre><code>def __init__(\n    self,\n    path: str | Path,\n    *,\n    author: str | None = None,\n    meta: Dict[str, Any] | None = None,\n    segment_delimiter: str = \"\\n\",\n):\n    self.path: Path = Path(path)\n    self.author: str | None = author\n    self.meta: Dict[str, Any] = meta or {}\n    self._segments: Dict[ID, TextSegment] = {}\n\n    if self.path.exists():\n        if self.path.suffix.lower() in {\".csv\", \".tsv\"}:\n            self._load_csv()\n        else:\n            self._load_plain(segment_delimiter)\n</code></pre>"},{"location":"api/document/#locisimiles.document.Document.add_segment","title":"<code>add_segment(text, seg_id, *, row_id=None, meta=None)</code>","text":"<p>Add a new text segment to the document.</p> Source code in <code>src/locisimiles/document.py</code> <pre><code>def add_segment(\n    self,\n    text: str,\n    seg_id: ID,\n    *,\n    row_id: int | None = None,\n    meta: Dict[str, Any] | None = None,\n) -&gt; None:\n    \"\"\"Add a new text segment to the document.\"\"\"\n    if seg_id in self._segments:\n        raise ValueError(f\"Segment id {seg_id!r} already exists in document\")\n    if row_id is None:\n        row_id = len(self._segments)\n    self._segments[seg_id] = TextSegment(\n        text, seg_id, row_id=row_id, meta=meta)\n</code></pre>"},{"location":"api/document/#locisimiles.document.Document.get_text","title":"<code>get_text(seg_id)</code>","text":"<p>Return raw text of a segment.</p> Source code in <code>src/locisimiles/document.py</code> <pre><code>def get_text(self, seg_id: ID) -&gt; str:\n    \"\"\"Return raw text of a segment.\"\"\"\n    return self._segments[seg_id].text\n</code></pre>"},{"location":"api/document/#locisimiles.document.Document.ids","title":"<code>ids()</code>","text":"<p>Return segment IDs in original order.</p> Source code in <code>src/locisimiles/document.py</code> <pre><code>def ids(self) -&gt; List[ID]:\n    \"\"\"Return segment IDs in original order.\"\"\"\n    return [s.id for s in self]\n</code></pre>"},{"location":"api/document/#locisimiles.document.Document.remove_segment","title":"<code>remove_segment(seg_id)</code>","text":"<p>Delete a segment if present.</p> Source code in <code>src/locisimiles/document.py</code> <pre><code>def remove_segment(self, seg_id: ID) -&gt; None:\n    \"\"\"Delete a segment if present.\"\"\"\n    self._segments.pop(seg_id, None)\n</code></pre>"},{"location":"api/document/#locisimiles.document.TextSegment","title":"<code>TextSegment(text, seg_id, *, row_id=None, meta=None)</code>","text":"<p>Atomic unit of text inside a document.</p> Source code in <code>src/locisimiles/document.py</code> <pre><code>def __init__(\n    self,\n    text: str,\n    seg_id: ID,\n    *,\n    row_id: int | None = None,\n    meta: Dict[str, Any] | None = None,\n):\n    self.text: str = text\n    self.id: ID = seg_id\n    self.row_id: int | None = row_id\n    self.meta: Dict[str, Any] = meta or {}\n</code></pre>"},{"location":"api/evaluator/","title":"Evaluator API","text":"<p>The evaluator module provides tools for assessing the quality of intertextual detection.</p>"},{"location":"api/evaluator/#locisimiles.evaluator.IntertextEvaluator","title":"<code>IntertextEvaluator(*, query_doc, source_doc, ground_truth_csv, pipeline, top_k=5, threshold='auto', auto_threshold_metric='smr')</code>","text":"<p>Compute sentence- and document-level scores for intertextual link prediction.</p> Source code in <code>src/locisimiles/evaluator.py</code> <pre><code>def __init__(\n    self,\n    *,\n    query_doc: Document,\n    source_doc: Document,\n    ground_truth_csv: str | pd.DataFrame,\n    pipeline: ClassificationPipelineWithCandidategeneration,\n    top_k: int = 5,\n    threshold: float | str = \"auto\",\n    auto_threshold_metric: str = \"smr\",\n):\n    # Persist inputs\n    self.query_doc = query_doc\n    self.source_doc = source_doc\n    self.pipeline = pipeline\n    self.top_k = top_k\n    self._auto_threshold_metric = auto_threshold_metric\n    self._threshold_is_auto = (threshold == \"auto\")\n\n    # 1) LOAD GOLD LABELS \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    self.gold_labels = self._load_gold_labels(ground_truth_csv)\n\n    # 2) RUN PIPELINE ONCE \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    self.predictions: FullDict = pipeline.run(\n        query=query_doc,\n        source=source_doc,\n        top_k=top_k,\n    )\n\n    # Inform user if top_k &lt; |D_s|\n    self.num_source_sentences = len(self.source_doc.ids())\n    if top_k &lt; self.num_source_sentences:\n        print(\n            f\"[IntertextEvaluator] top_k={top_k} &lt; {self.num_source_sentences} \"\n            \"source sentences \u2192 pairs not returned by the pipeline \"\n            \"will be treated as negatives.\"\n        )\n\n    # 3) AUTO-THRESHOLD \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    if self._threshold_is_auto:\n        # Temporarily set a default threshold to allow find_best_threshold to run\n        self.threshold = 0.5\n        best_result, _ = self.find_best_threshold(metric=auto_threshold_metric)\n        self.threshold = best_result[\"best_threshold\"]\n        print(\n            f\"[IntertextEvaluator] Auto-threshold enabled: \"\n            f\"best {auto_threshold_metric} at threshold={self.threshold:.2f}\"\n        )\n    else:\n        self.threshold = float(threshold)\n\n    # Internal caches (populated lazily)\n    self._per_sentence_df: pd.DataFrame | None = None\n    self._conf_matrix_cache: Dict[str, Tuple[int, int, int, int]] = {}\n</code></pre>"},{"location":"api/evaluator/#locisimiles.evaluator.IntertextEvaluator.confusion_matrix","title":"<code>confusion_matrix(query_id)</code>","text":"<p>Return 2x2 confusion matrix [[TP,FP],[FN,TN]] for one query sentence.</p> Source code in <code>src/locisimiles/evaluator.py</code> <pre><code>def confusion_matrix(self, query_id: str) -&gt; np.ndarray:\n    \"\"\"Return 2x2 confusion matrix [[TP,FP],[FN,TN]] for one query sentence.\"\"\"\n    if query_id not in self._conf_matrix_cache:\n        self.evaluate_single_query(query_id)  # populate if missing\n    tp, fp, fn, tn = self._conf_matrix_cache[query_id]\n    return np.array([[tp, fp], [fn, tn]], dtype=int)\n</code></pre>"},{"location":"api/evaluator/#locisimiles.evaluator.IntertextEvaluator.evaluate","title":"<code>evaluate(*, average='macro', with_match_only=False)</code>","text":"<p>Compute aggregated metrics across queries.</p> <ul> <li>Precision, Recall, F1, Accuracy: ALWAYS computed on queries with at least    one ground truth match (otherwise these metrics are meaningless).</li> <li>FPR, FNR, SMR: Computed on ALL queries by default (measures false alarms    on queries that shouldn't have matches). If with_match_only=True, these    are also restricted to queries with matches.</li> </ul> Source code in <code>src/locisimiles/evaluator.py</code> <pre><code>def evaluate(self, *, average: str = \"macro\", with_match_only: bool = False) -&gt; Dict[str, float]:\n    \"\"\"\n    Compute aggregated metrics across queries.\n\n    - Precision, Recall, F1, Accuracy: ALWAYS computed on queries with at least \n      one ground truth match (otherwise these metrics are meaningless).\n    - FPR, FNR, SMR: Computed on ALL queries by default (measures false alarms \n      on queries that shouldn't have matches). If with_match_only=True, these \n      are also restricted to queries with matches.\n    \"\"\"\n    # Get queries with matches for TP-dependent metrics\n    df_with_match = self.evaluate_all_queries(with_match_only=True)\n\n    # Get all queries for error-rate metrics (unless with_match_only=True)\n    if with_match_only:\n        df_all = df_with_match\n    else:\n        df_all = self.evaluate_all_queries(with_match_only=False)\n\n    # Sums from queries WITH matches (for precision, recall, f1, accuracy)\n    tp_match = int(df_with_match[\"tp\"].sum())\n    fp_match = int(df_with_match[\"fp\"].sum())\n    fn_match = int(df_with_match[\"fn\"].sum())\n    tn_match = int(df_with_match[\"tn\"].sum())\n    total_match = tp_match + fp_match + fn_match + tn_match\n\n    # Sums from ALL queries (for fpr, fnr, smr)\n    tp_all = int(df_all[\"tp\"].sum())\n    fp_all = int(df_all[\"fp\"].sum())\n    fn_all = int(df_all[\"fn\"].sum())\n    tn_all = int(df_all[\"tn\"].sum())\n    total_all = tp_all + fp_all + fn_all + tn_all\n\n    if average not in [\"macro\", \"micro\"]:\n        raise ValueError(\"average must be 'macro' or 'micro'\")\n\n    # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 MACRO (uniform) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    if average == \"macro\":\n        # TP-dependent metrics from queries with matches\n        precision = float(df_with_match[\"precision\"].mean()) if len(df_with_match) else 0.0\n        recall    = float(df_with_match[\"recall\"].mean()) if len(df_with_match) else 0.0\n        f1        = float(df_with_match[\"f1\"].mean()) if len(df_with_match) else 0.0\n        accuracy  = float(df_with_match[\"accuracy\"].mean()) if len(df_with_match) else 0.0\n\n        # Error-rate metrics from all queries (or match-only if requested)\n        fpr = float(df_all[\"fpr\"].mean()) if len(df_all) else 0.0\n        fnr = float(df_all[\"fnr\"].mean()) if len(df_all) else 0.0\n        smr = float(df_all[\"smr\"].mean()) if len(df_all) else 0.0\n\n        aggregated_metrics = {\n            \"precision\": precision, \"recall\": recall, \"f1\": f1,\n            \"accuracy\": accuracy,   \"fpr\": fpr,       \"fnr\": fnr,\n            \"smr\": smr,\n            \"tp\": tp_all, \"fp\": fp_all, \"fn\": fn_all, \"tn\": tn_all,\n        }\n\n    # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 MICRO (pooled) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    if average == \"micro\":\n        # TP-dependent metrics from queries with matches\n        precision = _precision(tp_match, fp_match)\n        recall    = _recall(tp_match, fn_match)\n        f1        = _f1(precision, recall)\n        accuracy  = (tp_match + tn_match) / total_match if total_match else 0.0\n\n        # Error-rate metrics from all queries (or match-only if requested)\n        fpr = fp_all / total_all if total_all else 0.0\n        fnr = fn_all / total_all if total_all else 0.0\n        smr = (fp_all + fn_all) / total_all if total_all else 0.0\n\n        aggregated_metrics = {\n            \"precision\": precision, \"recall\": recall, \"f1\": f1,\n            \"accuracy\": accuracy,   \"fpr\": fpr,       \"fnr\": fnr,\n            \"smr\": smr,\n            \"tp\": tp_all, \"fp\": fp_all, \"fn\": fn_all, \"tn\": tn_all,\n        }\n\n    return pd.DataFrame([aggregated_metrics]).reset_index(drop=True)\n</code></pre>"},{"location":"api/evaluator/#locisimiles.evaluator.IntertextEvaluator.evaluate_all_queries","title":"<code>evaluate_all_queries(with_match_only=False)</code>","text":"<p>Compute metrics for every query sentence (cached).</p> Source code in <code>src/locisimiles/evaluator.py</code> <pre><code>def evaluate_all_queries(self, with_match_only: bool = False) -&gt; pd.DataFrame:\n    \"\"\"Compute metrics for every query sentence (cached).\"\"\"\n    # if self._per_sentence_df is not None:\n    #     return self._per_sentence_df.copy()\n\n    # Ignore queries without ground truth labels if requested\n    query_ids = self.query_ids_with_match() if with_match_only else self.query_doc.ids()\n\n    # Evaluate each query sentence\n    records = [self.evaluate_single_query(q_id) for q_id in query_ids]\n    self._per_sentence_df = pd.DataFrame(records)\n    return self._per_sentence_df.copy()\n</code></pre>"},{"location":"api/evaluator/#locisimiles.evaluator.IntertextEvaluator.evaluate_k_values","title":"<code>evaluate_k_values(*, k_values=None, average='micro', with_match_only=False)</code>","text":"<p>Evaluate metrics for different top_k values WITHOUT re-running the pipeline.</p> Source code in <code>src/locisimiles/evaluator.py</code> <pre><code>def evaluate_k_values(\n    self,\n    *,\n    k_values: List[int] | None = None,\n    average: str = \"micro\",\n    with_match_only: bool = False,\n) -&gt; Dict[int, Dict[str, float]]:\n    \"\"\"Evaluate metrics for different top_k values WITHOUT re-running the pipeline.\"\"\"\n    if k_values is None:\n        # Default k values, capped at the original top_k\n        k_values = [k for k in [1, 2, 3, 5, 10, 15, 20, 25, 50] if k &lt;= self.top_k]\n\n    # Validate k values\n    k_values = [k for k in k_values if k &lt;= self.top_k]\n    if not k_values:\n        raise ValueError(f\"All k_values exceed the original top_k={self.top_k}\")\n\n    # Store original predictions\n    original_predictions = self.predictions\n    results: Dict[int, Dict[str, float]] = {}\n\n    # Evaluate for each k\n    for k in k_values:\n        filtered_predictions: FullDict = {}\n        for q_id, result_list in original_predictions.items():\n            filtered_predictions[q_id] = result_list[:k]\n\n        # Temporarily replace predictions\n        self.predictions = filtered_predictions\n        self._per_sentence_df = None\n        self._conf_matrix_cache = {}\n\n        # Evaluate at this k\n        metrics_df = self.evaluate(average=average, with_match_only=with_match_only)\n        results[k] = metrics_df.iloc[0].to_dict()\n\n    # Restore original predictions\n    self.predictions = original_predictions\n    self._per_sentence_df = None\n    self._conf_matrix_cache = {}\n\n    return results\n</code></pre>"},{"location":"api/evaluator/#locisimiles.evaluator.IntertextEvaluator.evaluate_single_query","title":"<code>evaluate_single_query(query_id)</code>","text":"<p>Compute metrics for one query sentence.</p> Source code in <code>src/locisimiles/evaluator.py</code> <pre><code>def evaluate_single_query(self, query_id: str) -&gt; Dict[str, float]:\n    \"\"\"Compute metrics for one query sentence.\"\"\"\n    source_ids       = self.source_doc.ids()\n    predicted_links  = self._predicted_link_set()\n\n    gold_vec = np.array(\n        [self.gold_labels.get((query_id, s_id), 0) for s_id in source_ids],\n        dtype=int,\n    )\n    pred_vec = np.array(\n        [1 if (query_id, s_id) in predicted_links else 0 for s_id in source_ids],\n        dtype=int,\n    )\n\n    # Calculate confusion matrix components\n    tp = int(((gold_vec == 1) &amp; (pred_vec == 1)).sum())\n    fp = int(((gold_vec == 0) &amp; (pred_vec == 1)).sum())\n    fn = int(((gold_vec == 1) &amp; (pred_vec == 0)).sum())\n    tn = int(((gold_vec == 0) &amp; (pred_vec == 0)).sum())\n\n    # Calculate metrics\n    precision  = _precision(tp, fp)\n    recall     = _recall(tp, fn)\n    f1         = _f1(precision, recall)\n    accuracy   = (tp + tn) / len(source_ids) if source_ids else 0.0\n    total_errs = fp + fn\n    smr        = _smr(tp, fp, fn, tn)\n    fp_rate    = _fp_rate(tp, fp, fn, tn)\n    fn_rate    = _fn_rate(tp, fp, fn, tn)\n\n    # cache confusion matrix for this query\n    self._conf_matrix_cache[query_id] = (tp, fp, fn, tn)\n\n    return {\n        \"query_id\":  query_id,\n        \"precision\": precision,\n        \"recall\":    recall,\n        \"f1\":        f1,\n        \"accuracy\":  accuracy,\n        \"errors\":    total_errs,\n        \"tp\": tp, \"fp\": fp, \"fn\": fn, \"tn\": tn,\n        \"fpr\": fp_rate,\n        \"fnr\": fn_rate,\n        \"smr\": smr,\n    }\n</code></pre>"},{"location":"api/evaluator/#locisimiles.evaluator.IntertextEvaluator.find_best_threshold","title":"<code>find_best_threshold(*, metric='f1', thresholds=None, average='micro', with_match_only=False)</code>","text":"<p>Find the optimal probability threshold based on the given metric.</p> Source code in <code>src/locisimiles/evaluator.py</code> <pre><code>def find_best_threshold(\n    self,\n    *,\n    metric: str = \"f1\",\n    thresholds: List[float] | None = None,\n    average: str = \"micro\",\n    with_match_only: bool = False,\n) -&gt; Tuple[Dict[str, float], pd.DataFrame]:\n    \"\"\"Find the optimal probability threshold based on the given metric. \"\"\"\n\n    if thresholds is None:\n        thresholds = [round(t * 0.1, 2) for t in range(1, 10)]  # 0.1 to 0.9\n\n    # Metrics\n    maximize_metrics = {\"f1\", \"precision\", \"recall\", \"accuracy\"}\n    minimize_metrics = {\"smr\", \"fpr\", \"fnr\"}\n    valid_metrics = maximize_metrics | minimize_metrics\n    if metric not in valid_metrics:\n        raise ValueError(f\"metric must be one of {valid_metrics}\")\n\n    # Store original threshold to restore later\n    original_threshold = self.threshold\n    results = []\n\n    # Evaluate for each threshold\n    for thresh in thresholds:\n        self.threshold = thresh\n        self._per_sentence_df = None\n        self._conf_matrix_cache = {}\n\n        metrics_df = self.evaluate(average=average, with_match_only=with_match_only)\n        row = metrics_df.iloc[0].to_dict()\n        row[\"threshold\"] = thresh\n        results.append(row)\n\n    # Restore original threshold\n    self.threshold = original_threshold\n    self._per_sentence_df = None\n    self._conf_matrix_cache = {}\n\n    # Build results DataFrame\n    results_df = pd.DataFrame(results)\n    results_df = results_df[[\"threshold\"] + [c for c in results_df.columns if c != \"threshold\"]]\n\n    # Find best threshold (maximize or minimize depending on metric)\n    if metric in minimize_metrics:\n        best_idx = results_df[metric].idxmin()\n    else:\n        best_idx = results_df[metric].idxmax()\n    best_row = results_df.loc[best_idx].to_dict()\n    best_threshold = best_row[\"threshold\"]\n    best_metric_value = best_row[metric]\n\n    best_result = {\n        \"best_threshold\": best_threshold,\n        f\"best_{metric}\": best_metric_value,\n        **{k: v for k, v in best_row.items() if k != \"threshold\"},\n    }\n\n    return best_result, results_df\n</code></pre>"},{"location":"api/evaluator/#locisimiles.evaluator.IntertextEvaluator.query_ids_with_match","title":"<code>query_ids_with_match()</code>","text":"<p>Return query IDs that have ground truth labels.</p> Source code in <code>src/locisimiles/evaluator.py</code> <pre><code>def query_ids_with_match(self) -&gt; List[str]:\n    \"\"\"Return query IDs that have ground truth labels.\"\"\"\n    return list({q_id for q_id, _ in self.gold_labels.keys()})\n</code></pre>"},{"location":"api/pipelines/","title":"Pipelines API","text":"<p>The pipeline module provides the main processing pipelines for intertextual detection.</p>"},{"location":"api/pipelines/#retrieval-pipeline","title":"Retrieval Pipeline","text":"<p>Retrieval-only pipeline: Uses embedding similarity without classification.</p>"},{"location":"api/pipelines/#locisimiles.pipeline.retrieval.RetrievalPipeline","title":"<code>RetrievalPipeline(*, embedding_model_name='julian-schelb/SPhilBerta-emb-lat-intertext-v1', device=None)</code>","text":"<p>A retrieval-only pipeline for intertextuality detection.</p> <p>Uses embedding similarity to find candidate segments without a classification stage. Binary decisions are made based on rank (top-k) or similarity threshold.</p> <p>To maintain compatibility with the evaluator, results are returned in FullDict format where the \"probability\" field is set to 1.0 for positive predictions and 0.0 for negative predictions based on the decision criteria.</p> Source code in <code>src/locisimiles/pipeline/retrieval.py</code> <pre><code>def __init__(\n    self,\n    *,\n    embedding_model_name: str = \"julian-schelb/SPhilBerta-emb-lat-intertext-v1\",\n    device: str | int | None = None,\n):\n    self.device = device if device is not None else \"cpu\"\n    self._source_index: chromadb.Collection | None = None\n\n    # -------- Load Embedding Model ----------\n    self.embedder = SentenceTransformer(embedding_model_name, device=self.device)\n\n    # Keep results in memory for later access\n    self._last_sim: SimDict | None = None\n    self._last_full: FullDict | None = None\n</code></pre>"},{"location":"api/pipelines/#locisimiles.pipeline.retrieval.RetrievalPipeline.build_source_index","title":"<code>build_source_index(source_segments, source_embeddings, collection_name='source_segments', batch_size=5000)</code>","text":"<p>Create a Chroma collection from source_segments and their embeddings.</p> Source code in <code>src/locisimiles/pipeline/retrieval.py</code> <pre><code>def build_source_index(\n    self,\n    source_segments: Sequence[TextSegment],\n    source_embeddings: np.ndarray,\n    collection_name: str = \"source_segments\",\n    batch_size: int = 5000,\n):\n    \"\"\"Create a Chroma collection from *source_segments* and their embeddings.\"\"\"\n\n    client = chromadb.EphemeralClient()\n    unique_name = f\"{collection_name}_{int(time.time() * 1000000)}\"\n\n    try:\n        client.delete_collection(name=unique_name)\n    except Exception:\n        pass\n\n    col = client.create_collection(\n        name=unique_name,\n        metadata={\"hnsw:space\": \"cosine\"}\n    )\n\n    ids = [s.id for s in source_segments]\n    embeddings = source_embeddings.tolist()\n\n    for i in range(0, len(ids), batch_size):\n        col.add(\n            ids=ids[i:i + batch_size],\n            embeddings=embeddings[i:i + batch_size],\n        )\n\n    return col\n</code></pre>"},{"location":"api/pipelines/#locisimiles.pipeline.retrieval.RetrievalPipeline.retrieve","title":"<code>retrieve(*, query, source, top_k=100, query_prompt_name='query', source_prompt_name='match', **kwargs)</code>","text":"<p>Retrieve candidate segments from source based on similarity to query.</p> <p>Returns a dictionary mapping query segment IDs to lists of (source segment, similarity score) pairs, sorted by similarity (descending).</p> Source code in <code>src/locisimiles/pipeline/retrieval.py</code> <pre><code>def retrieve(\n    self,\n    *,\n    query: Document,\n    source: Document,\n    top_k: int = 100,\n    query_prompt_name: str = \"query\",\n    source_prompt_name: str = \"match\",\n    **kwargs: Any,\n) -&gt; SimDict:\n    \"\"\"\n    Retrieve candidate segments from *source* based on similarity to *query*.\n\n    Returns a dictionary mapping query segment IDs to lists of\n    (source segment, similarity score) pairs, sorted by similarity (descending).\n    \"\"\"\n    query_segments = list(query.segments.values())\n    source_segments = list(source.segments.values())\n\n    query_embeddings = self._embed(\n        [s.text for s in tqdm(query_segments, desc=\"Embedding query segments\")],\n        prompt_name=query_prompt_name\n    )\n\n    source_embeddings = self._embed(\n        [s.text for s in tqdm(source_segments, desc=\"Embedding source segments\")],\n        prompt_name=source_prompt_name\n    )\n\n    self._source_index = self.build_source_index(\n        source_segments=source_segments,\n        source_embeddings=source_embeddings,\n        collection_name=\"source_segments\",\n    )\n\n    similarity_results = self._compute_similarity(\n        query_segments=query_segments,\n        query_embeddings=query_embeddings,\n        source_document=source,\n        top_k=top_k,\n    )\n\n    self._last_sim = similarity_results\n    return similarity_results\n</code></pre>"},{"location":"api/pipelines/#locisimiles.pipeline.retrieval.RetrievalPipeline.run","title":"<code>run(*, query, source, top_k=10, similarity_threshold=None, query_prompt_name='query', source_prompt_name='match', **kwargs)</code>","text":"<p>Run the retrieval pipeline and return results compatible with the evaluator.</p> <p>Binary decisions are made using one of two criteria: - top_k (default): The top-k ranked candidates per query are predicted    as positive (prob=1.0), all others as negative (prob=0.0). - similarity_threshold: If provided, candidates with similarity &gt;= threshold   are predicted as positive, regardless of rank.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Document</code> <p>Query document</p> required <code>source</code> <code>Document</code> <p>Source document  </p> required <code>top_k</code> <code>int</code> <p>Number of top candidates to mark as positive (default: 10).    Used when similarity_threshold is None.</p> <code>10</code> <code>similarity_threshold</code> <code>float | None</code> <p>If provided, use this similarity cutoff instead    of top_k. Candidates with similarity &gt;= threshold are positive.</p> <code>None</code> <code>query_prompt_name</code> <code>str</code> <p>Prompt name for query embeddings</p> <code>'query'</code> <code>source_prompt_name</code> <code>str</code> <p>Prompt name for source embeddings</p> <code>'match'</code> <p>Returns:</p> Type Description <code>FullDict</code> <p>FullDict mapping query IDs to lists of (segment, similarity, probability)</p> <code>FullDict</code> <p>tuples. Probability is 1.0 for positive predictions, 0.0 for negative.</p> Source code in <code>src/locisimiles/pipeline/retrieval.py</code> <pre><code>def run(\n    self,\n    *,\n    query: Document,\n    source: Document,\n    top_k: int = 10,\n    similarity_threshold: float | None = None,\n    query_prompt_name: str = \"query\",\n    source_prompt_name: str = \"match\",\n    **kwargs: Any,\n) -&gt; FullDict:\n    \"\"\"\n    Run the retrieval pipeline and return results compatible with the evaluator.\n\n    Binary decisions are made using one of two criteria:\n    - **top_k** (default): The top-k ranked candidates per query are predicted \n      as positive (prob=1.0), all others as negative (prob=0.0).\n    - **similarity_threshold**: If provided, candidates with similarity &gt;= threshold\n      are predicted as positive, regardless of rank.\n\n    Args:\n        query: Query document\n        source: Source document  \n        top_k: Number of top candidates to mark as positive (default: 10).\n               Used when similarity_threshold is None.\n        similarity_threshold: If provided, use this similarity cutoff instead\n               of top_k. Candidates with similarity &gt;= threshold are positive.\n        query_prompt_name: Prompt name for query embeddings\n        source_prompt_name: Prompt name for source embeddings\n\n    Returns:\n        FullDict mapping query IDs to lists of (segment, similarity, probability)\n        tuples. Probability is 1.0 for positive predictions, 0.0 for negative.\n    \"\"\"\n    # Retrieve more candidates than top_k to ensure we have enough for evaluation\n    # When using similarity_threshold, we need all candidates\n    retrieve_k = len(source) if similarity_threshold is not None else top_k\n\n    similarity_dict = self.retrieve(\n        query=query,\n        source=source,\n        top_k=retrieve_k,\n        query_prompt_name=query_prompt_name,\n        source_prompt_name=source_prompt_name,\n    )\n\n    # Convert to FullDict format with binary \"probabilities\"\n    full_results: FullDict = {}\n\n    for query_id, similarity_pairs in similarity_dict.items():\n        full_results[query_id] = []\n\n        for rank, (segment, similarity) in enumerate(similarity_pairs):\n            # Determine if this candidate should be predicted as positive\n            if similarity_threshold is not None:\n                # Use similarity threshold\n                is_positive = similarity &gt;= similarity_threshold\n            else:\n                # Use top-k ranking (0-indexed, so rank &lt; top_k)\n                is_positive = rank &lt; top_k\n\n            # Set probability to 1.0 for positive, 0.0 for negative\n            probability = 1.0 if is_positive else 0.0\n            full_results[query_id].append((segment, similarity, probability))\n\n    self._last_full = full_results\n    return full_results\n</code></pre>"},{"location":"api/pipelines/#classification-pipeline","title":"Classification Pipeline","text":"<p>Classification-only pipeline: Exhaustive pairwise comparison without retrieval.</p>"},{"location":"api/pipelines/#locisimiles.pipeline.classification.ClassificationPipeline","title":"<code>ClassificationPipeline(*, classification_name='julian-schelb/PhilBerta-class-latin-intertext-v1', device=None, pos_class_idx=1)</code>","text":"<p>A simpler pipeline for intertextuality classification without candidate generation. It classifies all possible pairs between query and source segments without a retrieval stage. Suitable for smaller document pairs or when exhaustive comparison is needed.</p> Source code in <code>src/locisimiles/pipeline/classification.py</code> <pre><code>def __init__(\n    self,\n    *,\n    classification_name: str = \"julian-schelb/PhilBerta-class-latin-intertext-v1\",\n    device: str | int | None = None,\n    pos_class_idx: int = 1,  # Index of the positive class in the classifier\n):\n    self.device = device if device is not None else \"cpu\"\n    self.pos_class_idx = pos_class_idx\n\n    # -------- Load Classification Model ----------\n    self.clf_tokenizer = AutoTokenizer.from_pretrained(classification_name)\n    self.clf_model = AutoModelForSequenceClassification.from_pretrained(classification_name)\n    self.clf_model.to(self.device).eval()\n\n    # Keep results in memory for later access\n    self._last_results: FullDict | None = None\n</code></pre>"},{"location":"api/pipelines/#locisimiles.pipeline.classification.ClassificationPipeline.debug_input_sequence","title":"<code>debug_input_sequence(query_text, candidate_text, max_len=512)</code>","text":"<p>Debug method to inspect how a query-candidate pair is encoded.</p> Source code in <code>src/locisimiles/pipeline/classification.py</code> <pre><code>def debug_input_sequence(self, query_text: str, candidate_text: str, max_len: int = 512) -&gt; Dict[str, Any]:\n    \"\"\"Debug method to inspect how a query-candidate pair is encoded.\"\"\"\n    # Truncate the pair\n    query_trunc, candidate_trunc = self._truncate_pair(query_text, candidate_text, max_len)\n\n    # Encode the pair\n    encoding = self.clf_tokenizer(\n        query_trunc,\n        candidate_trunc,\n        add_special_tokens=True,\n        padding=True,\n        truncation=True,\n        max_length=max_len,\n        return_tensors=\"pt\",\n    )\n\n    # Decode with special tokens visible\n    decoded_text = self.clf_tokenizer.decode(encoding['input_ids'].squeeze(), skip_special_tokens=False)\n\n    return {\n        \"query\": query_text,\n        \"candidate\": candidate_text,\n        \"query_truncated\": query_trunc,\n        \"candidate_truncated\": candidate_trunc,\n        \"input_ids\": encoding['input_ids'].squeeze().tolist(),\n        \"attention_mask\": encoding['attention_mask'].squeeze().tolist(),\n        \"input_text\": decoded_text,\n    }\n</code></pre>"},{"location":"api/pipelines/#locisimiles.pipeline.classification.ClassificationPipeline.run","title":"<code>run(*, query, source, batch_size=32, **kwargs)</code>","text":"<p>Run classification on all query-source segment pairs. Returns a dictionary mapping query segment IDs to lists of (source segment, similarity_score=None, P(positive)) tuples.</p> <p>Note: Since there's no retrieval stage, similarity_score is set to None.</p> Source code in <code>src/locisimiles/pipeline/classification.py</code> <pre><code>def run(\n    self,\n    *,\n    query: Document,\n    source: Document,\n    batch_size: int = 32,\n    **kwargs: Any,\n) -&gt; FullDict:\n    \"\"\"\n    Run classification on all query-source segment pairs.\n    Returns a dictionary mapping query segment IDs to lists of\n    (source segment, similarity_score=None, P(positive)) tuples.\n\n    Note: Since there's no retrieval stage, similarity_score is set to None.\n    \"\"\"\n    results: FullDict = {}\n\n    # Extract all source segments\n    source_segments = list(source.segments.values())\n    source_texts = [s.text for s in source_segments]\n\n    # For each query segment, classify against all source segments\n    for query_segment in tqdm(query.segments.values(), desc=\"Classifying pairs\"):\n        query_text = query_segment.text\n\n        # Predict probabilities for all source segments\n        probabilities = self._predict(\n            query_text, \n            source_texts, \n            batch_size=batch_size\n        )\n\n        # Build results with None for similarity score (no retrieval stage)\n        results[query_segment.id] = [\n            (source_seg, None, prob)\n            for source_seg, prob in zip(source_segments, probabilities)\n        ]\n\n    self._last_results = results\n    return results\n</code></pre>"},{"location":"api/pipelines/#two-stage-pipeline","title":"Two-Stage Pipeline","text":"<p>Two-stage pipeline: Retrieval (candidate generation) + Classification.</p>"},{"location":"api/pipelines/#locisimiles.pipeline.two_stage.ClassificationPipelineWithCandidategeneration","title":"<code>ClassificationPipelineWithCandidategeneration(*, classification_name='julian-schelb/PhilBerta-class-latin-intertext-v1', embedding_model_name='julian-schelb/SPhilBerta-emb-lat-intertext-v1', device=None, pos_class_idx=1)</code>","text":"<p>A pipeline for intertextuality classification with candidate generation. It first generates candidate segments from a source document based on similarity to a query segment, and then classifies these candidates as intertextual or not using a pre-trained model.</p> Source code in <code>src/locisimiles/pipeline/two_stage.py</code> <pre><code>def __init__(\n    self,\n    *,\n    classification_name: str = \"julian-schelb/PhilBerta-class-latin-intertext-v1\",\n    embedding_model_name: str = \"julian-schelb/SPhilBerta-emb-lat-intertext-v1\",\n    device: str | int | None = None,\n    pos_class_idx: int = 1,  # Index of the positive class in the classifier\n):\n    self.device = device if device is not None else \"cpu\"\n    self.pos_class_idx = pos_class_idx\n    self._source_index: chromadb.Collection | None = None\n\n    # -------- Load Models ----------\n    self.embedder = SentenceTransformer(embedding_model_name, device=self.device)\n    self.clf_tokenizer = AutoTokenizer.from_pretrained(classification_name)\n    self.clf_model = AutoModelForSequenceClassification.from_pretrained(classification_name)\n    self.clf_model.to(self.device).eval()\n\n    # Keep results in memory for later access\n    self._last_sim: SimDict | None = None\n    self._last_full: FullDict | None = None\n</code></pre>"},{"location":"api/pipelines/#locisimiles.pipeline.two_stage.ClassificationPipelineWithCandidategeneration.build_source_index","title":"<code>build_source_index(source_segments, source_embeddings, collection_name='source_segments', batch_size=5000)</code>","text":"<p>Create a Chroma collection from source_segments and their embeddings.</p> Source code in <code>src/locisimiles/pipeline/two_stage.py</code> <pre><code>def build_source_index(\n    self,\n    source_segments: Sequence[TextSegment],\n    source_embeddings: np.ndarray,\n    collection_name: str = \"source_segments\",\n    batch_size: int = 5000,  # Safe batch size\n):\n    \"\"\"Create a Chroma collection from *source_segments* and their embeddings.\"\"\"\n\n    # Use EphemeralClient for non-persistent, in-memory storage\n    # Create new client each time to ensure clean state\n    client = chromadb.EphemeralClient()\n\n    # Use unique collection name to avoid conflicts in same session\n    unique_name = f\"{collection_name}_{int(time.time() * 1000000)}\"\n\n    # Delete collection if it exists (should not happen with unique names, but just in case)\n    try:\n        client.delete_collection(name=unique_name)\n    except Exception:\n        pass  # Collection doesn't exist, which is fine\n\n    # Create fresh collection with unique name\n    col = client.create_collection(\n        name=unique_name,\n        metadata={\"hnsw:space\": \"cosine\"}  # Use cosine distance\n    )\n\n    # Extract IDs and embeddings\n    ids = [s.id for s in source_segments]\n    embeddings = source_embeddings.tolist()\n\n    # Add segments to the collection in batches\n    for i in range(0, len(ids), batch_size):\n        col.add(\n            ids=ids[i:i + batch_size],\n            embeddings=embeddings[i:i + batch_size],\n        )\n\n    return col\n</code></pre>"},{"location":"api/pipelines/#locisimiles.pipeline.two_stage.ClassificationPipelineWithCandidategeneration.check_candidates","title":"<code>check_candidates(*, query, source, candidates=None, batch_size=32, **kwargs)</code>","text":"<p>Classify candidates generated from source. If candidates is not provided, it will be generated using generate_candidates. Returns a dictionary mapping query segment IDs to lists of (source segment, similarity score, P(positive)) tuples.</p> Source code in <code>src/locisimiles/pipeline/two_stage.py</code> <pre><code>def check_candidates(\n    self,\n    *,\n    query: Document,\n    source: Document,\n    candidates: SimDict | None = None,\n    batch_size: int = 32,\n    **kwargs: Any,\n) -&gt; FullDict:\n    \"\"\"\n    Classify candidates generated from *source*.\n    If *candidates* is not provided, it will be generated using\n    *generate_candidates*.\n    Returns a dictionary mapping query segment IDs to lists of\n    (source segment, similarity score, P(positive)) tuples.\n    \"\"\"\n\n    full_results: FullDict = {}\n\n    for query_id, similarity_pairs in tqdm(candidates.items(), desc=\"Check candidates\"):\n\n        # Predict probabilities for the current query and its candidates\n        candidate_texts = [segment.text for segment, _ in similarity_pairs]\n        predicted_probabilities = self._predict(\n            query[query_id].text, candidate_texts, batch_size=batch_size\n        )\n\n        # Combine segments, similarity scores, and probabilities into results\n        full_results[query_id] = []\n        for (segment, similarity_score), probability in zip(similarity_pairs, predicted_probabilities):\n            full_results[query_id].append((segment, similarity_score, probability))\n\n    self._last_full = full_results\n    return full_results\n</code></pre>"},{"location":"api/pipelines/#locisimiles.pipeline.two_stage.ClassificationPipelineWithCandidategeneration.debug_input_sequence","title":"<code>debug_input_sequence(query_text, candidate_text, max_len=512)</code>","text":"<p>Debug method to inspect how a query-candidate pair is encoded.</p> <p>Returns a dictionary with: - query: Original query text - candidate: Original candidate text - query_truncated: Truncated query text - candidate_truncated: Truncated candidate text - input_ids: Token IDs as list - input_text: Decoded text with special tokens visible - attention_mask: Attention mask as list</p> Source code in <code>src/locisimiles/pipeline/two_stage.py</code> <pre><code>def debug_input_sequence(self, query_text: str, candidate_text: str, max_len: int = 512) -&gt; Dict[str, Any]:\n    \"\"\"Debug method to inspect how a query-candidate pair is encoded.\n\n    Returns a dictionary with:\n    - query: Original query text\n    - candidate: Original candidate text\n    - query_truncated: Truncated query text\n    - candidate_truncated: Truncated candidate text\n    - input_ids: Token IDs as list\n    - input_text: Decoded text with special tokens visible\n    - attention_mask: Attention mask as list\n    \"\"\"\n    # Truncate the pair\n    query_trunc, candidate_trunc = self._truncate_pair(query_text, candidate_text, max_len)\n\n    # Encode the pair\n    encoding = self.clf_tokenizer(\n        query_trunc,\n        candidate_trunc,\n        add_special_tokens=True,\n        padding=True,\n        truncation=True,\n        max_length=max_len,\n        return_tensors=\"pt\",\n    )\n\n    # Decode with special tokens visible\n    decoded_text = self.clf_tokenizer.decode(encoding['input_ids'].squeeze(), skip_special_tokens=False)\n\n    return {\n        \"query\": query_text,\n        \"candidate\": candidate_text,\n        \"query_truncated\": query_trunc,\n        \"candidate_truncated\": candidate_trunc,\n        \"input_ids\": encoding['input_ids'].squeeze().tolist(),\n        \"attention_mask\": encoding['attention_mask'].squeeze().tolist(),\n        \"input_text\": decoded_text,\n    }\n</code></pre>"},{"location":"api/pipelines/#locisimiles.pipeline.two_stage.ClassificationPipelineWithCandidategeneration.generate_candidates","title":"<code>generate_candidates(*, query, source, top_k=5, query_prompt_name='query', source_prompt_name='match', **kwargs)</code>","text":"<p>Generate candidate segments from source based on similarity to query. Returns a dictionary mapping query segment IDs to lists of (source segment, similarity score) pairs.</p> Source code in <code>src/locisimiles/pipeline/two_stage.py</code> <pre><code>def generate_candidates(\n    self,\n    *,\n    query: Document,\n    source: Document,\n    top_k: int = 5,\n    query_prompt_name: str = \"query\",\n    source_prompt_name: str = \"match\",\n    **kwargs: Any,\n) -&gt; SimDict:\n    \"\"\"\n    Generate candidate segments from *source* based on similarity to *query*.\n    Returns a dictionary mapping query segment IDs to lists of\n    (source segment, similarity score) pairs.\n    \"\"\"\n    # Extract segments from query and source documents\n    query_segments = list(query.segments.values())\n    source_segments = list(source.segments.values())\n\n    # Embed query and source segments\n    query_embeddings = self._embed(\n        [s.text for s in tqdm(query_segments, desc=\"Embedding query segments\")],\n        prompt_name=query_prompt_name\n    )\n\n    # Embed source segments with a progress bar\n    source_embeddings = self._embed(\n        [s.text for s in tqdm(source_segments, desc=\"Embedding source segments\")],\n        prompt_name=source_prompt_name\n    )\n\n    # Build the source index for fast retrieval\n    self._source_index = self.build_source_index(\n        source_segments=source_segments,\n        source_embeddings=source_embeddings,\n        collection_name=\"source_segments\",\n    )\n\n    # Compute similarity between query and source segments\n    similarity_results = self._compute_similarity(\n        query_segments=query_segments,\n        query_embeddings=query_embeddings,\n        source_document=source,\n        top_k=top_k,\n    )\n\n    # Cache the results and return\n    self._last_sim = similarity_results\n    return similarity_results\n</code></pre>"},{"location":"api/pipelines/#locisimiles.pipeline.two_stage.ClassificationPipelineWithCandidategeneration.run","title":"<code>run(*, query, source, top_k=5, query_prompt_name='query', source_prompt_name='match', **kwargs)</code>","text":"<p>Run the full pipeline: generate candidates and classify them. Returns a dictionary mapping query segment IDs to lists of (source segment, similarity score, P(positive)) tuples.</p> Source code in <code>src/locisimiles/pipeline/two_stage.py</code> <pre><code>def run(\n    self,\n    *,\n    query: Document,\n    source: Document,\n    top_k: int = 5,\n    query_prompt_name: str = \"query\",\n    source_prompt_name: str = \"match\",\n    **kwargs: Any,\n) -&gt; FullDict:\n    \"\"\"\n    Run the full pipeline: generate candidates and classify them.\n    Returns a dictionary mapping query segment IDs to lists of\n    (source segment, similarity score, P(positive)) tuples.\n    \"\"\"\n    similarity_dict = self.generate_candidates(\n        query=query,\n        source=source,\n        top_k=top_k,\n        query_prompt_name=query_prompt_name,\n        source_prompt_name=source_prompt_name,\n    )\n    return self.check_candidates(\n        query=query,\n        source=source,\n        candidates=similarity_dict,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/pipelines/#type-definitions","title":"Type Definitions","text":"<p>Shared type definitions and utilities for pipeline modules.</p>"},{"location":"api/pipelines/#locisimiles.pipeline._types.pretty_print","title":"<code>pretty_print(results)</code>","text":"<p>Human-friendly dump of run() output.</p> Source code in <code>src/locisimiles/pipeline/_types.py</code> <pre><code>def pretty_print(results: FullDict) -&gt; None:\n    \"\"\"Human-friendly dump of *run()* output.\"\"\"\n    for qid, lst in results.items():\n        print(f\"\\n\u25b6 Query segment {qid!r}:\")\n        for src_seg, sim, ppos in lst:\n            sim_str = f\"{sim:+.3f}\" if sim is not None else \"N/A\"\n            print(f\"  {src_seg.id:&lt;25}  sim={sim_str}  P(pos)={ppos:.3f}\")\n</code></pre>"}]}