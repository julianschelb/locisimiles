{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"LociSimiles","text":"<p>A Python package for extracting intertextualities in Latin literature using pre-trained language models.</p> <p>LociSimiles enables researchers to detect textual reuse, quotations, and allusions between Latin texts, from verbatim citations to subtle paraphrases and thematic echoes.</p>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<pre><code>pip install locisimiles\n</code></pre> <p>Or install with development dependencies:</p> <pre><code>pip install \"locisimiles[dev]\"\n</code></pre>"},{"location":"#basic-usage","title":"Basic Usage","text":"<pre><code>from locisimiles import Document, RetrievalPipeline\n\n# Load your documents\nsource = Document.from_csv(\"source_texts.csv\")\ntarget = Document.from_csv(\"target_texts.csv\")\n\n# Create retrieval pipeline\npipeline = RetrievalPipeline()\n\n# Find similar passages\nresults = pipeline.retrieve(source, target, top_k=5)\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>Getting Started - Installation and first steps</li> <li>Examples - Working examples and tutorials</li> <li>CLI Reference - Command-line interface documentation</li> <li>API Reference - Complete API documentation</li> <li>Development - Contributing and development setup</li> </ul>"},{"location":"#authors","title":"Authors","text":"<ul> <li>Julian Schelb - University of Konstanz</li> <li>Michael Wittweiler - University of Zurich</li> </ul>"},{"location":"#citation","title":"Citation","text":"<p>If you use LociSimiles in your research, please cite our paper:</p> <pre><code>@article{schelb2026locisimiles,\n  title={Loci Similes: A Benchmark for Extracting Intertextualities in Latin Literature},\n  author={Schelb, Julian and Wittweiler, Michael and Revellio, Marie and Feichtinger, Barbara and Spitz, Andreas},\n  journal={arXiv preprint arXiv:2601.07533},\n  year={2026}\n}\n</code></pre>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License.</p>"},{"location":"cli/","title":"CLI Reference","text":"<p>LociSimiles provides a command-line interface for common workflows.</p>"},{"location":"cli/#installation","title":"Installation","text":"<p>The CLI is installed automatically with the package:</p> <pre><code>pip install locisimiles\n</code></pre>"},{"location":"cli/#commands","title":"Commands","text":""},{"location":"cli/#locisimiles-run","title":"<code>locisimiles run</code>","text":"<p>Run the intertextual detection pipeline on source and target documents.</p> <pre><code>locisimiles run SOURCE TARGET [OPTIONS]\n</code></pre>"},{"location":"cli/#arguments","title":"Arguments","text":"Argument Description <code>SOURCE</code> Path to the source CSV file <code>TARGET</code> Path to the target CSV file"},{"location":"cli/#options","title":"Options","text":"Option Default Description <code>--output</code>, <code>-o</code> <code>results.csv</code> Output file path <code>--model</code>, <code>-m</code> <code>sentence-transformers/all-MiniLM-L6-v2</code> Model name or path <code>--top-k</code>, <code>-k</code> <code>10</code> Number of candidates to retrieve <code>--threshold</code>, <code>-t</code> <code>0.5</code> Classification threshold <code>--batch-size</code>, <code>-b</code> <code>32</code> Batch size for processing"},{"location":"cli/#examples","title":"Examples","text":"<p>Basic usage:</p> <pre><code>locisimiles run source.csv target.csv\n</code></pre> <p>With custom output and model:</p> <pre><code>locisimiles run source.csv target.csv \\\n    --output results.csv \\\n    --model bert-base-multilingual-cased \\\n    --top-k 20\n</code></pre>"},{"location":"cli/#locisimiles-evaluate","title":"<code>locisimiles evaluate</code>","text":"<p>Evaluate detection results against ground truth.</p> <pre><code>locisimiles evaluate PREDICTIONS GROUND_TRUTH [OPTIONS]\n</code></pre>"},{"location":"cli/#arguments_1","title":"Arguments","text":"Argument Description <code>PREDICTIONS</code> Path to predictions CSV file <code>GROUND_TRUTH</code> Path to ground truth CSV file"},{"location":"cli/#options_1","title":"Options","text":"Option Default Description <code>--output</code>, <code>-o</code> <code>None</code> Output file for metrics (prints to stdout if not specified)"},{"location":"cli/#examples_1","title":"Examples","text":"<pre><code>locisimiles evaluate results.csv ground_truth.csv\n</code></pre> <p>Save metrics to file:</p> <pre><code>locisimiles evaluate results.csv ground_truth.csv -o metrics.json\n</code></pre>"},{"location":"cli/#input-file-formats","title":"Input File Formats","text":""},{"location":"cli/#sourcetarget-csv","title":"Source/Target CSV","text":"<p>CSV files should contain at minimum an ID column and a text column:</p> <pre><code>id,text\n1,\"Arma virumque cano Troiae qui primus ab oris\"\n2,\"Italiam fato profugus Laviniaque venit\"\n</code></pre>"},{"location":"cli/#ground-truth-csv","title":"Ground Truth CSV","text":"<p>Ground truth files should contain query-reference pairs with labels:</p> <pre><code>query_id,reference_id,label\n1,42,1\n2,15,0\n</code></pre> <p>Where <code>label</code> is <code>1</code> for true matches and <code>0</code> for non-matches.</p>"},{"location":"cli/#output-format","title":"Output Format","text":"<p>The pipeline outputs a CSV with the following columns:</p> Column Description <code>query_id</code> ID of the source text segment <code>reference_id</code> ID of the matched target segment <code>score</code> Similarity/classification score <code>above_threshold</code> Whether the score exceeds the threshold"},{"location":"cli/#environment-variables","title":"Environment Variables","text":"Variable Description <code>LOCISIMILES_CACHE_DIR</code> Directory for model caching <code>LOCISIMILES_DEVICE</code> Device for computation (<code>cpu</code>, <code>cuda</code>, <code>mps</code>)"},{"location":"development/","title":"Development Guide","text":"<p>This guide covers setting up a development environment and contributing to LociSimiles.</p>"},{"location":"development/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10 or higher</li> <li>Git</li> <li>pip package manager</li> </ul>"},{"location":"development/#setup","title":"Setup","text":""},{"location":"development/#clone-the-repository","title":"Clone the Repository","text":"<pre><code>git clone https://github.com/julianschelb/locisimiles.git\ncd locisimiles\n</code></pre>"},{"location":"development/#create-virtual-environment","title":"Create Virtual Environment","text":"<pre><code>python -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n</code></pre>"},{"location":"development/#install-development-dependencies","title":"Install Development Dependencies","text":"<pre><code>pip install -e \".[dev]\"\n</code></pre> <p>This installs the package in editable mode along with development tools:</p> <ul> <li>pytest - Testing framework</li> <li>pytest-cov - Coverage reporting</li> <li>poethepoet - Task runner</li> <li>mkdocs - Documentation</li> <li>mkdocs-material - Documentation theme</li> </ul>"},{"location":"development/#running-tests","title":"Running Tests","text":""},{"location":"development/#using-poe-recommended","title":"Using Poe (Recommended)","text":"<pre><code># Run all tests\npoe test\n\n# Run tests with coverage report\npoe test-cov\n</code></pre>"},{"location":"development/#using-pytest-directly","title":"Using Pytest Directly","text":"<pre><code># Run all tests\npytest\n\n# Run with verbose output\npytest -v\n\n# Run specific test file\npytest tests/test_document.py\n\n# Run specific test\npytest tests/test_document.py::TestDocument::test_from_csv\n\n# Run with coverage\npytest --cov=locisimiles --cov-report=term-missing\n\n# Generate HTML coverage report\npytest --cov=locisimiles --cov-report=html\n</code></pre>"},{"location":"development/#documentation","title":"Documentation","text":""},{"location":"development/#serve-locally","title":"Serve Locally","text":"<pre><code>poe docs\n</code></pre> <p>This starts a local server at <code>http://127.0.0.1:8000</code> with live reload.</p>"},{"location":"development/#build-static-site","title":"Build Static Site","text":"<pre><code>poe docs-build\n</code></pre> <p>Output is written to the <code>site/</code> directory.</p>"},{"location":"development/#deploy-to-github-pages","title":"Deploy to GitHub Pages","text":"<pre><code>poe docs-deploy\n</code></pre> <p>This builds and pushes to the <code>gh-pages</code> branch.</p>"},{"location":"development/#project-structure","title":"Project Structure","text":"<pre><code>locisimiles/\n\u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 locisimiles/\n\u2502       \u251c\u2500\u2500 __init__.py          # Package exports\n\u2502       \u251c\u2500\u2500 cli.py               # Command-line interface\n\u2502       \u251c\u2500\u2500 document.py          # Document and TextSegment classes\n\u2502       \u251c\u2500\u2500 evaluator.py         # Evaluation metrics\n\u2502       \u2514\u2500\u2500 pipeline/\n\u2502           \u251c\u2500\u2500 __init__.py      # Pipeline exports\n\u2502           \u251c\u2500\u2500 _types.py        # Type definitions\n\u2502           \u251c\u2500\u2500 classification.py # Classification pipeline\n\u2502           \u251c\u2500\u2500 retrieval.py     # Retrieval pipeline\n\u2502           \u2514\u2500\u2500 two_stage.py     # Combined pipeline\n\u251c\u2500\u2500 tests/\n\u2502   \u251c\u2500\u2500 conftest.py              # Shared test fixtures\n\u2502   \u251c\u2500\u2500 test_document.py         # Document tests\n\u2502   \u251c\u2500\u2500 test_evaluator.py        # Evaluator tests\n\u2502   \u251c\u2500\u2500 test_cli.py              # CLI tests\n\u2502   \u251c\u2500\u2500 test_types.py            # Type definition tests\n\u2502   \u251c\u2500\u2500 test_retrieval.py        # Retrieval pipeline tests\n\u2502   \u251c\u2500\u2500 test_classification.py   # Classification pipeline tests\n\u2502   \u2514\u2500\u2500 test_two_stage.py        # Two-stage pipeline tests\n\u251c\u2500\u2500 docs/                        # Documentation source\n\u251c\u2500\u2500 examples/                    # Example scripts and notebooks\n\u251c\u2500\u2500 pyproject.toml               # Project configuration\n\u2514\u2500\u2500 mkdocs.yml                   # Documentation configuration\n</code></pre>"},{"location":"development/#code-style","title":"Code Style","text":""},{"location":"development/#general-guidelines","title":"General Guidelines","text":"<ul> <li>Follow PEP 8 style guidelines</li> <li>Use type hints for function signatures</li> <li>Write docstrings for public APIs</li> <li>Keep functions focused and small</li> </ul>"},{"location":"development/#example","title":"Example","text":"<pre><code>def compute_similarity(\n    text_a: str,\n    text_b: str,\n    model: Optional[str] = None\n) -&gt; float:\n    \"\"\"Compute semantic similarity between two texts.\n\n    Args:\n        text_a: First text string.\n        text_b: Second text string.\n        model: Optional model name. Defaults to MiniLM.\n\n    Returns:\n        Similarity score between 0 and 1.\n\n    Raises:\n        ValueError: If either text is empty.\n    \"\"\"\n    if not text_a or not text_b:\n        raise ValueError(\"Texts cannot be empty\")\n    ...\n</code></pre>"},{"location":"development/#testing-guidelines","title":"Testing Guidelines","text":""},{"location":"development/#writing-tests","title":"Writing Tests","text":"<ul> <li>Place tests in the <code>tests/</code> directory</li> <li>Name test files <code>test_&lt;module&gt;.py</code></li> <li>Name test classes <code>Test&lt;ClassName&gt;</code></li> <li>Name test methods <code>test_&lt;behavior&gt;</code></li> </ul>"},{"location":"development/#using-fixtures","title":"Using Fixtures","text":"<p>Shared fixtures are defined in <code>conftest.py</code>:</p> <pre><code>def test_document_loading(sample_csv_file):\n    \"\"\"Test loading document from CSV.\"\"\"\n    doc = Document.from_csv(sample_csv_file)\n    assert len(doc) &gt; 0\n</code></pre>"},{"location":"development/#mocking-external-dependencies","title":"Mocking External Dependencies","text":"<p>Use unittest.mock for ML models:</p> <pre><code>from unittest.mock import MagicMock, patch\n\ndef test_retrieval_with_mock(mock_embedder):\n    \"\"\"Test retrieval with mocked embedding model.\"\"\"\n    pipeline = RetrievalPipeline()\n    pipeline.model = mock_embedder\n    # Test without actual model loading\n</code></pre>"},{"location":"development/#contributing","title":"Contributing","text":""},{"location":"development/#workflow","title":"Workflow","text":"<ol> <li>Fork the repository</li> <li>Create a feature branch: <code>git checkout -b feature/my-feature</code></li> <li>Make changes and add tests</li> <li>Ensure tests pass: <code>poe test</code></li> <li>Commit changes: <code>git commit -m \"Add my feature\"</code></li> <li>Push to fork: <code>git push origin feature/my-feature</code></li> <li>Open a Pull Request</li> </ol>"},{"location":"development/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<ul> <li>Include tests for new functionality</li> <li>Update documentation as needed</li> <li>Keep commits focused and atomic</li> <li>Write clear commit messages</li> </ul>"},{"location":"development/#available-poe-tasks","title":"Available Poe Tasks","text":"Task Command Description <code>test</code> <code>poe test</code> Run all tests <code>test-cov</code> <code>poe test-cov</code> Run tests with coverage <code>docs</code> <code>poe docs</code> Serve documentation locally <code>docs-build</code> <code>poe docs-build</code> Build documentation <code>docs-deploy</code> <code>poe docs-deploy</code> Deploy to GitHub Pages"},{"location":"examples/","title":"Examples","text":"<p>This section provides working examples demonstrating LociSimiles usage.</p>"},{"location":"examples/#sample-data","title":"Sample Data","text":"<p>The examples use sample Latin texts:</p> <ul> <li>Hieronymus samples - Query texts from Jerome's writings</li> <li>Vergil samples - Source texts from Virgil's works</li> <li>Ground truth - Annotated intertextual links for evaluation</li> </ul>"},{"location":"examples/#quick-start-example","title":"Quick Start Example","text":"<pre><code>from locisimiles.document import Document\nfrom locisimiles.evaluator import IntertextEvaluator\nfrom locisimiles.pipeline import (\n    ClassificationPipelineWithCandidategeneration,\n    pretty_print,\n)\n\n# Load example query and source documents\nquery_doc = Document(\"./hieronymus_samples.csv\", author=\"Hieronymus\")\nsource_doc = Document(\"./vergil_samples.csv\", author=\"Vergil\")\n\nprint(\"Loaded query and source documents:\")\nprint(f\"Query Document: {query_doc}\")\nprint(f\"Source Document: {source_doc}\")\nprint(\"=\" * 70)\n\n\n# Load the pipeline with pre-trained models\npipeline_two_stage = ClassificationPipelineWithCandidategeneration(\n    classification_name=\"julian-schelb/PhilBerta-class-latin-intertext-v1\",\n    embedding_model_name=\"julian-schelb/SPhilBerta-emb-lat-intertext-v1\",\n    device=\"mps\",\n)\n\n# Run the pipeline with the query and source documents\nresults_two_stage = pipeline_two_stage.run(\n    query=query_doc,  # Query document\n    source=source_doc,  # Source document\n    top_k=10,  # Number of top similar candidates to classify\n)\nprint(\"\\nResults of the two-stage pipeline run:\")\npretty_print(results_two_stage)\n\nevaluator = IntertextEvaluator(\n    query_doc=query_doc,\n    source_doc=source_doc,\n    ground_truth_csv=\"./ground_truth.csv\",\n    pipeline=pipeline_two_stage,\n    top_k=10,\n    threshold=0.5,\n)\n\nprint(\"\\nSingle sentence:\\n\", evaluator.evaluate_single_query(\"hier. adv. iovin. 1.41\"))\nprint(\"\\nPer-sentence head:\\n\", evaluator.evaluate_all_queries().head(20))\nprint(\"\\nMacro scores:\\n\", evaluator.evaluate(average=\"macro\", with_match_only=True))\nprint(\"\\nMicro scores:\\n\", evaluator.evaluate(average=\"micro\", with_match_only=True))\n</code></pre>"},{"location":"examples/#jupyter-notebook","title":"Jupyter Notebook","text":"<p>For an interactive walkthrough, see the example notebook.</p> <p>The notebook covers:</p> <ol> <li>Loading Documents - Creating Document objects from CSV files</li> <li>Two-Stage Pipeline - Using retrieval + classification</li> <li>Finding Optimal Threshold - Automatic threshold tuning</li> <li>Evaluating Different K Values - Comparing top-k settings</li> <li>Classification-Only Pipeline - Exhaustive pairwise comparison</li> </ol>"},{"location":"examples/#two-stage-pipeline","title":"Two-Stage Pipeline","text":"<p>The recommended approach combines fast retrieval with accurate classification:</p> <pre><code>from locisimiles.pipeline import ClassificationPipelineWithCandidategeneration\nfrom locisimiles.document import Document\n\n# Load documents\nquery_doc = Document(\"./hieronymus_samples.csv\", author=\"Hieronymus\")\nsource_doc = Document(\"./vergil_samples.csv\", author=\"Vergil\")\n\n# Initialize pipeline with pre-trained models\npipeline = ClassificationPipelineWithCandidategeneration(\n    classification_name=\"julian-schelb/PhilBerta-class-latin-intertext-v1\",\n    embedding_model_name=\"julian-schelb/SPhilBerta-emb-lat-intertext-v1\",\n    device=\"cpu\",  # or \"cuda\", \"mps\"\n)\n\n# Run the pipeline\nresults = pipeline.run(\n    query=query_doc,\n    source=source_doc,\n    top_k=10  # Number of candidates per query\n)\n</code></pre>"},{"location":"examples/#modular-pipeline-recommended","title":"Modular Pipeline (Recommended)","text":"<p>Build custom pipelines by composing a generator and a judge:</p> <pre><code>from locisimiles import Pipeline\nfrom locisimiles.pipeline.generator import EmbeddingCandidateGenerator\nfrom locisimiles.pipeline.judge import ClassificationJudge\nfrom locisimiles.document import Document\n\n# Load documents\nquery_doc = Document(\"./hieronymus_samples.csv\", author=\"Hieronymus\")\nsource_doc = Document(\"./vergil_samples.csv\", author=\"Vergil\")\n\n# Compose a custom pipeline\npipeline = Pipeline(\n    generator=EmbeddingCandidateGenerator(\n        embedding_model_name=\"julian-schelb/SPhilBerta-emb-lat-intertext-v1\",\n        device=\"cpu\",\n    ),\n    judge=ClassificationJudge(\n        classification_name=\"julian-schelb/PhilBerta-class-latin-intertext-v1\",\n        device=\"cpu\",\n    ),\n)\n\n# Run end-to-end\nresults = pipeline.run(query=query_doc, source=source_doc, top_k=10)\n\n# Or run stages separately\ncandidates = pipeline.generate_candidates(query=query_doc, source=source_doc, top_k=10)\nresults = pipeline.judge_candidates(query=query_doc, candidates=candidates)\n</code></pre>"},{"location":"examples/#custom-combinations","title":"Custom Combinations","text":"<p>Mix and match generators and judges:</p> <pre><code>from locisimiles import Pipeline\nfrom locisimiles.pipeline.generator import (\n    EmbeddingCandidateGenerator,\n    ExhaustiveCandidateGenerator,\n    RuleBasedCandidateGenerator,\n)\nfrom locisimiles.pipeline.judge import (\n    ClassificationJudge,\n    ThresholdJudge,\n    IdentityJudge,\n)\n\n# Retrieval + threshold (fast, no classifier needed)\npipeline = Pipeline(\n    generator=EmbeddingCandidateGenerator(device=\"cpu\"),\n    judge=ThresholdJudge(top_k=5),\n)\n\n# Rule-based candidates + classification scoring\npipeline = Pipeline(\n    generator=RuleBasedCandidateGenerator(min_shared_words=2),\n    judge=ClassificationJudge(device=\"cpu\"),\n)\n\n# Exhaustive + identity (all pairs, no filtering)\npipeline = Pipeline(\n    generator=ExhaustiveCandidateGenerator(),\n    judge=IdentityJudge(),\n)\n</code></pre>"},{"location":"examples/#saving-results","title":"Saving Results","text":"<p>Pipeline results can be saved to CSV or JSON directly from the pipeline object, or by using the standalone utility functions.</p>"},{"location":"examples/#save-from-the-pipeline","title":"Save from the pipeline","text":"<pre><code># Run the pipeline\nresults = pipeline.run(query=query_doc, source=source_doc, top_k=10)\n\n# Save to CSV (columns: query_id, source_id, source_text, candidate_score, judgment_score)\npipeline.to_csv(\"results.csv\")\n\n# Save to JSON (object keyed by query_id)\npipeline.to_json(\"results.json\")\n</code></pre>"},{"location":"examples/#save-with-explicit-results","title":"Save with explicit results","text":"<pre><code>pipeline.to_csv(\"results.csv\", results=results)\npipeline.to_json(\"results.json\", results=results)\n</code></pre>"},{"location":"examples/#standalone-utility-functions","title":"Standalone utility functions","text":"<p>If you don't have a pipeline instance, use the standalone functions:</p> <pre><code>from locisimiles.pipeline import results_to_csv, results_to_json\n\nresults_to_csv(results, \"results.csv\")\nresults_to_json(results, \"results.json\")\n</code></pre>"},{"location":"examples/#evaluation","title":"Evaluation","text":"<p>Evaluate your results against ground truth annotations:</p> <pre><code>from locisimiles.evaluator import IntertextEvaluator\n\nevaluator = IntertextEvaluator(\n    query_doc=query_doc,\n    source_doc=source_doc,\n    ground_truth_csv=\"./ground_truth.csv\",\n    pipeline=pipeline,\n    top_k=10,\n    threshold=0.5,\n)\n\n# Evaluate a single query\nprint(evaluator.evaluate_single_query(\"hier. adv. iovin. 1.41\"))\n\n# Get metrics for all queries\nprint(evaluator.evaluate(average=\"macro\"))\nprint(evaluator.evaluate(average=\"micro\"))\n</code></pre>"},{"location":"examples/#finding-the-best-threshold","title":"Finding the Best Threshold","text":"<p>Automatically find the optimal probability threshold:</p> <pre><code>best_result, all_thresholds_df = evaluator.find_best_threshold(\n    metric=\"f1\",       # Optimize for F1 (or 'precision', 'recall', 'smr')\n    average=\"micro\",   # Use micro-averaging\n)\n\nprint(f\"Best threshold: {best_result['best_threshold']}\")\nprint(f\"Best F1 score: {best_result['best_f1']:.4f}\")\n</code></pre>"},{"location":"examples/#classification-only-pipeline","title":"Classification-Only Pipeline","text":"<p>For smaller datasets, use exhaustive pairwise comparison:</p> <pre><code>from locisimiles.pipeline import ClassificationPipeline\n\npipeline_clf = ClassificationPipeline(\n    classification_name=\"julian-schelb/PhilBerta-class-latin-intertext-v1\",\n    device=\"cpu\",\n)\n\nresults = pipeline_clf.run(\n    query=query_doc,\n    source=source_doc,\n    batch_size=32,\n)\n\n# Filter high-probability matches\nthreshold = 0.7\nfor query_id, judgments in results.items():\n    high_prob = [j for j in judgments if j.judgment_score &gt; threshold]\n    if high_prob:\n        print(f\"Query {query_id}:\")\n        for j in high_prob:\n            print(f\"  {j.segment.id}: P={j.judgment_score:.3f}\")\n</code></pre>"},{"location":"examples/#running-the-examples","title":"Running the Examples","text":"<p>To run the examples locally:</p> <pre><code>cd examples\npip install -r requirements.txt\npython example.py\n</code></pre> <p>Or open <code>example.ipynb</code> in Jupyter for the interactive version.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This guide will help you get started with LociSimiles for finding intertextual links in Latin literature.</p>"},{"location":"getting-started/#installation","title":"Installation","text":""},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10 or higher</li> <li>pip package manager</li> </ul>"},{"location":"getting-started/#installing-from-pypi","title":"Installing from PyPI","text":"<pre><code>pip install locisimiles\n</code></pre>"},{"location":"getting-started/#installing-from-source","title":"Installing from Source","text":"<pre><code>git clone https://github.com/julianschelb/locisimiles.git\ncd locisimiles\npip install -e \".[dev]\"\n</code></pre>"},{"location":"getting-started/#basic-concepts","title":"Basic Concepts","text":""},{"location":"getting-started/#documents-and-segments","title":"Documents and Segments","text":"<p>LociSimiles works with Documents containing TextSegments. Each segment represents a unit of text (e.g., a verse, sentence, or passage).</p> <pre><code>from locisimiles import Document, TextSegment\n\n# Create segments manually\nsegments = [\n    TextSegment(id=\"1\", text=\"Arma virumque cano\"),\n    TextSegment(id=\"2\", text=\"Troiae qui primus ab oris\"),\n]\n\n# Create a document\ndoc = Document(segments=segments)\n</code></pre>"},{"location":"getting-started/#loading-from-csv","title":"Loading from CSV","text":"<p>Documents are typically loaded from CSV files:</p> <pre><code>doc = Document.from_csv(\"texts.csv\")\n</code></pre> <p>The CSV should have columns for <code>id</code> and <code>text</code> (column names are configurable).</p>"},{"location":"getting-started/#pipelines","title":"Pipelines","text":"<p>LociSimiles provides ready-to-use pipelines for detecting intertextual links. Each pipeline takes a query document and a source document and returns scored matches.</p>"},{"location":"getting-started/#two-stage-pipeline","title":"Two-Stage Pipeline","text":"<p>The recommended pipeline for most use cases. It first retrieves the most promising candidates using embedding similarity, then classifies each candidate pair with a fine-tuned transformer model.</p> <pre><code>from locisimiles import ClassificationPipelineWithCandidategeneration\nfrom locisimiles import Document\n\n# Load documents\nquery = Document(\"query.csv\")\nsource = Document(\"source.csv\")\n\n# Define pipeline\npipeline = ClassificationPipelineWithCandidategeneration(\n    classification_name=\"julian-schelb/PhilBerta-class-latin-intertext-v1\",\n    embedding_model_name=\"julian-schelb/SPhilBerta-emb-lat-intertext-v1\",\n    device=\"cpu\",  # or \"cuda\", \"mps\"\n)\n\n# Run pipeline\nresults = pipeline.run(query=query, source=source, top_k=10)\n</code></pre>"},{"location":"getting-started/#classification-pipeline","title":"Classification Pipeline","text":"<p>Classifies every possible query\u2013source pair using a fine-tuned sequence-classification model. More thorough but slower \u2014 best suited for smaller datasets.</p> <pre><code>from locisimiles import ClassificationPipeline\nfrom locisimiles import Document\n\n# Load documents\nquery = Document(\"query.csv\")\nsource = Document(\"source.csv\")\n\n# Define pipeline\npipeline = ClassificationPipeline(\n    classification_name=\"julian-schelb/PhilBerta-class-latin-intertext-v1\",\n    device=\"cpu\",\n)\n\n# Run pipeline\nresults = pipeline.run(query=query, source=source, batch_size=32)\n</code></pre>"},{"location":"getting-started/#retrieval-pipeline","title":"Retrieval Pipeline","text":"<p>A fast, lightweight pipeline that ranks source segments by embedding similarity and applies a top-k or threshold criterion. No classification model needed.</p> <pre><code>from locisimiles import RetrievalPipeline\nfrom locisimiles import Document\n\n# Load documents\nquery = Document(\"query.csv\")\nsource = Document(\"source.csv\")\n\n# Define pipeline\npipeline = RetrievalPipeline(\n    embedding_model_name=\"julian-schelb/SPhilBerta-emb-lat-intertext-v1\",\n    device=\"cpu\",\n)\n\n# Run pipeline\nresults = pipeline.run(query=query, source=source, top_k=5)\n</code></pre>"},{"location":"getting-started/#rule-based-pipeline","title":"Rule-Based Pipeline","text":"<p>A purely lexical pipeline that does not require any neural models. It finds shared words between segments and applies distance, punctuation, and optional POS / similarity filters.</p> <pre><code>from locisimiles import RuleBasedPipeline\nfrom locisimiles import Document\n\n# Load documents\nquery = Document(\"query.csv\")\nsource = Document(\"source.csv\")\n\n# Define pipeline\npipeline = RuleBasedPipeline(min_shared_words=2, max_distance=3)\n\n# Run pipeline\nresults = pipeline.run(query=query, source=source)\n</code></pre>"},{"location":"getting-started/#pipeline-summary","title":"Pipeline Summary","text":"Pipeline Speed Models required Best for <code>ClassificationPipelineWithCandidategeneration</code> Medium Embedding + classifier Most use cases <code>ClassificationPipeline</code> Slow Classifier Small datasets, exhaustive comparison <code>RetrievalPipeline</code> Fast Embedding Quick similarity search <code>RuleBasedPipeline</code> Fast None No GPU, lexical matching"},{"location":"getting-started/#saving-results","title":"Saving Results","text":"<p>Save pipeline output to CSV or JSON:</p> <pre><code>results = pipeline.run(query=query, source=source, top_k=10)\n\n# Save directly from the pipeline\npipeline.to_csv(\"results.csv\")\npipeline.to_json(\"results.json\")\n\n# Or use standalone utility functions\nfrom locisimiles.pipeline import results_to_csv, results_to_json\nresults_to_csv(results, \"results.csv\")\nresults_to_json(results, \"results.json\")\n</code></pre>"},{"location":"getting-started/#building-custom-pipelines","title":"Building Custom Pipelines","text":"<p>For advanced use cases you can compose your own pipeline from individual generators and judges using the generic <code>Pipeline</code> class.</p> <p>A generator selects candidate source segments for each query segment. A judge then scores or classifies each candidate pair.</p>"},{"location":"getting-started/#available-generators","title":"Available Generators","text":"Generator Description <code>EmbeddingCandidateGenerator</code> Semantic similarity via sentence transformers + ChromaDB <code>ExhaustiveCandidateGenerator</code> All pairs \u2014 no filtering <code>RuleBasedCandidateGenerator</code> Lexical matching + linguistic filters"},{"location":"getting-started/#available-judges","title":"Available Judges","text":"Judge Description <code>ClassificationJudge</code> Transformer sequence classification (P(positive)) <code>ThresholdJudge</code> Binary decisions from candidate scores (top-k or threshold) <code>IdentityJudge</code> Pass-through \u2014 <code>judgment_score = 1.0</code>"},{"location":"getting-started/#example","title":"Example","text":"<pre><code>from locisimiles import Pipeline\nfrom locisimiles.pipeline.generator import EmbeddingCandidateGenerator\nfrom locisimiles.pipeline.judge import ClassificationJudge\n\npipeline = Pipeline(\n    generator=EmbeddingCandidateGenerator(device=\"cpu\"),\n    judge=ClassificationJudge(device=\"cpu\"),\n)\n\nresults = pipeline.run(query=query_doc, source=source_doc, top_k=10)\n\n# You can also run each stage separately\ncandidates = pipeline.generate_candidates(query=query_doc, source=source_doc, top_k=10)\nresults = pipeline.judge_candidates(query=query_doc, candidates=candidates)\n</code></pre>"},{"location":"getting-started/#evaluation","title":"Evaluation","text":"<p>Use the <code>IntertextEvaluator</code> to assess detection quality:</p> <pre><code>from locisimiles import IntertextEvaluator\n\nevaluator = IntertextEvaluator(\n    predictions=predictions,\n    ground_truth=ground_truth\n)\n\nmetrics = evaluator.evaluate()\nprint(f\"Precision: {metrics['precision']:.3f}\")\nprint(f\"Recall: {metrics['recall']:.3f}\")\nprint(f\"F1: {metrics['f1']:.3f}\")\n</code></pre>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<ul> <li>See the CLI Reference for command-line usage</li> <li>Explore the API Reference for detailed documentation</li> <li>Check out the examples for complete workflows</li> </ul>"},{"location":"api/","title":"API Reference","text":"<p>This section provides detailed documentation for the LociSimiles Python API, auto-generated from source code docstrings.</p>"},{"location":"api/#core-modules","title":"Core Modules","text":""},{"location":"api/#document-module","title":"Document Module","text":"<p>The Document module provides classes for representing and loading text collections:</p> <ul> <li><code>TextSegment</code> - Individual text unit with ID and content</li> <li><code>Document</code> - Container for text segments</li> </ul>"},{"location":"api/#pipeline-module","title":"Pipeline Module","text":"<p>The Pipelines module provides the main processing pipelines:</p> <ul> <li><code>Pipeline</code> - Generic composer: combine any generator + judge</li> <li><code>RetrievalPipeline</code> - Semantic similarity retrieval</li> <li><code>ClassificationPipeline</code> - Text pair classification</li> <li><code>ClassificationPipelineWithCandidategeneration</code> - Two-stage retrieval + classification</li> <li><code>RuleBasedPipeline</code> - Lexical matching + linguistic filters</li> </ul>"},{"location":"api/#generators-module","title":"Generators Module","text":"<p>The Generators module provides candidate-generation components:</p> <ul> <li><code>EmbeddingCandidateGenerator</code> - Semantic embedding similarity</li> <li><code>ExhaustiveCandidateGenerator</code> - All-pairs (no filtering)</li> <li><code>RuleBasedCandidateGenerator</code> - Lexical matching + linguistic filters</li> </ul>"},{"location":"api/#judges-module","title":"Judges Module","text":"<p>The Judges module provides scoring/classification components:</p> <ul> <li><code>ClassificationJudge</code> - Transformer-based sequence classification</li> <li><code>ThresholdJudge</code> - Binary decisions from candidate scores</li> <li><code>IdentityJudge</code> - Pass-through (judgment_score = 1.0)</li> </ul>"},{"location":"api/#evaluator-module","title":"Evaluator Module","text":"<p>The Evaluator module provides tools for assessing detection quality:</p> <ul> <li><code>IntertextEvaluator</code> - Main evaluation class</li> </ul>"},{"location":"api/#quick-reference","title":"Quick Reference","text":""},{"location":"api/#loading-documents","title":"Loading Documents","text":"<pre><code>from locisimiles import Document\n\ndoc = Document(\"texts.csv\")\n</code></pre>"},{"location":"api/#saving-results","title":"Saving Results","text":"<pre><code># Save from a pipeline instance\nresults = pipeline.run(query=query_doc, source=source_doc, top_k=10)\npipeline.to_csv(\"results.csv\")\npipeline.to_json(\"results.json\")\n\n# Or use standalone functions\nfrom locisimiles.pipeline import results_to_csv, results_to_json\nresults_to_csv(results, \"results.csv\")\nresults_to_json(results, \"results.json\")\n</code></pre>"},{"location":"api/#evaluating-results","title":"Evaluating Results","text":"<pre><code>from locisimiles import IntertextEvaluator\n\nevaluator = IntertextEvaluator(predictions, ground_truth)\nmetrics = evaluator.evaluate()\n</code></pre>"},{"location":"api/custom-pipelines/","title":"Custom Pipelines","text":"<p>Build your own pipeline by combining any generator with any judge.</p>"},{"location":"api/custom-pipelines/#pipeline","title":"Pipeline","text":""},{"location":"api/custom-pipelines/#locisimiles.pipeline.pipeline.Pipeline","title":"locisimiles.pipeline.pipeline.Pipeline","text":"<pre><code>Pipeline(\n    generator: CandidateGeneratorBase, judge: JudgeBase\n)\n</code></pre> <p>Compose a candidate generator and a judge into a full pipeline.</p> <p>This is the recommended way to build custom pipelines.  Any <code>CandidateGeneratorBase</code> can be paired with any <code>JudgeBase</code>.</p> PARAMETER DESCRIPTION <code>generator</code> <p>Candidate-generation component.</p> <p> TYPE: <code>CandidateGeneratorBase</code> </p> <code>judge</code> <p>Scoring / classification component.</p> <p> TYPE: <code>JudgeBase</code> </p> Example <pre><code>from locisimiles.pipeline import Pipeline\nfrom locisimiles.pipeline.generator import EmbeddingCandidateGenerator\nfrom locisimiles.pipeline.judge import ClassificationJudge\nfrom locisimiles.document import Document\n\n# Load documents\nquery = Document(\"query.csv\")\nsource = Document(\"source.csv\")\n\n# Build a custom pipeline\npipeline = Pipeline(\n    generator=EmbeddingCandidateGenerator(device=\"cpu\"),\n    judge=ClassificationJudge(device=\"cpu\"),\n)\n\n# Run pipeline\nresults = pipeline.run(query=query, source=source, top_k=10)\n\n# Save results\npipeline.to_csv(\"results.csv\")\npipeline.to_json(\"results.json\")\n</code></pre>"},{"location":"api/custom-pipelines/#locisimiles.pipeline.pipeline.Pipeline.generate_candidates","title":"generate_candidates","text":"<pre><code>generate_candidates(\n    *, query: Document, source: Document, **kwargs: Any\n) -&gt; CandidateGeneratorOutput\n</code></pre> <p>Run only the candidate-generation stage.</p> PARAMETER DESCRIPTION <code>query</code> <p>Query document.</p> <p> TYPE: <code>Document</code> </p> <code>source</code> <p>Source document.</p> <p> TYPE: <code>Document</code> </p> <code>**kwargs</code> <p>Forwarded to the generator's <code>generate()</code> method.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>CandidateGeneratorOutput</code> <p><code>CandidateGeneratorOutput</code> mapping query IDs \u2192 <code>Candidate</code> lists.</p>"},{"location":"api/custom-pipelines/#locisimiles.pipeline.pipeline.Pipeline.judge_candidates","title":"judge_candidates","text":"<pre><code>judge_candidates(\n    *,\n    query: Document,\n    candidates: CandidateGeneratorOutput,\n    **kwargs: Any,\n) -&gt; CandidateJudgeOutput\n</code></pre> <p>Run only the judgment stage on pre-generated candidates.</p> PARAMETER DESCRIPTION <code>query</code> <p>Query document.</p> <p> TYPE: <code>Document</code> </p> <code>candidates</code> <p>Output from a candidate generator.</p> <p> TYPE: <code>CandidateGeneratorOutput</code> </p> <code>**kwargs</code> <p>Forwarded to the judge's <code>judge()</code> method.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>CandidateJudgeOutput</code> <p><code>CandidateJudgeOutput</code> mapping query IDs \u2192 <code>CandidateJudge</code> lists.</p>"},{"location":"api/custom-pipelines/#locisimiles.pipeline.pipeline.Pipeline.run","title":"run","text":"<pre><code>run(\n    *, query: Document, source: Document, **kwargs: Any\n) -&gt; CandidateJudgeOutput\n</code></pre> <p>Run both stages: generate candidates then judge them.</p> <p>All kwargs are forwarded to both the generator and the judge; each component ignores keys it does not recognise.</p> PARAMETER DESCRIPTION <code>query</code> <p>Query document.</p> <p> TYPE: <code>Document</code> </p> <code>source</code> <p>Source document.</p> <p> TYPE: <code>Document</code> </p> <code>**kwargs</code> <p>Forwarded to both stages.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>CandidateJudgeOutput</code> <p><code>CandidateJudgeOutput</code> with judgment scores.</p>"},{"location":"api/custom-pipelines/#locisimiles.pipeline.pipeline.Pipeline.to_csv","title":"to_csv","text":"<pre><code>to_csv(\n    path: Union[str, Path],\n    results: CandidateJudgeOutput | None = None,\n) -&gt; None\n</code></pre> <p>Save pipeline results to a CSV file.</p> <p>If results is <code>None</code>, the results from the last <code>run()</code> call are used.</p> <p>Columns: <code>query_id</code>, <code>source_id</code>, <code>source_text</code>, <code>candidate_score</code>, <code>judgment_score</code>.</p> PARAMETER DESCRIPTION <code>path</code> <p>Destination file path (e.g. <code>\"results.csv\"</code>).</p> <p> TYPE: <code>Union[str, Path]</code> </p> <code>results</code> <p>Explicit results to save.  Defaults to the last <code>run()</code> output.</p> <p> TYPE: <code>CandidateJudgeOutput | None</code> DEFAULT: <code>None</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If no results are available.</p> Example <pre><code>results = pipeline.run(query=query_doc, source=source_doc)\npipeline.to_csv(\"results.csv\")\n</code></pre>"},{"location":"api/custom-pipelines/#locisimiles.pipeline.pipeline.Pipeline.to_json","title":"to_json","text":"<pre><code>to_json(\n    path: Union[str, Path],\n    results: CandidateJudgeOutput | None = None,\n    *,\n    indent: int = 2,\n) -&gt; None\n</code></pre> <p>Save pipeline results to a JSON file.</p> <p>If results is <code>None</code>, the results from the last <code>run()</code> call are used.</p> <p>Produces a JSON object keyed by query segment ID, where each value is a list of match objects with <code>source_id</code>, <code>source_text</code>, <code>candidate_score</code>, and <code>judgment_score</code>.</p> PARAMETER DESCRIPTION <code>path</code> <p>Destination file path (e.g. <code>\"results.json\"</code>).</p> <p> TYPE: <code>Union[str, Path]</code> </p> <code>results</code> <p>Explicit results to save.  Defaults to the last <code>run()</code> output.</p> <p> TYPE: <code>CandidateJudgeOutput | None</code> DEFAULT: <code>None</code> </p> <code>indent</code> <p>JSON indentation level (default <code>2</code>).</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If no results are available.</p> Example <pre><code>results = pipeline.run(query=query_doc, source=source_doc)\npipeline.to_json(\"results.json\")\n</code></pre>"},{"location":"api/custom-pipelines/#type-definitions","title":"Type Definitions","text":"<p>Data classes and type aliases used across all pipelines.</p>"},{"location":"api/custom-pipelines/#locisimiles.pipeline._types","title":"locisimiles.pipeline._types","text":"<p>Shared type definitions and utilities for pipeline modules.</p> <p>This module defines the common data structures used across all pipeline implementations for representing detection results.</p>"},{"location":"api/custom-pipelines/#locisimiles.pipeline._types--pipeline-architecture","title":"Pipeline Architecture","text":"<p>Every pipeline follows a two-phase pattern:</p> <ol> <li>Candidate Generation \u2014 narrows the search space, producing a    <code>CandidateGeneratorOutput</code> (mapping of query IDs \u2192 <code>Candidate</code> lists).</li> <li>Judgment \u2014 scores or classifies candidate pairs, producing a    <code>CandidateJudgeOutput</code> (mapping of query IDs \u2192 <code>CandidateJudge</code> lists).</li> </ol>"},{"location":"api/custom-pipelines/#locisimiles.pipeline._types.Candidate","title":"Candidate  <code>dataclass</code>","text":"<pre><code>Candidate(segment: TextSegment, score: float)\n</code></pre> <p>A single candidate match produced by a candidate-generation stage.</p> ATTRIBUTE DESCRIPTION <code>segment</code> <p>The matching source segment.</p> <p> TYPE: <code>TextSegment</code> </p> <code>score</code> <p>Relevance score (e.g. cosine similarity, shared-word ratio).</p> <p> TYPE: <code>float</code> </p> Example <pre><code>from locisimiles.pipeline import Candidate\nfrom locisimiles.document import TextSegment\n\ncandidate = Candidate(\n    segment=TextSegment(\"Arma virumque cano\", seg_id=\"verg. aen. 1.1\"),\n    score=0.85,\n)\nprint(candidate.segment.id)  # \"verg. aen. 1.1\"\nprint(candidate.score)       # 0.85\n</code></pre>"},{"location":"api/custom-pipelines/#locisimiles.pipeline._types.CandidateJudge","title":"CandidateJudge  <code>dataclass</code>","text":"<pre><code>CandidateJudge(\n    segment: TextSegment,\n    candidate_score: Optional[float],\n    judgment_score: float,\n)\n</code></pre> <p>A single scored candidate after the judgment (classification / filtering) stage.</p> ATTRIBUTE DESCRIPTION <code>segment</code> <p>The matching source segment.</p> <p> TYPE: <code>TextSegment</code> </p> <code>candidate_score</code> <p>Score from candidate generation (<code>None</code> when the generator is exhaustive, i.e. all pairs are candidates).</p> <p> TYPE: <code>Optional[float]</code> </p> <code>judgment_score</code> <p>Final judgment value \u2014 e.g. a classification probability, a binary 1.0/0.0 decision, or a rule-based score.</p> <p> TYPE: <code>float</code> </p> Example <pre><code>from locisimiles.pipeline import CandidateJudge\nfrom locisimiles.document import TextSegment\n\nresult = CandidateJudge(\n    segment=TextSegment(\"Arma virumque cano\", seg_id=\"verg. aen. 1.1\"),\n    candidate_score=0.85,\n    judgment_score=0.95,\n)\nprint(result.judgment_score)  # 0.95\n</code></pre>"},{"location":"api/custom-pipelines/#locisimiles.pipeline._types.CandidateGeneratorOutput","title":"CandidateGeneratorOutput  <code>module-attribute</code>","text":"<pre><code>CandidateGeneratorOutput = Dict[str, List[Candidate]]\n</code></pre> <p>Mapping from query segment IDs \u2192 ranked lists of <code>Candidate</code> objects.</p> <p>This is the output type of every candidate-generation stage.</p>"},{"location":"api/custom-pipelines/#locisimiles.pipeline._types.CandidateJudgeOutput","title":"CandidateJudgeOutput  <code>module-attribute</code>","text":"<pre><code>CandidateJudgeOutput = Dict[str, List[CandidateJudge]]\n</code></pre> <p>Mapping from query segment IDs \u2192 lists of <code>CandidateJudge</code> objects.</p> <p>This is the standard output type of every pipeline's <code>run()</code> method and is consumed by the evaluator.</p>"},{"location":"api/custom-pipelines/#locisimiles.pipeline._types.CandidateJudgeInput","title":"CandidateJudgeInput  <code>module-attribute</code>","text":"<pre><code>CandidateJudgeInput = CandidateGeneratorOutput\n</code></pre> <p>Alias: the judge receives exactly what the generator produced.</p>"},{"location":"api/custom-pipelines/#locisimiles.pipeline._types.pretty_print","title":"pretty_print","text":"<pre><code>pretty_print(results: CandidateJudgeOutput) -&gt; None\n</code></pre> <p>Print pipeline results in a human-readable format.</p> <p>Displays each query segment and its candidate matches with candidate scores and judgment scores.</p> PARAMETER DESCRIPTION <code>results</code> <p>Pipeline output in <code>CandidateJudgeOutput</code> format.</p> <p> TYPE: <code>CandidateJudgeOutput</code> </p> Example <pre><code>from locisimiles.pipeline import pretty_print\n\nresults = pipeline.run(query=query_doc, source=source_doc, top_k=5)\npretty_print(results)\n\n# Output:\n# \u25b6 Query segment 'hier. adv. iovin. 1.41':\n#   verg. aen. 1.1              candidate=+0.823  judgment=0.951\n#   verg. aen. 2.45             candidate=+0.654  judgment=0.234\n</code></pre>"},{"location":"api/custom-pipelines/#locisimiles.pipeline._types.results_to_csv","title":"results_to_csv","text":"<pre><code>results_to_csv(\n    results: CandidateJudgeOutput, path: Union[str, Path]\n) -&gt; None\n</code></pre> <p>Save pipeline results to a CSV file.</p> <p>Writes one row per query-source match with the following columns:</p> <ul> <li><code>query_id</code> - identifier of the query segment.</li> <li><code>source_id</code> - identifier of the matching source segment.</li> <li><code>source_text</code> - raw text of the source segment.</li> <li><code>candidate_score</code> - score from the candidate-generation stage   (empty when not available).</li> <li><code>judgment_score</code> - final judgment / classification score.</li> </ul> PARAMETER DESCRIPTION <code>results</code> <p>Pipeline output in <code>CandidateJudgeOutput</code> format.</p> <p> TYPE: <code>CandidateJudgeOutput</code> </p> <code>path</code> <p>Destination file path (e.g. <code>\"results.csv\"</code>).</p> <p> TYPE: <code>Union[str, Path]</code> </p> Example <pre><code>from locisimiles.pipeline import results_to_csv\n\nresults = pipeline.run(query=query_doc, source=source_doc, top_k=5)\nresults_to_csv(results, \"results.csv\")\n</code></pre>"},{"location":"api/custom-pipelines/#locisimiles.pipeline._types.results_to_json","title":"results_to_json","text":"<pre><code>results_to_json(\n    results: CandidateJudgeOutput,\n    path: Union[str, Path],\n    *,\n    indent: int = 2,\n) -&gt; None\n</code></pre> <p>Save pipeline results to a JSON file.</p> <p>Produces a JSON object keyed by query segment ID.  Each value is a list of match objects with <code>source_id</code>, <code>source_text</code>, <code>candidate_score</code>, and <code>judgment_score</code>.</p> PARAMETER DESCRIPTION <code>results</code> <p>Pipeline output in <code>CandidateJudgeOutput</code> format.</p> <p> TYPE: <code>CandidateJudgeOutput</code> </p> <code>path</code> <p>Destination file path (e.g. <code>\"results.json\"</code>).</p> <p> TYPE: <code>Union[str, Path]</code> </p> <code>indent</code> <p>JSON indentation level (default <code>2</code>).</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> Example <pre><code>from locisimiles.pipeline import results_to_json\n\nresults = pipeline.run(query=query_doc, source=source_doc, top_k=5)\nresults_to_json(results, \"results.json\")\n</code></pre>"},{"location":"api/document/","title":"Document Module","text":"<p>Classes for representing and loading text collections.</p>"},{"location":"api/document/#textsegment","title":"TextSegment","text":"<p>An individual unit of text with an identifier.</p>"},{"location":"api/document/#locisimiles.document.TextSegment","title":"locisimiles.document.TextSegment","text":"<pre><code>TextSegment(\n    text: str,\n    seg_id: ID,\n    *,\n    row_id: int | None = None,\n    meta: Dict[str, Any] | None = None,\n)\n</code></pre> <p>Atomic unit of text inside a document.</p> <p>A TextSegment represents a single passage, sentence, or verse from a larger document. Each segment has a unique identifier and optional metadata.</p> ATTRIBUTE DESCRIPTION <code>text</code> <p>The raw text content of the segment.</p> <p> TYPE: <code>str</code> </p> <code>id</code> <p>Unique identifier for the segment (e.g., \"verg. aen. 1.1\").</p> <p> TYPE: <code>ID</code> </p> <code>row_id</code> <p>Position of the segment in the original document (0-indexed).</p> <p> TYPE: <code>int | None</code> </p> <code>meta</code> <p>Optional dictionary of additional metadata.</p> <p> TYPE: <code>Dict[str, Any]</code> </p> Example <pre><code>segment = TextSegment(\n    text=\"Arma virumque cano, Troiae qui primus ab oris\",\n    seg_id=\"verg. aen. 1.1\",\n    row_id=0,\n    meta={\"book\": 1, \"line\": 1}\n)\nprint(segment.text)  # \"Arma virumque cano...\"\nprint(segment.id)    # \"verg. aen. 1.1\"\n</code></pre>"},{"location":"api/document/#document","title":"Document","text":"<p>A collection of text segments with loading utilities.</p>"},{"location":"api/document/#locisimiles.document.Document","title":"locisimiles.document.Document","text":"<pre><code>Document(\n    path: str | Path,\n    *,\n    author: str | None = None,\n    meta: Dict[str, Any] | None = None,\n    segment_delimiter: str = \"\\n\",\n)\n</code></pre> <p>Collection of text segments representing a document.</p> <p>A Document is a container for TextSegments loaded from a file. It supports CSV/TSV files with 'seg_id' and 'text' columns, or plain text files where segments are separated by a delimiter.</p> ATTRIBUTE DESCRIPTION <code>path</code> <p>Path to the source file.</p> <p> TYPE: <code>Path</code> </p> <code>author</code> <p>Optional author name for the document.</p> <p> TYPE: <code>str | None</code> </p> <code>meta</code> <p>Optional dictionary of document-level metadata.</p> <p> TYPE: <code>Dict[str, Any]</code> </p> Example <pre><code>from locisimiles.document import Document\n\n# Load from CSV (must have 'seg_id' and 'text' columns)\nvergil = Document(\"vergil_samples.csv\", author=\"Vergil\")\n\n# Access segments\nprint(len(vergil))           # Number of segments\nprint(vergil.ids())          # List of segment IDs\nprint(vergil.get_text(\"verg. aen. 1.1\"))  # Get text by ID\n\n# Iterate over segments\nfor segment in vergil:\n    print(f\"{segment.id}: {segment.text[:50]}...\")\n\n# Add custom segments\nvergil.add_segment(\n    text=\"Custom text\",\n    seg_id=\"custom.1\",\n    meta={\"source\": \"manual\"}\n)\n</code></pre>"},{"location":"api/document/#locisimiles.document.Document.ids","title":"ids","text":"<pre><code>ids() -&gt; List[ID]\n</code></pre> <p>Return segment IDs in original order.</p>"},{"location":"api/document/#locisimiles.document.Document.get_text","title":"get_text","text":"<pre><code>get_text(seg_id: ID) -&gt; str\n</code></pre> <p>Return raw text of a segment.</p>"},{"location":"api/document/#locisimiles.document.Document.add_segment","title":"add_segment","text":"<pre><code>add_segment(\n    text: str,\n    seg_id: ID,\n    *,\n    row_id: int | None = None,\n    meta: Dict[str, Any] | None = None,\n) -&gt; None\n</code></pre> <p>Add a new text segment to the document.</p>"},{"location":"api/document/#locisimiles.document.Document.remove_segment","title":"remove_segment","text":"<pre><code>remove_segment(seg_id: ID) -&gt; None\n</code></pre> <p>Delete a segment if present.</p>"},{"location":"api/document/#locisimiles.document.Document.statistics","title":"statistics","text":"<pre><code>statistics() -&gt; Dict[str, Any]\n</code></pre> <p>Return descriptive statistics (segment count, char/word totals, averages, min/max).</p>"},{"location":"api/document/#locisimiles.document.Document.sentencize","title":"sentencize","text":"<pre><code>sentencize(\n    *,\n    splitter: Optional[Callable[[str], List[str]]] = None,\n    id_separator: str = \".\",\n) -&gt; Document\n</code></pre> <p>Re-segment this document so that each segment contains exactly one sentence.</p> <p>All segment texts are first joined (in row-id order) and then sentence-split as a single block.  This correctly handles:</p> <ul> <li>Segments containing multiple sentences \u2192 split into separate   segments.</li> <li>A single sentence spanning multiple rows \u2192 merged into one   segment.</li> </ul> <p>New segment IDs are derived from the original segment whose text starts the sentence, with a numeric suffix appended (e.g. <code>\"seg1.1\"</code>, <code>\"seg1.2\"</code>).</p> PARAMETER DESCRIPTION <code>splitter</code> <p>A callable that takes a <code>str</code> and returns a list of sentence strings.  When <code>None</code> a simple punctuation-based splitter is used.  To use spaCy::</p> <pre><code>import spacy\nnlp = spacy.load(\"la_core_web_lg\")\ndoc.sentencize(splitter=lambda t: [s.text for s in nlp(t).sents])\n</code></pre> <p> TYPE: <code>Optional[Callable[[str], List[str]]]</code> DEFAULT: <code>None</code> </p> <code>id_separator</code> <p>Separator inserted between the original segment ID and the sentence index when a segment is split (e.g. <code>\"seg1\"</code> \u2192 <code>\"seg1.1\"</code>, <code>\"seg1.2\"</code>).</p> <p> TYPE: <code>str</code> DEFAULT: <code>'.'</code> </p> RETURNS DESCRIPTION <code>Document</code> <p>The modified <code>Document</code> with one sentence per segment.</p> Example <pre><code>doc = Document(\"mixed.csv\")\ndoc.sentencize()\n</code></pre>"},{"location":"api/document/#locisimiles.document.Document.save_plain","title":"save_plain","text":"<pre><code>save_plain(\n    path: str | Path, *, delimiter: str = \"\\n\"\n) -&gt; Path\n</code></pre> <p>Write all segment texts to a plain-text file.</p>"},{"location":"api/document/#locisimiles.document.Document.save_csv","title":"save_csv","text":"<pre><code>save_csv(path: str | Path) -&gt; Path\n</code></pre> <p>Write all segments to a CSV file with <code>seg_id</code> and <code>text</code> columns.</p>"},{"location":"api/evaluator/","title":"Evaluator Module","text":"<p>Tools for assessing detection quality.</p>"},{"location":"api/evaluator/#intertextevaluator","title":"IntertextEvaluator","text":"<p>Evaluate detection results against ground truth annotations.</p> <p>Computes precision, recall, F1, and other metrics for intertextual link detection.</p>"},{"location":"api/evaluator/#locisimiles.evaluator.IntertextEvaluator","title":"locisimiles.evaluator.IntertextEvaluator","text":"<pre><code>IntertextEvaluator(\n    *,\n    query_doc: Document,\n    source_doc: Document,\n    ground_truth_csv: str | DataFrame,\n    pipeline: Pipeline,\n    top_k: int = 5,\n    threshold: float | str = \"auto\",\n    auto_threshold_metric: str = \"smr\",\n)\n</code></pre> <p>Evaluator for measuring intertextuality detection performance.</p> <p>This class computes sentence-level and document-level evaluation metrics by comparing pipeline predictions against ground truth annotations.</p> Supported metrics <ul> <li>Precision: TP / (TP + FP)</li> <li>Recall: TP / (TP + FN)</li> <li>F1: Harmonic mean of precision and recall</li> <li>SMR: Source Match Rate (error rate)</li> <li>Accuracy: (TP + TN) / Total</li> </ul> <p>The evaluator runs the pipeline once during initialization and caches the results for efficient metric computation across different thresholds.</p> ATTRIBUTE DESCRIPTION <code>query_doc</code> <p>The query document being analyzed.</p> <p> </p> <code>source_doc</code> <p>The source document containing potential quotation origins.</p> <p> </p> <code>predictions</code> <p>Cached pipeline predictions (CandidateJudgeOutput format).</p> <p> TYPE: <code>CandidateJudgeOutput</code> </p> <code>threshold</code> <p>Probability threshold for positive classification.</p> <p> </p> <code>gold_labels</code> <p>Ground truth annotations loaded from CSV.</p> <p> </p> Example <pre><code>from locisimiles.evaluator import IntertextEvaluator\nfrom locisimiles.pipeline import TwoStagePipeline\nfrom locisimiles.document import Document\n\n# Load documents\nquery_doc = Document(\"hieronymus.csv\")\nsource_doc = Document(\"vergil.csv\")\n\n# Initialize pipeline\npipeline = TwoStagePipeline(device=\"cpu\")\n\n# Create evaluator with auto-threshold\nevaluator = IntertextEvaluator(\n    query_doc=query_doc,\n    source_doc=source_doc,\n    ground_truth_csv=\"ground_truth.csv\",\n    pipeline=pipeline,\n    top_k=10,\n    threshold=\"auto\",  # Automatically find best threshold\n    auto_threshold_metric=\"smr\",\n)\n\n# Get evaluation metrics\nprint(evaluator.evaluate(average=\"micro\"))\nprint(evaluator.evaluate(average=\"macro\"))\n\n# Evaluate single query\nprint(evaluator.evaluate_single_query(\"hier. adv. iovin. 1.41\"))\n\n# Find optimal threshold for different metrics\nbest, all_thresholds = evaluator.find_best_threshold(metric=\"f1\")\nprint(f\"Best F1 at threshold {best['best_threshold']}: {best['best_f1']:.3f}\")\n</code></pre>"},{"location":"api/evaluator/#locisimiles.evaluator.IntertextEvaluator.evaluate_single_query","title":"evaluate_single_query","text":"<pre><code>evaluate_single_query(query_id: str) -&gt; Dict[str, float]\n</code></pre> <p>Compute metrics for one query sentence.</p>"},{"location":"api/evaluator/#locisimiles.evaluator.IntertextEvaluator.query_ids_with_match","title":"query_ids_with_match","text":"<pre><code>query_ids_with_match() -&gt; List[str]\n</code></pre> <p>Return query IDs that have ground truth labels.</p>"},{"location":"api/evaluator/#locisimiles.evaluator.IntertextEvaluator.evaluate_all_queries","title":"evaluate_all_queries","text":"<pre><code>evaluate_all_queries(\n    with_match_only: bool = False,\n) -&gt; DataFrame\n</code></pre> <p>Compute metrics for every query sentence (cached).</p>"},{"location":"api/evaluator/#locisimiles.evaluator.IntertextEvaluator.evaluate","title":"evaluate","text":"<pre><code>evaluate(\n    *, average: str = \"macro\", with_match_only: bool = False\n) -&gt; Dict[str, float]\n</code></pre> <p>Compute aggregated metrics across queries.</p> <ul> <li>Precision, Recall, F1, Accuracy: ALWAYS computed on queries with at least   one ground truth match (otherwise these metrics are meaningless).</li> <li>FPR, FNR, SMR: Computed on ALL queries by default (measures false alarms   on queries that shouldn't have matches). If with_match_only=True, these   are also restricted to queries with matches.</li> </ul>"},{"location":"api/evaluator/#locisimiles.evaluator.IntertextEvaluator.confusion_matrix","title":"confusion_matrix","text":"<pre><code>confusion_matrix(query_id: str) -&gt; ndarray\n</code></pre> <p>Return 2x2 confusion matrix [[TP,FP],[FN,TN]] for one query sentence.</p>"},{"location":"api/evaluator/#locisimiles.evaluator.IntertextEvaluator.find_best_threshold","title":"find_best_threshold","text":"<pre><code>find_best_threshold(\n    *,\n    metric: str = \"f1\",\n    thresholds: List[float] | None = None,\n    average: str = \"micro\",\n    with_match_only: bool = False,\n) -&gt; Tuple[Dict[str, float], DataFrame]\n</code></pre> <p>Find the optimal probability threshold based on the given metric.</p>"},{"location":"api/evaluator/#locisimiles.evaluator.IntertextEvaluator.evaluate_k_values","title":"evaluate_k_values","text":"<pre><code>evaluate_k_values(\n    *,\n    k_values: List[int] | None = None,\n    average: str = \"micro\",\n    with_match_only: bool = False,\n) -&gt; Dict[int, Dict[str, float]]\n</code></pre> <p>Evaluate metrics for different top_k values WITHOUT re-running the pipeline.</p>"},{"location":"api/generators/","title":"Generators","text":"<p>Candidate generators narrow the search space by selecting source segments that are most likely to be relevant for each query segment.</p> <p>All generators inherit from <code>CandidateGeneratorBase</code> and implement a <code>generate()</code> method returning <code>CandidateGeneratorOutput</code>.</p>"},{"location":"api/generators/#candidategeneratorbase","title":"CandidateGeneratorBase","text":""},{"location":"api/generators/#locisimiles.pipeline.generator._base.CandidateGeneratorBase","title":"locisimiles.pipeline.generator._base.CandidateGeneratorBase","text":"<p>Abstract base class for candidate generators.</p> <p>A candidate generator narrows the search space by producing a ranked list of source segments for each query segment.  The output is a <code>CandidateGeneratorOutput</code> \u2014 a dictionary mapping query-segment IDs to lists of <code>Candidate</code> objects, each containing a source segment and a relevance score.</p> <p>Subclasses must implement <code>generate()</code>.</p> <p>Available implementations:</p> <ul> <li><code>EmbeddingCandidateGenerator</code> \u2014 semantic similarity via sentence   transformers + ChromaDB.</li> <li><code>ExhaustiveCandidateGenerator</code> \u2014 returns all query\u2013source pairs   without filtering.</li> <li><code>RuleBasedCandidateGenerator</code> \u2014 lexical matching with linguistic   filters for Latin texts.</li> </ul>"},{"location":"api/generators/#locisimiles.pipeline.generator._base.CandidateGeneratorBase.generate","title":"generate  <code>abstractmethod</code>","text":"<pre><code>generate(\n    *, query: Document, source: Document, **kwargs: Any\n) -&gt; CandidateGeneratorOutput\n</code></pre> <p>Generate candidate segments from source for each query segment.</p> PARAMETER DESCRIPTION <code>query</code> <p>Query document.</p> <p> TYPE: <code>Document</code> </p> <code>source</code> <p>Source document.</p> <p> TYPE: <code>Document</code> </p> <code>**kwargs</code> <p>Generator-specific parameters.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>CandidateGeneratorOutput</code> <p>Mapping of query segment IDs \u2192 lists of <code>Candidate</code> objects.</p>"},{"location":"api/generators/#embeddingcandidategenerator","title":"EmbeddingCandidateGenerator","text":"<p>Generate candidates using semantic embedding similarity with sentence transformers and ChromaDB.</p>"},{"location":"api/generators/#locisimiles.pipeline.generator.embedding.EmbeddingCandidateGenerator","title":"locisimiles.pipeline.generator.embedding.EmbeddingCandidateGenerator","text":"<pre><code>EmbeddingCandidateGenerator(\n    *,\n    embedding_model_name: str = \"julian-schelb/SPhilBerta-emb-lat-intertext-v1\",\n    device: str | int | None = None,\n)\n</code></pre> <p>Generate candidates using semantic embedding similarity.</p> <p>Encodes query and source segments with a sentence-transformer model, builds an ephemeral ChromaDB index on the source embeddings, and retrieves the most similar source segments for each query segment.</p> <p>The number of candidates per query is controlled by the <code>top_k</code> parameter passed to <code>generate()</code>.</p> PARAMETER DESCRIPTION <code>embedding_model_name</code> <p>HuggingFace model identifier for the sentence-transformer.  Defaults to the pre-trained Latin intertextuality model.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'julian-schelb/SPhilBerta-emb-lat-intertext-v1'</code> </p> <code>device</code> <p>Torch device string (<code>\"cpu\"</code>, <code>\"cuda\"</code>, <code>\"mps\"</code>).</p> <p> TYPE: <code>str | int | None</code> DEFAULT: <code>None</code> </p> Example <pre><code>from locisimiles.pipeline.generator import EmbeddingCandidateGenerator\nfrom locisimiles.document import Document\n\n# Load documents\nquery = Document(\"query.csv\")\nsource = Document(\"source.csv\")\n\n# Generate candidates\ngenerator = EmbeddingCandidateGenerator(device=\"cpu\")\ncandidates = generator.generate(query=query, source=source, top_k=10)\n\n# candidates is a dict: {query_id: [Candidate, ...]}\nfor query_id, cands in candidates.items():\n    print(f\"{query_id}: {len(cands)} candidates\")\n</code></pre>"},{"location":"api/generators/#locisimiles.pipeline.generator.embedding.EmbeddingCandidateGenerator.build_source_index","title":"build_source_index","text":"<pre><code>build_source_index(\n    source_segments: Sequence[TextSegment],\n    source_embeddings: ndarray,\n    collection_name: str = \"source_segments\",\n    batch_size: int = 5000,\n) -&gt; Collection\n</code></pre> <p>Create an ephemeral Chroma collection from segments and embeddings.</p>"},{"location":"api/generators/#locisimiles.pipeline.generator.embedding.EmbeddingCandidateGenerator.generate","title":"generate","text":"<pre><code>generate(\n    *,\n    query: Document,\n    source: Document,\n    top_k: int = 100,\n    query_prompt_name: str = \"query\",\n    source_prompt_name: str = \"match\",\n    **kwargs: Any,\n) -&gt; CandidateGeneratorOutput\n</code></pre> <p>Generate candidates by embedding similarity.</p> <p>Encodes all segments, indexes the source embeddings, and returns the <code>top_k</code> most similar source segments for each query segment.</p> PARAMETER DESCRIPTION <code>query</code> <p>Query document.</p> <p> TYPE: <code>Document</code> </p> <code>source</code> <p>Source document.</p> <p> TYPE: <code>Document</code> </p> <code>top_k</code> <p>Number of most-similar source segments to return per query segment.</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> <code>query_prompt_name</code> <p>Prompt name passed to the sentence-transformer for query encoding.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'query'</code> </p> <code>source_prompt_name</code> <p>Prompt name passed to the sentence-transformer for source encoding.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'match'</code> </p> RETURNS DESCRIPTION <code>CandidateGeneratorOutput</code> <p>Mapping of query segment IDs \u2192 ranked lists of <code>Candidate</code></p> <code>CandidateGeneratorOutput</code> <p>sorted by descending cosine similarity.</p>"},{"location":"api/generators/#exhaustivecandidategenerator","title":"ExhaustiveCandidateGenerator","text":"<p>Return all source segments as candidates (no filtering).</p>"},{"location":"api/generators/#locisimiles.pipeline.generator.exhaustive.ExhaustiveCandidateGenerator","title":"locisimiles.pipeline.generator.exhaustive.ExhaustiveCandidateGenerator","text":"<p>Treat every source segment as a candidate for every query segment.</p> <p>No scoring or ranking is performed.  Each <code>Candidate.score</code> is set to <code>1.0</code> since all pairs are treated equally.</p> <p>This generator is typically paired with a judge (e.g. <code>ClassificationJudge</code>) that performs the actual scoring. Best suited for smaller datasets where comparing all pairs is feasible.</p> Example <pre><code>from locisimiles.pipeline.generator import ExhaustiveCandidateGenerator\nfrom locisimiles.document import Document\n\n# Load documents\nquery = Document(\"query.csv\")\nsource = Document(\"source.csv\")\n\n# Generate all possible pairs\ngenerator = ExhaustiveCandidateGenerator()\ncandidates = generator.generate(query=query, source=source)\n\n# Total pairs = len(query) \u00d7 len(source)\ntotal = sum(len(c) for c in candidates.values())\nprint(f\"{total} candidate pairs\")\n</code></pre>"},{"location":"api/generators/#locisimiles.pipeline.generator.exhaustive.ExhaustiveCandidateGenerator.generate","title":"generate","text":"<pre><code>generate(\n    *, query: Document, source: Document, **kwargs: Any\n) -&gt; CandidateGeneratorOutput\n</code></pre> <p>Return all source segments as candidates for each query segment.</p> PARAMETER DESCRIPTION <code>query</code> <p>Query document.</p> <p> TYPE: <code>Document</code> </p> <code>source</code> <p>Source document.</p> <p> TYPE: <code>Document</code> </p> RETURNS DESCRIPTION <code>CandidateGeneratorOutput</code> <p>Mapping of query segment IDs \u2192 lists of <code>Candidate</code> with</p> <code>CandidateGeneratorOutput</code> <p><code>score=1.0</code>.</p>"},{"location":"api/generators/#rulebasedcandidategenerator","title":"RuleBasedCandidateGenerator","text":"<p>Generate candidates using lexical matching and linguistic filters.</p>"},{"location":"api/generators/#locisimiles.pipeline.generator.rule_based.RuleBasedCandidateGenerator","title":"locisimiles.pipeline.generator.rule_based.RuleBasedCandidateGenerator","text":"<pre><code>RuleBasedCandidateGenerator(\n    *,\n    min_shared_words: int = 2,\n    min_complura: int = 4,\n    max_distance: int = 3,\n    similarity_threshold: float = 0.3,\n    stopwords: Optional[Set[str]] = None,\n    use_htrg: bool = False,\n    use_similarity: bool = False,\n    pos_model: str = \"enelpol/evalatin2022-pos-open\",\n    spacy_model: str = \"la_core_web_lg\",\n    device: Optional[str] = None,\n)\n</code></pre> <p>Generate candidates using lexical matching and linguistic filters.</p> <p>This generator implements a multi-stage rule-based approach to detect potential intertextuality between Latin texts.  It combines orthographic normalization, shared-word matching, distance criteria, punctuation agreement (scissa), and optional POS / embedding-based filters.</p> <p>No neural models are required by default.  The optional HTRG (Part-of-Speech) filter needs <code>torch</code> and <code>transformers</code>, and the similarity filter needs <code>spacy</code> with a Latin model.</p> PARAMETER DESCRIPTION <code>min_shared_words</code> <p>Minimum number of shared non-stopwords required for a segment pair to be considered a match.</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>min_complura</code> <p>Minimum adjacent tokens for complura detection.</p> <p> TYPE: <code>int</code> DEFAULT: <code>4</code> </p> <code>max_distance</code> <p>Maximum allowed distance between shared words within a segment.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>similarity_threshold</code> <p>Cosine similarity threshold for the optional embedding-based filter.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.3</code> </p> <code>stopwords</code> <p>Set of stopwords to exclude from matching.  Uses a built-in Latin stopword list if <code>None</code>.</p> <p> TYPE: <code>Optional[Set[str]]</code> DEFAULT: <code>None</code> </p> <code>use_htrg</code> <p>Enable the HTRG (POS-based) filter.  Requires <code>torch</code>.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>use_similarity</code> <p>Enable the word-embedding similarity filter. Requires <code>spacy</code>.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>pos_model</code> <p>HuggingFace model name for POS tagging.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'enelpol/evalatin2022-pos-open'</code> </p> <code>spacy_model</code> <p>spaCy model name for word embeddings.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'la_core_web_lg'</code> </p> <code>device</code> <p>Device for neural models (<code>\"cuda\"</code>, <code>\"cpu\"</code>, or <code>None</code> for auto-detection).</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> Example <pre><code>from locisimiles.pipeline.generator import RuleBasedCandidateGenerator\nfrom locisimiles.document import Document\n\n# Load documents\nquery = Document(\"query.csv\")\nsource = Document(\"source.csv\")\n\n# Create generator\ngenerator = RuleBasedCandidateGenerator(min_shared_words=3)\n\n# Generate candidates (genre hints improve preprocessing)\ncandidates = generator.generate(\n    query=query,\n    source=source,\n    query_genre=\"prose\",\n    source_genre=\"poetry\",\n)\n\n# Optionally load custom stopwords\ngenerator.load_stopwords(\"my_stopwords.txt\")\n</code></pre>"},{"location":"api/generators/#locisimiles.pipeline.generator.rule_based.RuleBasedCandidateGenerator.generate","title":"generate","text":"<pre><code>generate(\n    *,\n    query: Document,\n    source: Document,\n    top_k: Optional[int] = None,\n    query_genre: str = \"prose\",\n    source_genre: str = \"poetry\",\n    threshold: float = 0.5,\n    **kwargs: Any,\n) -&gt; CandidateGeneratorOutput\n</code></pre> <p>Run the rule-based matching pipeline on query and source documents.</p> PARAMETER DESCRIPTION <code>query</code> <p>Query document (text being analyzed for intertextuality).</p> <p> TYPE: <code>Document</code> </p> <code>source</code> <p>Source document (potential origin of quotations).</p> <p> TYPE: <code>Document</code> </p> <code>top_k</code> <p>Maximum matches per query (<code>None</code> = no limit).</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>query_genre</code> <p>Genre of query (<code>\"prose\"</code> or <code>\"poetry\"</code>).</p> <p> TYPE: <code>str</code> DEFAULT: <code>'prose'</code> </p> <code>source_genre</code> <p>Genre of source (<code>\"prose\"</code> or <code>\"poetry\"</code>).</p> <p> TYPE: <code>str</code> DEFAULT: <code>'poetry'</code> </p> <code>threshold</code> <p>Not used (included for API compatibility).</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.5</code> </p> RETURNS DESCRIPTION <code>CandidateGeneratorOutput</code> <p><code>CandidateGeneratorOutput</code> mapping query segment IDs to lists</p> <code>CandidateGeneratorOutput</code> <p>of <code>Candidate</code> objects.</p>"},{"location":"api/generators/#locisimiles.pipeline.generator.rule_based.RuleBasedCandidateGenerator.load_stopwords","title":"load_stopwords","text":"<pre><code>load_stopwords(filepath: Union[str, Path]) -&gt; None\n</code></pre> <p>Load stopwords from a file (one word per line).</p> PARAMETER DESCRIPTION <code>filepath</code> <p>Path to stopwords file.</p> <p> TYPE: <code>Union[str, Path]</code> </p>"},{"location":"api/judges/","title":"Judges","text":"<p>Judges score or classify candidates produced by a generator.</p> <p>All judges inherit from <code>JudgeBase</code> and implement a <code>judge()</code> method returning <code>CandidateJudgeOutput</code>.</p>"},{"location":"api/judges/#judgebase","title":"JudgeBase","text":""},{"location":"api/judges/#locisimiles.pipeline.judge._base.JudgeBase","title":"locisimiles.pipeline.judge._base.JudgeBase","text":"<p>Abstract base class for candidate judges.</p> <p>A judge receives the output of a candidate generator and produces a <code>CandidateJudgeOutput</code> \u2014 a dictionary mapping query-segment IDs to lists of <code>CandidateJudge</code> objects, each containing a source segment, the original candidate score, and a final judgment score.</p> <p>Subclasses must implement <code>judge()</code>.</p> <p>Available implementations:</p> <ul> <li><code>ClassificationJudge</code> \u2014 scores pairs with a fine-tuned   transformer classification model.</li> <li><code>ThresholdJudge</code> \u2014 applies a top-k or score-threshold rule.</li> <li><code>IdentityJudge</code> \u2014 passes candidates through unchanged   (<code>judgment_score = 1.0</code>).</li> </ul>"},{"location":"api/judges/#locisimiles.pipeline.judge._base.JudgeBase.judge","title":"judge  <code>abstractmethod</code>","text":"<pre><code>judge(\n    *,\n    query: Document,\n    candidates: CandidateGeneratorOutput,\n    **kwargs: Any,\n) -&gt; CandidateJudgeOutput\n</code></pre> <p>Score or classify candidates.</p> PARAMETER DESCRIPTION <code>query</code> <p>Query document (needed to look up query-segment texts).</p> <p> TYPE: <code>Document</code> </p> <code>candidates</code> <p>Output from a candidate generator.</p> <p> TYPE: <code>CandidateGeneratorOutput</code> </p> <code>**kwargs</code> <p>Judge-specific parameters.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>CandidateJudgeOutput</code> <p>Mapping of query segment IDs \u2192 lists of <code>CandidateJudge</code> objects.</p>"},{"location":"api/judges/#classificationjudge","title":"ClassificationJudge","text":"<p>Judge candidates using a transformer sequence-classification model.</p>"},{"location":"api/judges/#locisimiles.pipeline.judge.classification.ClassificationJudge","title":"locisimiles.pipeline.judge.classification.ClassificationJudge","text":"<pre><code>ClassificationJudge(\n    *,\n    classification_name: str = \"julian-schelb/PhilBerta-class-latin-intertext-v1\",\n    device: str | int | None = None,\n    pos_class_idx: int = 1,\n)\n</code></pre> <p>Judge candidates using a transformer classification model.</p> <p>Loads a pre-trained sequence-classification model and tokenizer. For each query\u2013candidate pair the model outputs P(positive), which is stored as <code>judgment_score</code>.</p> <p>The default model is <code>julian-schelb/PhilBerta-class-latin-intertext-v1</code>, a fine-tuned classifier for Latin intertextuality detection.</p> PARAMETER DESCRIPTION <code>classification_name</code> <p>HuggingFace model identifier.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'julian-schelb/PhilBerta-class-latin-intertext-v1'</code> </p> <code>device</code> <p>Torch device string (<code>\"cpu\"</code>, <code>\"cuda\"</code>, <code>\"mps\"</code>).</p> <p> TYPE: <code>str | int | None</code> DEFAULT: <code>None</code> </p> <code>pos_class_idx</code> <p>Index of the positive class in the classifier output.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> Example <pre><code>from locisimiles.pipeline.judge import ClassificationJudge\n\n# Create judge with default model\njudge = ClassificationJudge(device=\"cpu\")\n\n# Score pre-generated candidates\nresults = judge.judge(query=query_doc, candidates=candidates)\n\n# Each result has a judgment_score (probability of being a match)\nfor qid, judgments in results.items():\n    for j in judgments:\n        if j.judgment_score &gt; 0.5:\n            print(f\"{qid} \u2192 {j.segment.id}: {j.judgment_score:.3f}\")\n</code></pre>"},{"location":"api/judges/#locisimiles.pipeline.judge.classification.ClassificationJudge.debug_input_sequence","title":"debug_input_sequence","text":"<pre><code>debug_input_sequence(\n    query_text: str, candidate_text: str, max_len: int = 512\n) -&gt; Dict[str, Any]\n</code></pre> <p>Inspect how a query\u2013candidate pair is tokenised and encoded.</p> <p>Useful for debugging classification results or understanding how text truncation affects model input.</p> PARAMETER DESCRIPTION <code>query_text</code> <p>Raw query text.</p> <p> TYPE: <code>str</code> </p> <code>candidate_text</code> <p>Raw candidate text.</p> <p> TYPE: <code>str</code> </p> <code>max_len</code> <p>Maximum token length.</p> <p> TYPE: <code>int</code> DEFAULT: <code>512</code> </p> RETURNS DESCRIPTION <code>Dict[str, Any]</code> <p>Dictionary with keys:</p> <code>Dict[str, Any]</code> <ul> <li><code>query</code> / <code>candidate</code> \u2014 original texts.</li> </ul> <code>Dict[str, Any]</code> <ul> <li><code>query_truncated</code> / <code>candidate_truncated</code> \u2014 after truncation.</li> </ul> <code>Dict[str, Any]</code> <ul> <li><code>input_ids</code> \u2014 token ID list.</li> </ul> <code>Dict[str, Any]</code> <ul> <li><code>attention_mask</code> \u2014 attention mask list.</li> </ul> <code>Dict[str, Any]</code> <ul> <li><code>input_text</code> \u2014 decoded input with special tokens visible.</li> </ul> Example <pre><code>judge = ClassificationJudge(device=\"cpu\")\ninfo = judge.debug_input_sequence(\n    \"Arma virumque cano\",\n    \"Troiae qui primus ab oris\",\n)\nprint(info[\"input_text\"])\n</code></pre>"},{"location":"api/judges/#locisimiles.pipeline.judge.classification.ClassificationJudge.judge","title":"judge","text":"<pre><code>judge(\n    *,\n    query: Document,\n    candidates: CandidateGeneratorOutput,\n    batch_size: int = 32,\n    **kwargs: Any,\n) -&gt; CandidateJudgeOutput\n</code></pre> <p>Classify each candidate pair using the loaded model.</p> PARAMETER DESCRIPTION <code>query</code> <p>Query document.</p> <p> TYPE: <code>Document</code> </p> <code>candidates</code> <p>Output from a candidate generator.</p> <p> TYPE: <code>CandidateGeneratorOutput</code> </p> <code>batch_size</code> <p>Batch size for the classifier.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> RETURNS DESCRIPTION <code>CandidateJudgeOutput</code> <p><code>CandidateJudgeOutput</code> with <code>judgment_score</code> =</p> <code>CandidateJudgeOutput</code> <p>P(positive) from the classifier.</p>"},{"location":"api/judges/#thresholdjudge","title":"ThresholdJudge","text":"<p>Binary decisions based on candidate scores (top-k or threshold).</p>"},{"location":"api/judges/#locisimiles.pipeline.judge.threshold.ThresholdJudge","title":"locisimiles.pipeline.judge.threshold.ThresholdJudge","text":"<pre><code>ThresholdJudge(\n    *,\n    top_k: int = 10,\n    similarity_threshold: Optional[float] = None,\n)\n</code></pre> <p>Judge candidates using a simple score threshold or top-k cut-off.</p> <p>Two strategies are available (mutually exclusive):</p> <ul> <li>Top-k (default): the first top_k candidates per query (assumed   to be sorted by score descending) receive <code>judgment_score = 1.0</code>;   the rest get <code>0.0</code>.</li> <li>Similarity threshold: if similarity_threshold is provided,   every candidate whose <code>score &gt;= similarity_threshold</code> receives   <code>judgment_score = 1.0</code>.</li> </ul> PARAMETER DESCRIPTION <code>top_k</code> <p>Number of top candidates to mark as positive.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>similarity_threshold</code> <p>Score threshold for positive decisions. If set, overrides <code>top_k</code>.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> Example <pre><code>from locisimiles.pipeline.judge import ThresholdJudge\n\n# Keep the 5 best candidates per query\njudge = ThresholdJudge(top_k=5)\nresults = judge.judge(query=query_doc, candidates=candidates)\n\n# Or use a similarity threshold instead\njudge = ThresholdJudge(similarity_threshold=0.7)\nresults = judge.judge(query=query_doc, candidates=candidates)\n</code></pre>"},{"location":"api/judges/#locisimiles.pipeline.judge.threshold.ThresholdJudge.judge","title":"judge","text":"<pre><code>judge(\n    *,\n    query: Document,\n    candidates: CandidateGeneratorOutput,\n    top_k: Optional[int] = None,\n    similarity_threshold: Optional[float] = None,\n    **kwargs: Any,\n) -&gt; CandidateJudgeOutput\n</code></pre> <p>Apply threshold or top-k rule to produce binary judgments.</p> PARAMETER DESCRIPTION <code>query</code> <p>Query document (unused but required by protocol).</p> <p> TYPE: <code>Document</code> </p> <code>candidates</code> <p>Output from a candidate generator.</p> <p> TYPE: <code>CandidateGeneratorOutput</code> </p> <code>top_k</code> <p>Override instance <code>top_k</code>.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>similarity_threshold</code> <p>Override instance <code>similarity_threshold</code>.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>CandidateJudgeOutput</code> <p><code>CandidateJudgeOutput</code> with <code>judgment_score</code> \u2208 {0.0, 1.0}.</p>"},{"location":"api/judges/#identityjudge","title":"IdentityJudge","text":"<p>Pass-through judge that marks every candidate as positive.</p>"},{"location":"api/judges/#locisimiles.pipeline.judge.identity.IdentityJudge","title":"locisimiles.pipeline.judge.identity.IdentityJudge","text":"<p>Pass every candidate through with <code>judgment_score = 1.0</code>.</p> <p>Useful when the candidate generator already performs all the filtering and scoring that is needed (e.g. the rule-based generator). No additional models are loaded.</p> Example <pre><code>from locisimiles.pipeline.judge import IdentityJudge\n\njudge = IdentityJudge()\nresults = judge.judge(query=query_doc, candidates=candidates)\n\n# Every candidate gets judgment_score = 1.0\nfor qid, judgments in results.items():\n    for j in judgments:\n        print(j.judgment_score)  # 1.0\n</code></pre>"},{"location":"api/judges/#locisimiles.pipeline.judge.identity.IdentityJudge.judge","title":"judge","text":"<pre><code>judge(\n    *,\n    query: Document,\n    candidates: CandidateGeneratorOutput,\n    **kwargs: Any,\n) -&gt; CandidateJudgeOutput\n</code></pre> <p>Convert every <code>Candidate</code> to <code>CandidateJudge</code> with judgment_score = 1.0.</p>"},{"location":"api/pipelines/","title":"Pipelines","text":"<p>Ready-to-use pipelines for detecting intertextual parallels in Latin literature.</p> <p>Each pipeline loads its own models and exposes a single <code>run()</code> method that accepts two <code>Document</code> objects and returns scored results.</p>"},{"location":"api/pipelines/#classificationpipelinewithcandidategeneration","title":"ClassificationPipelineWithCandidategeneration","text":"<p>Two-stage pipeline: embedding retrieval + classification.</p> <p>Combines a fast embedding-based retrieval step with a more expensive classification step to efficiently identify intertextual parallels in large corpora.</p> <p>Pipeline steps:</p> <ol> <li>Retrieval - Encode all segments with a sentence-transformer    model and retrieve the top_k most similar source segments for    each query segment using cosine similarity.</li> <li>Classification - Feed each query-candidate pair into a    fine-tuned sequence-classification model.  The positive-class    probability is used as the judgment score.</li> </ol> PARAMETER DESCRIPTION <code>classification_name</code> <p>HuggingFace model identifier for the sequence-classification model.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'julian-schelb/PhilBerta-class-latin-intertext-v1'</code> </p> <code>embedding_model_name</code> <p>HuggingFace model identifier for the sentence-transformer.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'julian-schelb/SPhilBerta-emb-lat-intertext-v1'</code> </p> <code>device</code> <p>Torch device string (<code>\"cpu\"</code>, <code>\"cuda\"</code>, \u2026).</p> <p> TYPE: <code>str | int | None</code> DEFAULT: <code>None</code> </p> <code>pos_class_idx</code> <p>Index of the positive class in the classifier output.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> Example <pre><code>from locisimiles.pipeline import ClassificationPipelineWithCandidategeneration\nfrom locisimiles.document import Document\n\n# Load documents\nquery = Document(\"query.csv\")\nsource = Document(\"source.csv\")\n\n# Define pipeline\npipeline = ClassificationPipelineWithCandidategeneration(device=\"cpu\")\n\n# Run pipeline\nresults = pipeline.run(query=query, source=source, top_k=10)\n</code></pre>"},{"location":"api/pipelines/#locisimiles.pipeline.two_stage.TwoStagePipeline.device","title":"device  <code>property</code>","text":"<pre><code>device: str\n</code></pre> <p>Device used by the classification judge.</p>"},{"location":"api/pipelines/#classificationpipeline","title":"ClassificationPipeline","text":"<p>Classification pipeline for exhaustive pairwise comparison.</p> <p>For each query segment every source segment is considered as a candidate.  Each query-source pair is then fed to a fine-tuned sequence-classification model that outputs the probability of the pair being an intertextual match.</p> <p>Pipeline steps:</p> <ol> <li>Candidate generation - Create all possible query-source pairs.</li> <li>Classification - Score each pair with a HuggingFace    sequence-classification model.  The positive-class probability    is used as the judgment score.</li> </ol> PARAMETER DESCRIPTION <code>classification_name</code> <p>HuggingFace model identifier for the sequence-classification model.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'julian-schelb/PhilBerta-class-latin-intertext-v1'</code> </p> <code>device</code> <p>Torch device string (<code>\"cpu\"</code>, <code>\"cuda\"</code>, ...).</p> <p> TYPE: <code>str | int | None</code> DEFAULT: <code>None</code> </p> <code>pos_class_idx</code> <p>Index of the positive class in the classifier output.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> Example <pre><code>from locisimiles.pipeline import ClassificationPipeline\nfrom locisimiles.document import Document\n\n# Load documents\nquery = Document(\"query.csv\")\nsource = Document(\"source.csv\")\n\n# Define pipeline\npipeline = ClassificationPipeline(device=\"cpu\")\n\n# Run pipeline\nresults = pipeline.run(query=query, source=source)\n</code></pre>"},{"location":"api/pipelines/#locisimiles.pipeline.classification.ExhaustiveClassificationPipeline.device","title":"device  <code>property</code>","text":"<pre><code>device: str\n</code></pre> <p>Device used by the classification judge.</p>"},{"location":"api/pipelines/#retrievalpipeline","title":"RetrievalPipeline","text":""},{"location":"api/pipelines/#locisimiles.pipeline.retrieval.RetrievalPipeline","title":"locisimiles.pipeline.retrieval.RetrievalPipeline","text":"<pre><code>RetrievalPipeline(\n    *,\n    embedding_model_name: str = \"julian-schelb/SPhilBerta-emb-lat-intertext-v1\",\n    device: str | int | None = None,\n    top_k: int = 10,\n    similarity_threshold: Optional[float] = None,\n)\n</code></pre> <p>Retrieval pipeline based on embedding similarity.</p> <p>Uses a sentence-transformer model to encode query and source segments into dense vectors and ranks candidates by cosine similarity.</p> <p>Pipeline steps:</p> <ol> <li>Encoding - Encode all query and source segments with a    sentence-transformer model.</li> <li>Retrieval - For each query, retrieve all source segments    ranked by cosine similarity.</li> <li>Thresholding - Mark the top_k most similar candidates as    positive, or all candidates above similarity_threshold if    provided.</li> </ol> PARAMETER DESCRIPTION <code>embedding_model_name</code> <p>HuggingFace model identifier for the sentence-transformer.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'julian-schelb/SPhilBerta-emb-lat-intertext-v1'</code> </p> <code>device</code> <p>Torch device string (<code>\"cpu\"</code>, <code>\"cuda\"</code>, \u2026).</p> <p> TYPE: <code>str | int | None</code> DEFAULT: <code>None</code> </p> <code>top_k</code> <p>Number of top candidates to mark as positive.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>similarity_threshold</code> <p>If provided, uses threshold instead of top-k.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> Example <pre><code>from locisimiles.pipeline import RetrievalPipeline\nfrom locisimiles.document import Document\n\n# Load documents\nquery = Document(\"query.csv\")\nsource = Document(\"source.csv\")\n\n# Define pipeline\npipeline = RetrievalPipeline(device=\"cpu\")\n\n# Run pipeline\nresults = pipeline.run(query=query, source=source, top_k=10)\n</code></pre>"},{"location":"api/pipelines/#locisimiles.pipeline.retrieval.RetrievalPipeline.device","title":"device  <code>property</code>","text":"<pre><code>device: str\n</code></pre> <p>Device used by the embedding generator.</p>"},{"location":"api/pipelines/#rulebasedpipeline","title":"RuleBasedPipeline","text":""},{"location":"api/pipelines/#locisimiles.pipeline.rule_based.RuleBasedPipeline","title":"locisimiles.pipeline.rule_based.RuleBasedPipeline","text":"<pre><code>RuleBasedPipeline(\n    *,\n    min_shared_words: int = 2,\n    min_complura: int = 4,\n    max_distance: int = 3,\n    similarity_threshold: float = 0.3,\n    stopwords: Optional[Set[str]] = None,\n    use_htrg: bool = False,\n    use_similarity: bool = False,\n    pos_model: str = \"enelpol/evalatin2022-pos-open\",\n    spacy_model: str = \"la_core_web_lg\",\n    device: Optional[str] = None,\n)\n</code></pre> <p>Rule-based pipeline for lexical intertextuality detection.</p> <p>Identifies potential quotations, allusions, and textual reuse between Latin documents using a multi-stage rule-based approach.</p> <p>Pipeline steps:</p> <ol> <li>Text preprocessing - Normalise prefix assimilations    (e.g. adt- \u2192 att-, inm- \u2192 imm-), quotation marks,    and whitespace connectors.  Apply genre-specific phrasing    rules for prose or poetry.</li> <li>Text matching - Tokenise both documents and find shared    non-stopword tokens between every query-source segment pair.</li> <li>Distance criterion - Discard matches where shared words    are too far apart (controlled by <code>max_distance</code>).</li> <li>Scissa filter - Compare punctuation patterns around shared    words to strengthen evidence of deliberate textual reuse.</li> <li>HTRG filter (optional) - Part-of-speech analysis using a    HuggingFace token-classification model.  Requires <code>torch</code>.</li> <li>Similarity filter (optional) - Word-embedding similarity    check using spaCy vectors.  Requires <code>spacy</code>.</li> </ol> PARAMETER DESCRIPTION <code>min_shared_words</code> <p>Minimum number of shared non-stopwords required.</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>min_complura</code> <p>Minimum adjacent tokens for complura detection.</p> <p> TYPE: <code>int</code> DEFAULT: <code>4</code> </p> <code>max_distance</code> <p>Maximum distance between shared words.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>similarity_threshold</code> <p>Threshold for semantic similarity filter.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.3</code> </p> <code>stopwords</code> <p>Set of stopwords to exclude.  Uses defaults if <code>None</code>.</p> <p> TYPE: <code>Optional[Set[str]]</code> DEFAULT: <code>None</code> </p> <code>use_htrg</code> <p>Whether to apply HTRG (POS-based) filter.  Requires torch.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>use_similarity</code> <p>Whether to apply similarity filter.  Requires spacy.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>pos_model</code> <p>HuggingFace model name for POS tagging.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'enelpol/evalatin2022-pos-open'</code> </p> <code>spacy_model</code> <p>spaCy model name for embeddings.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'la_core_web_lg'</code> </p> <code>device</code> <p>Device for neural models (<code>\"cuda\"</code>, <code>\"cpu\"</code>, or <code>None</code> for auto).</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> Example <pre><code>from locisimiles.pipeline import RuleBasedPipeline\nfrom locisimiles.document import Document\n\n# Load documents\nquery = Document(\"query.csv\")\nsource = Document(\"source.csv\")\n\n# Define pipeline\npipeline = RuleBasedPipeline()\n\n# Run pipeline\nresults = pipeline.run(query=query, source=source)\n</code></pre>"},{"location":"api/pipelines/#locisimiles.pipeline.rule_based.RuleBasedPipeline.min_shared_words","title":"min_shared_words  <code>property</code>","text":"<pre><code>min_shared_words: int\n</code></pre> <p>Minimum number of shared non-stopwords required.</p>"},{"location":"api/pipelines/#locisimiles.pipeline.rule_based.RuleBasedPipeline.min_complura","title":"min_complura  <code>property</code>","text":"<pre><code>min_complura: int\n</code></pre> <p>Minimum adjacent tokens for complura detection.</p>"},{"location":"api/pipelines/#locisimiles.pipeline.rule_based.RuleBasedPipeline.max_distance","title":"max_distance  <code>property</code>","text":"<pre><code>max_distance: int\n</code></pre> <p>Maximum distance between shared words.</p>"},{"location":"api/pipelines/#locisimiles.pipeline.rule_based.RuleBasedPipeline.similarity_threshold","title":"similarity_threshold  <code>property</code>","text":"<pre><code>similarity_threshold: float\n</code></pre> <p>Threshold for semantic similarity filter.</p>"},{"location":"api/pipelines/#locisimiles.pipeline.rule_based.RuleBasedPipeline.stopwords","title":"stopwords  <code>property</code>","text":"<pre><code>stopwords: Set[str]\n</code></pre> <p>Set of stopwords to exclude.</p>"},{"location":"api/pipelines/#locisimiles.pipeline.rule_based.RuleBasedPipeline.use_htrg","title":"use_htrg  <code>property</code>","text":"<pre><code>use_htrg: bool\n</code></pre> <p>Whether HTRG (POS-based) filter is enabled.</p>"},{"location":"api/pipelines/#locisimiles.pipeline.rule_based.RuleBasedPipeline.use_similarity","title":"use_similarity  <code>property</code>","text":"<pre><code>use_similarity: bool\n</code></pre> <p>Whether similarity filter is enabled.</p>"},{"location":"api/pipelines/#locisimiles.pipeline.rule_based.RuleBasedPipeline.device","title":"device  <code>property</code>","text":"<pre><code>device: str\n</code></pre> <p>Device for neural models.</p>"},{"location":"api/pipelines/#locisimiles.pipeline.rule_based.RuleBasedPipeline.load_stopwords","title":"load_stopwords","text":"<pre><code>load_stopwords(filepath: Union[str, Path]) -&gt; None\n</code></pre> <p>Load stopwords from a file (one word per line).</p> PARAMETER DESCRIPTION <code>filepath</code> <p>Path to stopwords file.</p> <p> TYPE: <code>Union[str, Path]</code> </p>"}]}