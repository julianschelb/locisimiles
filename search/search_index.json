{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"LociSimiles","text":"<p>A Python package for extracting intertextualities in Latin literature using pre-trained language models.</p> <p>LociSimiles enables researchers to detect textual reuse, quotations, and allusions between Latin texts, from verbatim citations to subtle paraphrases and thematic echoes.</p>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<pre><code>pip install locisimiles\n</code></pre> <p>Or install with development dependencies:</p> <pre><code>pip install \"locisimiles[dev]\"\n</code></pre>"},{"location":"#basic-usage","title":"Basic Usage","text":"<pre><code>from locisimiles import Document, RetrievalPipeline\n\n# Load your documents\nsource = Document.from_csv(\"source_texts.csv\")\ntarget = Document.from_csv(\"target_texts.csv\")\n\n# Create retrieval pipeline\npipeline = RetrievalPipeline()\n\n# Find similar passages\nresults = pipeline.retrieve(source, target, top_k=5)\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>Getting Started - Installation and first steps</li> <li>Examples - Working examples and tutorials</li> <li>CLI Reference - Command-line interface documentation</li> <li>API Reference - Complete API documentation</li> <li>Development - Contributing and development setup</li> </ul>"},{"location":"#authors","title":"Authors","text":"<ul> <li>Julian Schelb - University of Konstanz</li> <li>Michael Wittweiler - University of Zurich</li> </ul>"},{"location":"#citation","title":"Citation","text":"<p>If you use LociSimiles in your research, please cite our paper:</p> <pre><code>@article{schelb2026locisimiles,\n  title={Loci Similes: A Benchmark for Extracting Intertextualities in Latin Literature},\n  author={Schelb, Julian and Wittweiler, Michael and Revellio, Marie and Feichtinger, Barbara and Spitz, Andreas},\n  journal={arXiv preprint arXiv:2601.07533},\n  year={2026}\n}\n</code></pre>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License.</p>"},{"location":"cli/","title":"CLI Reference","text":"<p>LociSimiles provides a command-line interface for common workflows.</p>"},{"location":"cli/#installation","title":"Installation","text":"<p>The CLI is installed automatically with the package:</p> <pre><code>pip install locisimiles\n</code></pre>"},{"location":"cli/#commands","title":"Commands","text":""},{"location":"cli/#locisimiles-run","title":"<code>locisimiles run</code>","text":"<p>Run the intertextual detection pipeline on source and target documents.</p> <pre><code>locisimiles run SOURCE TARGET [OPTIONS]\n</code></pre>"},{"location":"cli/#arguments","title":"Arguments","text":"Argument Description <code>SOURCE</code> Path to the source CSV file <code>TARGET</code> Path to the target CSV file"},{"location":"cli/#options","title":"Options","text":"Option Default Description <code>--output</code>, <code>-o</code> <code>results.csv</code> Output file path <code>--model</code>, <code>-m</code> <code>sentence-transformers/all-MiniLM-L6-v2</code> Model name or path <code>--top-k</code>, <code>-k</code> <code>10</code> Number of candidates to retrieve <code>--threshold</code>, <code>-t</code> <code>0.5</code> Classification threshold <code>--batch-size</code>, <code>-b</code> <code>32</code> Batch size for processing"},{"location":"cli/#examples","title":"Examples","text":"<p>Basic usage:</p> <pre><code>locisimiles run source.csv target.csv\n</code></pre> <p>With custom output and model:</p> <pre><code>locisimiles run source.csv target.csv \\\n    --output results.csv \\\n    --model bert-base-multilingual-cased \\\n    --top-k 20\n</code></pre>"},{"location":"cli/#locisimiles-evaluate","title":"<code>locisimiles evaluate</code>","text":"<p>Evaluate detection results against ground truth.</p> <pre><code>locisimiles evaluate PREDICTIONS GROUND_TRUTH [OPTIONS]\n</code></pre>"},{"location":"cli/#arguments_1","title":"Arguments","text":"Argument Description <code>PREDICTIONS</code> Path to predictions CSV file <code>GROUND_TRUTH</code> Path to ground truth CSV file"},{"location":"cli/#options_1","title":"Options","text":"Option Default Description <code>--output</code>, <code>-o</code> <code>None</code> Output file for metrics (prints to stdout if not specified)"},{"location":"cli/#examples_1","title":"Examples","text":"<pre><code>locisimiles evaluate results.csv ground_truth.csv\n</code></pre> <p>Save metrics to file:</p> <pre><code>locisimiles evaluate results.csv ground_truth.csv -o metrics.json\n</code></pre>"},{"location":"cli/#input-file-formats","title":"Input File Formats","text":""},{"location":"cli/#sourcetarget-csv","title":"Source/Target CSV","text":"<p>CSV files should contain at minimum an ID column and a text column:</p> <pre><code>id,text\n1,\"Arma virumque cano Troiae qui primus ab oris\"\n2,\"Italiam fato profugus Laviniaque venit\"\n</code></pre>"},{"location":"cli/#ground-truth-csv","title":"Ground Truth CSV","text":"<p>Ground truth files should contain query-reference pairs with labels:</p> <pre><code>query_id,reference_id,label\n1,42,1\n2,15,0\n</code></pre> <p>Where <code>label</code> is <code>1</code> for true matches and <code>0</code> for non-matches.</p>"},{"location":"cli/#output-format","title":"Output Format","text":"<p>The pipeline outputs a CSV with the following columns:</p> Column Description <code>query_id</code> ID of the source text segment <code>reference_id</code> ID of the matched target segment <code>score</code> Similarity/classification score <code>above_threshold</code> Whether the score exceeds the threshold"},{"location":"cli/#environment-variables","title":"Environment Variables","text":"Variable Description <code>LOCISIMILES_CACHE_DIR</code> Directory for model caching <code>LOCISIMILES_DEVICE</code> Device for computation (<code>cpu</code>, <code>cuda</code>, <code>mps</code>)"},{"location":"development/","title":"Development Guide","text":"<p>This guide covers setting up a development environment and contributing to LociSimiles.</p>"},{"location":"development/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10 or higher</li> <li>Git</li> <li>pip package manager</li> </ul>"},{"location":"development/#setup","title":"Setup","text":""},{"location":"development/#clone-the-repository","title":"Clone the Repository","text":"<pre><code>git clone https://github.com/julianschelb/locisimiles.git\ncd locisimiles\n</code></pre>"},{"location":"development/#create-virtual-environment","title":"Create Virtual Environment","text":"<pre><code>python -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n</code></pre>"},{"location":"development/#install-development-dependencies","title":"Install Development Dependencies","text":"<pre><code>pip install -e \".[dev]\"\n</code></pre> <p>This installs the package in editable mode along with development tools:</p> <ul> <li>pytest - Testing framework</li> <li>pytest-cov - Coverage reporting</li> <li>poethepoet - Task runner</li> <li>mkdocs - Documentation</li> <li>mkdocs-material - Documentation theme</li> </ul>"},{"location":"development/#running-tests","title":"Running Tests","text":""},{"location":"development/#using-poe-recommended","title":"Using Poe (Recommended)","text":"<pre><code># Run all tests\npoe test\n\n# Run tests with coverage report\npoe test-cov\n</code></pre>"},{"location":"development/#using-pytest-directly","title":"Using Pytest Directly","text":"<pre><code># Run all tests\npytest\n\n# Run with verbose output\npytest -v\n\n# Run specific test file\npytest tests/test_document.py\n\n# Run specific test\npytest tests/test_document.py::TestDocument::test_from_csv\n\n# Run with coverage\npytest --cov=locisimiles --cov-report=term-missing\n\n# Generate HTML coverage report\npytest --cov=locisimiles --cov-report=html\n</code></pre>"},{"location":"development/#documentation","title":"Documentation","text":""},{"location":"development/#serve-locally","title":"Serve Locally","text":"<pre><code>poe docs\n</code></pre> <p>This starts a local server at <code>http://127.0.0.1:8000</code> with live reload.</p>"},{"location":"development/#build-static-site","title":"Build Static Site","text":"<pre><code>poe docs-build\n</code></pre> <p>Output is written to the <code>site/</code> directory.</p>"},{"location":"development/#deploy-to-github-pages","title":"Deploy to GitHub Pages","text":"<pre><code>poe docs-deploy\n</code></pre> <p>This builds and pushes to the <code>gh-pages</code> branch.</p>"},{"location":"development/#project-structure","title":"Project Structure","text":"<pre><code>locisimiles/\n\u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 locisimiles/\n\u2502       \u251c\u2500\u2500 __init__.py          # Package exports\n\u2502       \u251c\u2500\u2500 cli.py               # Command-line interface\n\u2502       \u251c\u2500\u2500 document.py          # Document and TextSegment classes\n\u2502       \u251c\u2500\u2500 evaluator.py         # Evaluation metrics\n\u2502       \u2514\u2500\u2500 pipeline/\n\u2502           \u251c\u2500\u2500 __init__.py      # Pipeline exports\n\u2502           \u251c\u2500\u2500 _types.py        # Type definitions\n\u2502           \u251c\u2500\u2500 classification.py # Classification pipeline\n\u2502           \u251c\u2500\u2500 retrieval.py     # Retrieval pipeline\n\u2502           \u2514\u2500\u2500 two_stage.py     # Combined pipeline\n\u251c\u2500\u2500 tests/\n\u2502   \u251c\u2500\u2500 conftest.py              # Shared test fixtures\n\u2502   \u251c\u2500\u2500 test_document.py         # Document tests\n\u2502   \u251c\u2500\u2500 test_evaluator.py        # Evaluator tests\n\u2502   \u251c\u2500\u2500 test_cli.py              # CLI tests\n\u2502   \u251c\u2500\u2500 test_types.py            # Type definition tests\n\u2502   \u251c\u2500\u2500 test_retrieval.py        # Retrieval pipeline tests\n\u2502   \u251c\u2500\u2500 test_classification.py   # Classification pipeline tests\n\u2502   \u2514\u2500\u2500 test_two_stage.py        # Two-stage pipeline tests\n\u251c\u2500\u2500 docs/                        # Documentation source\n\u251c\u2500\u2500 examples/                    # Example scripts and notebooks\n\u251c\u2500\u2500 pyproject.toml               # Project configuration\n\u2514\u2500\u2500 mkdocs.yml                   # Documentation configuration\n</code></pre>"},{"location":"development/#code-style","title":"Code Style","text":""},{"location":"development/#general-guidelines","title":"General Guidelines","text":"<ul> <li>Follow PEP 8 style guidelines</li> <li>Use type hints for function signatures</li> <li>Write docstrings for public APIs</li> <li>Keep functions focused and small</li> </ul>"},{"location":"development/#example","title":"Example","text":"<pre><code>def compute_similarity(\n    text_a: str,\n    text_b: str,\n    model: Optional[str] = None\n) -&gt; float:\n    \"\"\"Compute semantic similarity between two texts.\n\n    Args:\n        text_a: First text string.\n        text_b: Second text string.\n        model: Optional model name. Defaults to MiniLM.\n\n    Returns:\n        Similarity score between 0 and 1.\n\n    Raises:\n        ValueError: If either text is empty.\n    \"\"\"\n    if not text_a or not text_b:\n        raise ValueError(\"Texts cannot be empty\")\n    ...\n</code></pre>"},{"location":"development/#testing-guidelines","title":"Testing Guidelines","text":""},{"location":"development/#writing-tests","title":"Writing Tests","text":"<ul> <li>Place tests in the <code>tests/</code> directory</li> <li>Name test files <code>test_&lt;module&gt;.py</code></li> <li>Name test classes <code>Test&lt;ClassName&gt;</code></li> <li>Name test methods <code>test_&lt;behavior&gt;</code></li> </ul>"},{"location":"development/#using-fixtures","title":"Using Fixtures","text":"<p>Shared fixtures are defined in <code>conftest.py</code>:</p> <pre><code>def test_document_loading(sample_csv_file):\n    \"\"\"Test loading document from CSV.\"\"\"\n    doc = Document.from_csv(sample_csv_file)\n    assert len(doc) &gt; 0\n</code></pre>"},{"location":"development/#mocking-external-dependencies","title":"Mocking External Dependencies","text":"<p>Use unittest.mock for ML models:</p> <pre><code>from unittest.mock import MagicMock, patch\n\ndef test_retrieval_with_mock(mock_embedder):\n    \"\"\"Test retrieval with mocked embedding model.\"\"\"\n    pipeline = RetrievalPipeline()\n    pipeline.model = mock_embedder\n    # Test without actual model loading\n</code></pre>"},{"location":"development/#contributing","title":"Contributing","text":""},{"location":"development/#workflow","title":"Workflow","text":"<ol> <li>Fork the repository</li> <li>Create a feature branch: <code>git checkout -b feature/my-feature</code></li> <li>Make changes and add tests</li> <li>Ensure tests pass: <code>poe test</code></li> <li>Commit changes: <code>git commit -m \"Add my feature\"</code></li> <li>Push to fork: <code>git push origin feature/my-feature</code></li> <li>Open a Pull Request</li> </ol>"},{"location":"development/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<ul> <li>Include tests for new functionality</li> <li>Update documentation as needed</li> <li>Keep commits focused and atomic</li> <li>Write clear commit messages</li> </ul>"},{"location":"development/#available-poe-tasks","title":"Available Poe Tasks","text":"Task Command Description <code>test</code> <code>poe test</code> Run all tests <code>test-cov</code> <code>poe test-cov</code> Run tests with coverage <code>docs</code> <code>poe docs</code> Serve documentation locally <code>docs-build</code> <code>poe docs-build</code> Build documentation <code>docs-deploy</code> <code>poe docs-deploy</code> Deploy to GitHub Pages"},{"location":"examples/","title":"Examples","text":"<p>This section provides working examples demonstrating LociSimiles usage.</p>"},{"location":"examples/#sample-data","title":"Sample Data","text":"<p>The examples use sample Latin texts:</p> <ul> <li>Hieronymus samples - Query texts from Jerome's writings</li> <li>Vergil samples - Source texts from Virgil's works</li> <li>Ground truth - Annotated intertextual links for evaluation</li> </ul>"},{"location":"examples/#quick-start-example","title":"Quick Start Example","text":"<pre><code>from locisimiles.evaluator import IntertextEvaluator\nfrom locisimiles.pipeline import (\n    ClassificationPipeline,\n    ClassificationPipelineWithCandidategeneration,\n    pretty_print,\n)\nfrom locisimiles.document import Document\n\n# Load example query and source documents\nquery_doc = Document(\"./hieronymus_samples.csv\", author=\"Hieronymus\")\nsource_doc = Document(\"./vergil_samples.csv\", author=\"Vergil\")\n\nprint(\"Loaded query and source documents:\")\nprint(f\"Query Document: {query_doc}\")\nprint(f\"Source Document: {source_doc}\")\nprint(\"=\" * 70)\n\n\n# Load the pipeline with pre-trained models\npipeline_two_stage = ClassificationPipelineWithCandidategeneration(\n    classification_name=\"julian-schelb/PhilBerta-class-latin-intertext-v1\",\n    embedding_model_name=\"julian-schelb/SPhilBerta-emb-lat-intertext-v1\",\n    device=\"mps\",\n)\n\n# Run the pipeline with the query and source documents\nresults_two_stage = pipeline_two_stage.run(\n    query=query_doc,    # Query document\n    source=source_doc,  # Source document\n    top_k=10            # Number of top similar candidates to classify\n)\nprint(\"\\nResults of the two-stage pipeline run:\")\npretty_print(results_two_stage)\n\nevaluator = IntertextEvaluator(\n    query_doc=query_doc,\n    source_doc=source_doc,\n    ground_truth_csv=\"./ground_truth.csv\",\n    pipeline=pipeline_two_stage,\n    top_k=10,\n    threshold=0.5,\n)\n\nprint(\"\\nSingle sentence:\\n\", evaluator.evaluate_single_query(\"hier. adv. iovin. 1.41\"))\nprint(\"\\nPer-sentence head:\\n\", evaluator.evaluate_all_queries().head(20))\nprint(\"\\nMacro scores:\\n\", evaluator.evaluate(average=\"macro\", with_match_only=True))\nprint(\"\\nMicro scores:\\n\", evaluator.evaluate(average=\"micro\", with_match_only=True))\n</code></pre>"},{"location":"examples/#jupyter-notebook","title":"Jupyter Notebook","text":"<p>For an interactive walkthrough, see the example notebook.</p> <p>The notebook covers:</p> <ol> <li>Loading Documents - Creating Document objects from CSV files</li> <li>Two-Stage Pipeline - Using retrieval + classification</li> <li>Finding Optimal Threshold - Automatic threshold tuning</li> <li>Evaluating Different K Values - Comparing top-k settings</li> <li>Classification-Only Pipeline - Exhaustive pairwise comparison</li> </ol>"},{"location":"examples/#two-stage-pipeline","title":"Two-Stage Pipeline","text":"<p>The recommended approach combines fast retrieval with accurate classification:</p> <pre><code>from locisimiles.pipeline import ClassificationPipelineWithCandidategeneration\nfrom locisimiles.document import Document\n\n# Load documents\nquery_doc = Document(\"./hieronymus_samples.csv\", author=\"Hieronymus\")\nsource_doc = Document(\"./vergil_samples.csv\", author=\"Vergil\")\n\n# Initialize pipeline with pre-trained models\npipeline = ClassificationPipelineWithCandidategeneration(\n    classification_name=\"julian-schelb/PhilBerta-class-latin-intertext-v1\",\n    embedding_model_name=\"julian-schelb/SPhilBerta-emb-lat-intertext-v1\",\n    device=\"cpu\",  # or \"cuda\", \"mps\"\n)\n\n# Run the pipeline\nresults = pipeline.run(\n    query=query_doc,\n    source=source_doc,\n    top_k=10  # Number of candidates per query\n)\n</code></pre>"},{"location":"examples/#evaluation","title":"Evaluation","text":"<p>Evaluate your results against ground truth annotations:</p> <pre><code>from locisimiles.evaluator import IntertextEvaluator\n\nevaluator = IntertextEvaluator(\n    query_doc=query_doc,\n    source_doc=source_doc,\n    ground_truth_csv=\"./ground_truth.csv\",\n    pipeline=pipeline,\n    top_k=10,\n    threshold=0.5,\n)\n\n# Evaluate a single query\nprint(evaluator.evaluate_single_query(\"hier. adv. iovin. 1.41\"))\n\n# Get metrics for all queries\nprint(evaluator.evaluate(average=\"macro\"))\nprint(evaluator.evaluate(average=\"micro\"))\n</code></pre>"},{"location":"examples/#finding-the-best-threshold","title":"Finding the Best Threshold","text":"<p>Automatically find the optimal probability threshold:</p> <pre><code>best_result, all_thresholds_df = evaluator.find_best_threshold(\n    metric=\"f1\",       # Optimize for F1 (or 'precision', 'recall', 'smr')\n    average=\"micro\",   # Use micro-averaging\n)\n\nprint(f\"Best threshold: {best_result['best_threshold']}\")\nprint(f\"Best F1 score: {best_result['best_f1']:.4f}\")\n</code></pre>"},{"location":"examples/#classification-only-pipeline","title":"Classification-Only Pipeline","text":"<p>For smaller datasets, use exhaustive pairwise comparison:</p> <pre><code>from locisimiles.pipeline import ClassificationPipeline\n\npipeline_clf = ClassificationPipeline(\n    classification_name=\"julian-schelb/PhilBerta-class-latin-intertext-v1\",\n    device=\"cpu\",\n)\n\nresults = pipeline_clf.run(\n    query=query_doc,\n    source=source_doc,\n    batch_size=32,\n)\n\n# Filter high-probability matches\nthreshold = 0.7\nfor query_id, pairs in results.items():\n    high_prob = [(seg, prob) for seg, sim, prob in pairs if prob &gt; threshold]\n    if high_prob:\n        print(f\"Query {query_id}:\")\n        for seg, prob in high_prob:\n            print(f\"  {seg.id}: P={prob:.3f}\")\n</code></pre>"},{"location":"examples/#running-the-examples","title":"Running the Examples","text":"<p>To run the examples locally:</p> <pre><code>cd examples\npip install -r requirements.txt\npython example.py\n</code></pre> <p>Or open <code>example.ipynb</code> in Jupyter for the interactive version.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This guide will help you get started with LociSimiles for finding intertextual links in Latin literature.</p>"},{"location":"getting-started/#installation","title":"Installation","text":""},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10 or higher</li> <li>pip package manager</li> </ul>"},{"location":"getting-started/#installing-from-pypi","title":"Installing from PyPI","text":"<pre><code>pip install locisimiles\n</code></pre>"},{"location":"getting-started/#installing-from-source","title":"Installing from Source","text":"<pre><code>git clone https://github.com/julianschelb/locisimiles.git\ncd locisimiles\npip install -e \".[dev]\"\n</code></pre>"},{"location":"getting-started/#basic-concepts","title":"Basic Concepts","text":""},{"location":"getting-started/#documents-and-segments","title":"Documents and Segments","text":"<p>LociSimiles works with Documents containing TextSegments. Each segment represents a unit of text (e.g., a verse, sentence, or passage).</p> <pre><code>from locisimiles import Document, TextSegment\n\n# Create segments manually\nsegments = [\n    TextSegment(id=\"1\", text=\"Arma virumque cano\"),\n    TextSegment(id=\"2\", text=\"Troiae qui primus ab oris\"),\n]\n\n# Create a document\ndoc = Document(segments=segments)\n</code></pre>"},{"location":"getting-started/#loading-from-csv","title":"Loading from CSV","text":"<p>Documents are typically loaded from CSV files:</p> <pre><code>doc = Document.from_csv(\"texts.csv\")\n</code></pre> <p>The CSV should have columns for <code>id</code> and <code>text</code> (column names are configurable).</p>"},{"location":"getting-started/#pipelines","title":"Pipelines","text":"<p>LociSimiles provides three main pipeline types:</p>"},{"location":"getting-started/#1-retrieval-pipeline","title":"1. Retrieval Pipeline","text":"<p>Uses semantic embeddings to find similar passages:</p> <pre><code>from locisimiles import RetrievalPipeline\n\npipeline = RetrievalPipeline(\n    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n)\n\nresults = pipeline.retrieve(source_doc, target_doc, top_k=10)\n</code></pre>"},{"location":"getting-started/#2-classification-pipeline","title":"2. Classification Pipeline","text":"<p>Uses a transformer model to classify text pairs:</p> <pre><code>from locisimiles import ClassificationPipeline\n\npipeline = ClassificationPipeline(\n    model_name=\"bert-base-uncased\"\n)\n\nresults = pipeline.classify(pairs)\n</code></pre>"},{"location":"getting-started/#3-two-stage-pipeline","title":"3. Two-Stage Pipeline","text":"<p>Combines retrieval and classification for best results:</p> <pre><code>from locisimiles import TwoStagePipeline\n\npipeline = TwoStagePipeline(\n    retrieval_model=\"sentence-transformers/all-MiniLM-L6-v2\",\n    classification_model=\"bert-base-uncased\"\n)\n\nresults = pipeline.run(source_doc, target_doc)\n</code></pre>"},{"location":"getting-started/#evaluation","title":"Evaluation","text":"<p>Use the <code>IntertextEvaluator</code> to assess detection quality:</p> <pre><code>from locisimiles import IntertextEvaluator\n\nevaluator = IntertextEvaluator(\n    predictions=predictions,\n    ground_truth=ground_truth\n)\n\nmetrics = evaluator.evaluate()\nprint(f\"Precision: {metrics['precision']:.3f}\")\nprint(f\"Recall: {metrics['recall']:.3f}\")\nprint(f\"F1: {metrics['f1']:.3f}\")\n</code></pre>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<ul> <li>See the CLI Reference for command-line usage</li> <li>Explore the API Reference for detailed documentation</li> <li>Check out the examples for complete workflows</li> </ul>"},{"location":"api/","title":"API Reference","text":"<p>This section provides detailed documentation for the LociSimiles Python API, auto-generated from source code docstrings.</p>"},{"location":"api/#core-modules","title":"Core Modules","text":""},{"location":"api/#document-module","title":"Document Module","text":"<p>The Document module provides classes for representing and loading text collections:</p> <ul> <li><code>TextSegment</code> - Individual text unit with ID and content</li> <li><code>Document</code> - Container for text segments</li> </ul>"},{"location":"api/#pipeline-module","title":"Pipeline Module","text":"<p>The Pipelines module provides the main processing pipelines:</p> <ul> <li><code>RetrievalPipeline</code> - Semantic similarity retrieval</li> <li><code>ClassificationPipeline</code> - Text pair classification</li> <li><code>ClassificationPipelineWithCandidategeneration</code> - Two-stage retrieval + classification</li> </ul>"},{"location":"api/#evaluator-module","title":"Evaluator Module","text":"<p>The Evaluator module provides tools for assessing detection quality:</p> <ul> <li><code>IntertextEvaluator</code> - Main evaluation class</li> </ul>"},{"location":"api/#quick-reference","title":"Quick Reference","text":""},{"location":"api/#loading-documents","title":"Loading Documents","text":"<pre><code>from locisimiles import Document\n\n# From CSV file\ndoc = Document.from_csv(\"texts.csv\")\n\n# From list of d# From list of d# From list of dre#ords([\n    {\"id\"    {\"id\"    {\"id\"    {\"id\"    {\"id\"    {\"id\"    {\"id\"    {\"id\"    {\"id\"    {\"id\"    {\"id\"    {\"id\"    {\"id\"    {\"id\"  om     {\"id\"    mpor    {\"id\"    {\"id\"pel    {\"id\"    {\"id\"    {\"id\"    {\"id\"    {\"id\"    {\"id\"    {\"id\"    {\"id\"    {\"id\"    {\"id\"    {\"ipip    {\"id\"    {\"id\"    {\"id\"    {\"id\"    {\"id\" 0.5)\n</code></pre>"},{"location":"api/#evaluating-results","title":"Evaluating Results","text":"<pre><code>from locisimiles import IntertextEvaluator\n\nevaluator = IntertextEvaluator(predictions, ground_truth)\nmetrics = evaluator.evaluate()\n</code></pre>"},{"location":"api/document/","title":"Document Module","text":"<p>Classes for representing and loading text collections.</p>"},{"location":"api/document/#textsegment","title":"TextSegment","text":"<p>An individual unit of text with an identifier.</p>"},{"location":"api/document/#locisimiles.document.TextSegment","title":"locisimiles.document.TextSegment","text":"<pre><code>TextSegment(\n    text: str,\n    seg_id: ID,\n    *,\n    row_id: int | None = None,\n    meta: Dict[str, Any] | None = None,\n)\n</code></pre> <p>Atomic unit of text inside a document.</p> <p>A TextSegment represents a single passage, sentence, or verse from a larger document. Each segment has a unique identifier and optional metadata.</p> ATTRIBUTE DESCRIPTION <code>text</code> <p>The raw text content of the segment.</p> <p> TYPE: <code>str</code> </p> <code>id</code> <p>Unique identifier for the segment (e.g., \"verg. aen. 1.1\").</p> <p> TYPE: <code>ID</code> </p> <code>row_id</code> <p>Position of the segment in the original document (0-indexed).</p> <p> TYPE: <code>int | None</code> </p> <code>meta</code> <p>Optional dictionary of additional metadata.</p> <p> TYPE: <code>Dict[str, Any]</code> </p> Example <pre><code>segment = TextSegment(\n    text=\"Arma virumque cano, Troiae qui primus ab oris\",\n    seg_id=\"verg. aen. 1.1\",\n    row_id=0,\n    meta={\"book\": 1, \"line\": 1}\n)\nprint(segment.text)  # \"Arma virumque cano...\"\nprint(segment.id)    # \"verg. aen. 1.1\"\n</code></pre>"},{"location":"api/document/#document","title":"Document","text":"<p>A collection of text segments with loading utilities.</p>"},{"location":"api/document/#locisimiles.document.Document","title":"locisimiles.document.Document","text":"<pre><code>Document(\n    path: str | Path,\n    *,\n    author: str | None = None,\n    meta: Dict[str, Any] | None = None,\n    segment_delimiter: str = \"\\n\",\n)\n</code></pre> <p>Collection of text segments representing a document.</p> <p>A Document is a container for TextSegments loaded from a file. It supports CSV/TSV files with 'seg_id' and 'text' columns, or plain text files where segments are separated by a delimiter.</p> ATTRIBUTE DESCRIPTION <code>path</code> <p>Path to the source file.</p> <p> TYPE: <code>Path</code> </p> <code>author</code> <p>Optional author name for the document.</p> <p> TYPE: <code>str | None</code> </p> <code>meta</code> <p>Optional dictionary of document-level metadata.</p> <p> TYPE: <code>Dict[str, Any]</code> </p> Example <pre><code>from locisimiles.document import Document\n\n# Load from CSV (must have 'seg_id' and 'text' columns)\nvergil = Document(\"vergil_samples.csv\", author=\"Vergil\")\n\n# Access segments\nprint(len(vergil))           # Number of segments\nprint(vergil.ids())          # List of segment IDs\nprint(vergil.get_text(\"verg. aen. 1.1\"))  # Get text by ID\n\n# Iterate over segments\nfor segment in vergil:\n    print(f\"{segment.id}: {segment.text[:50]}...\")\n\n# Add custom segments\nvergil.add_segment(\n    text=\"Custom text\",\n    seg_id=\"custom.1\",\n    meta={\"source\": \"manual\"}\n)\n</code></pre>"},{"location":"api/document/#locisimiles.document.Document.ids","title":"ids","text":"<pre><code>ids() -&gt; List[ID]\n</code></pre> <p>Return segment IDs in original order.</p>"},{"location":"api/document/#locisimiles.document.Document.get_text","title":"get_text","text":"<pre><code>get_text(seg_id: ID) -&gt; str\n</code></pre> <p>Return raw text of a segment.</p>"},{"location":"api/document/#locisimiles.document.Document.add_segment","title":"add_segment","text":"<pre><code>add_segment(\n    text: str,\n    seg_id: ID,\n    *,\n    row_id: int | None = None,\n    meta: Dict[str, Any] | None = None,\n) -&gt; None\n</code></pre> <p>Add a new text segment to the document.</p>"},{"location":"api/document/#locisimiles.document.Document.remove_segment","title":"remove_segment","text":"<pre><code>remove_segment(seg_id: ID) -&gt; None\n</code></pre> <p>Delete a segment if present.</p>"},{"location":"api/evaluator/","title":"Evaluator Module","text":"<p>Tools for assessing detection quality.</p>"},{"location":"api/evaluator/#intertextevaluator","title":"IntertextEvaluator","text":"<p>Evaluate detection results against ground truth annotations.</p> <p>Computes precision, recall, F1, and other metrics for intertextual link detection.</p>"},{"location":"api/evaluator/#locisimiles.evaluator.IntertextEvaluator","title":"locisimiles.evaluator.IntertextEvaluator","text":"<pre><code>IntertextEvaluator(\n    *,\n    query_doc: Document,\n    source_doc: Document,\n    ground_truth_csv: str | DataFrame,\n    pipeline: ClassificationPipelineWithCandidategeneration,\n    top_k: int = 5,\n    threshold: float | str = \"auto\",\n    auto_threshold_metric: str = \"smr\",\n)\n</code></pre> <p>Evaluator for measuring intertextuality detection performance.</p> <p>This class computes sentence-level and document-level evaluation metrics by comparing pipeline predictions against ground truth annotations.</p> Supported metrics <ul> <li>Precision: TP / (TP + FP)</li> <li>Recall: TP / (TP + FN)</li> <li>F1: Harmonic mean of precision and recall</li> <li>SMR: Source Match Rate (error rate)</li> <li>Accuracy: (TP + TN) / Total</li> </ul> <p>The evaluator runs the pipeline once during initialization and caches the results for efficient metric computation across different thresholds.</p> ATTRIBUTE DESCRIPTION <code>query_doc</code> <p>The query document being analyzed.</p> <p> </p> <code>source_doc</code> <p>The source document containing potential quotation origins.</p> <p> </p> <code>predictions</code> <p>Cached pipeline predictions (FullDict format).</p> <p> TYPE: <code>FullDict</code> </p> <code>threshold</code> <p>Probability threshold for positive classification.</p> <p> </p> <code>gold_labels</code> <p>Ground truth annotations loaded from CSV.</p> <p> </p> Example <pre><code>from locisimiles.evaluator import IntertextEvaluator\nfrom locisimiles.pipeline import ClassificationPipelineWithCandidategeneration\nfrom locisimiles.document import Document\n\n# Load documents\nquery_doc = Document(\"hieronymus.csv\")\nsource_doc = Document(\"vergil.csv\")\n\n# Initialize pipeline\npipeline = ClassificationPipelineWithCandidategeneration(device=\"cpu\")\n\n# Create evaluator with auto-threshold\nevaluator = IntertextEvaluator(\n    query_doc=query_doc,\n    source_doc=source_doc,\n    ground_truth_csv=\"ground_truth.csv\",\n    pipeline=pipeline,\n    top_k=10,\n    threshold=\"auto\",  # Automatically find best threshold\n    auto_threshold_metric=\"smr\",\n)\n\n# Get evaluation metrics\nprint(evaluator.evaluate(average=\"micro\"))\nprint(evaluator.evaluate(average=\"macro\"))\n\n# Evaluate single query\nprint(evaluator.evaluate_single_query(\"hier. adv. iovin. 1.41\"))\n\n# Find optimal threshold for different metrics\nbest, all_thresholds = evaluator.find_best_threshold(metric=\"f1\")\nprint(f\"Best F1 at threshold {best['best_threshold']}: {best['best_f1']:.3f}\")\n</code></pre>"},{"location":"api/evaluator/#locisimiles.evaluator.IntertextEvaluator.evaluate_single_query","title":"evaluate_single_query","text":"<pre><code>evaluate_single_query(query_id: str) -&gt; Dict[str, float]\n</code></pre> <p>Compute metrics for one query sentence.</p>"},{"location":"api/evaluator/#locisimiles.evaluator.IntertextEvaluator.query_ids_with_match","title":"query_ids_with_match","text":"<pre><code>query_ids_with_match() -&gt; List[str]\n</code></pre> <p>Return query IDs that have ground truth labels.</p>"},{"location":"api/evaluator/#locisimiles.evaluator.IntertextEvaluator.evaluate_all_queries","title":"evaluate_all_queries","text":"<pre><code>evaluate_all_queries(\n    with_match_only: bool = False,\n) -&gt; DataFrame\n</code></pre> <p>Compute metrics for every query sentence (cached).</p>"},{"location":"api/evaluator/#locisimiles.evaluator.IntertextEvaluator.evaluate","title":"evaluate","text":"<pre><code>evaluate(\n    *, average: str = \"macro\", with_match_only: bool = False\n) -&gt; Dict[str, float]\n</code></pre> <p>Compute aggregated metrics across queries.</p> <ul> <li>Precision, Recall, F1, Accuracy: ALWAYS computed on queries with at least    one ground truth match (otherwise these metrics are meaningless).</li> <li>FPR, FNR, SMR: Computed on ALL queries by default (measures false alarms    on queries that shouldn't have matches). If with_match_only=True, these    are also restricted to queries with matches.</li> </ul>"},{"location":"api/evaluator/#locisimiles.evaluator.IntertextEvaluator.confusion_matrix","title":"confusion_matrix","text":"<pre><code>confusion_matrix(query_id: str) -&gt; ndarray\n</code></pre> <p>Return 2x2 confusion matrix [[TP,FP],[FN,TN]] for one query sentence.</p>"},{"location":"api/evaluator/#locisimiles.evaluator.IntertextEvaluator.find_best_threshold","title":"find_best_threshold","text":"<pre><code>find_best_threshold(\n    *,\n    metric: str = \"f1\",\n    thresholds: List[float] | None = None,\n    average: str = \"micro\",\n    with_match_only: bool = False,\n) -&gt; Tuple[Dict[str, float], DataFrame]\n</code></pre> <p>Find the optimal probability threshold based on the given metric.</p>"},{"location":"api/evaluator/#locisimiles.evaluator.IntertextEvaluator.evaluate_k_values","title":"evaluate_k_values","text":"<pre><code>evaluate_k_values(\n    *,\n    k_values: List[int] | None = None,\n    average: str = \"micro\",\n    with_match_only: bool = False,\n) -&gt; Dict[int, Dict[str, float]]\n</code></pre> <p>Evaluate metrics for different top_k values WITHOUT re-running the pipeline.</p>"},{"location":"api/pipelines/","title":"Pipelines Module","text":"<p>Processing pipelines for intertextual detection.</p>"},{"location":"api/pipelines/#retrievalpipeline","title":"RetrievalPipeline","text":"<p>Find similar passages using semantic embeddings.</p>"},{"location":"api/pipelines/#locisimiles.pipeline.retrieval.RetrievalPipeline","title":"locisimiles.pipeline.retrieval.RetrievalPipeline","text":"<pre><code>RetrievalPipeline(\n    *,\n    embedding_model_name: str = \"julian-schelb/SPhilBerta-emb-lat-intertext-v1\",\n    device: str | int | None = None,\n)\n</code></pre> <p>A retrieval-only pipeline for intertextuality detection.</p> <p>This pipeline uses semantic embeddings to find similar text passages without a classification stage. It's useful for fast candidate generation or when you want a simpler approach based purely on semantic similarity.</p> Binary decisions are made using one of two strategies <ul> <li>Top-k: The k most similar candidates are marked as positive</li> <li>Similarity threshold: Candidates above a threshold are positive</li> </ul> <p>The results are returned in FullDict format for compatibility with the evaluator, where \"probability\" is 1.0 for positive and 0.0 for negative.</p> ATTRIBUTE DESCRIPTION <code>embedder</code> <p>The sentence transformer model for computing embeddings.</p> <p> </p> <code>device</code> <p>The device used for computation ('cpu' or 'cuda').</p> <p> </p> Example <pre><code>from locisimiles.pipeline import RetrievalPipeline\nfrom locisimiles.document import Document\n\n# Load documents\nquery_doc = Document(\"hieronymus.csv\")\nsource_doc = Document(\"vergil.csv\")\n\n# Initialize pipeline\npipeline = RetrievalPipeline(\n    embedding_model_name=\"julian-schelb/SPhilBerta-emb-lat-intertext-v1\",\n    device=\"cpu\",\n)\n\n# Find top 5 similar passages for each query\nresults = pipeline.run(\n    query=query_doc,\n    source=source_doc,\n    top_k=5,\n)\n</code></pre>"},{"location":"api/pipelines/#locisimiles.pipeline.retrieval.RetrievalPipeline.build_source_index","title":"build_source_index","text":"<pre><code>build_source_index(\n    source_segments: Sequence[TextSegment],\n    source_embeddings: ndarray,\n    collection_name: str = \"source_segments\",\n    batch_size: int = 5000,\n)\n</code></pre> <p>Create a Chroma collection from source_segments and their embeddings.</p>"},{"location":"api/pipelines/#locisimiles.pipeline.retrieval.RetrievalPipeline.retrieve","title":"retrieve","text":"<pre><code>retrieve(\n    *,\n    query: Document,\n    source: Document,\n    top_k: int = 100,\n    query_prompt_name: str = \"query\",\n    source_prompt_name: str = \"match\",\n    **kwargs: Any,\n) -&gt; SimDict\n</code></pre> <p>Retrieve candidate segments from source based on similarity to query.</p> <p>Returns a dictionary mapping query segment IDs to lists of (source segment, similarity score) pairs, sorted by similarity (descending).</p>"},{"location":"api/pipelines/#locisimiles.pipeline.retrieval.RetrievalPipeline.run","title":"run","text":"<pre><code>run(\n    *,\n    query: Document,\n    source: Document,\n    top_k: int = 10,\n    similarity_threshold: float | None = None,\n    query_prompt_name: str = \"query\",\n    source_prompt_name: str = \"match\",\n    **kwargs: Any,\n) -&gt; FullDict\n</code></pre> <p>Run the retrieval pipeline and return results compatible with the evaluator.</p> <p>Binary decisions are made using one of two criteria: - top_k (default): The top-k ranked candidates per query are predicted    as positive (prob=1.0), all others as negative (prob=0.0). - similarity_threshold: If provided, candidates with similarity &gt;= threshold   are predicted as positive, regardless of rank.</p> PARAMETER DESCRIPTION <code>query</code> <p>Query document</p> <p> TYPE: <code>Document</code> </p> <code>source</code> <p>Source document  </p> <p> TYPE: <code>Document</code> </p> <code>top_k</code> <p>Number of top candidates to mark as positive (default: 10).    Used when similarity_threshold is None.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>similarity_threshold</code> <p>If provided, use this similarity cutoff instead    of top_k. Candidates with similarity &gt;= threshold are positive.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>query_prompt_name</code> <p>Prompt name for query embeddings</p> <p> TYPE: <code>str</code> DEFAULT: <code>'query'</code> </p> <code>source_prompt_name</code> <p>Prompt name for source embeddings</p> <p> TYPE: <code>str</code> DEFAULT: <code>'match'</code> </p> RETURNS DESCRIPTION <code>FullDict</code> <p>FullDict mapping query IDs to lists of (segment, similarity, probability)</p> <code>FullDict</code> <p>tuples. Probability is 1.0 for positive predictions, 0.0 for negative.</p>"},{"location":"api/pipelines/#classificationpipeline","title":"ClassificationPipeline","text":"<p>Classify text pairs using transformer models (exhaustive comparison).</p>"},{"location":"api/pipelines/#locisimiles.pipeline.classification.ClassificationPipeline","title":"locisimiles.pipeline.classification.ClassificationPipeline","text":"<pre><code>ClassificationPipeline(\n    *,\n    classification_name: str = \"julian-schelb/PhilBerta-class-latin-intertext-v1\",\n    device: str | int | None = None,\n    pos_class_idx: int = 1,\n)\n</code></pre> <p>A classification-only pipeline for exhaustive pairwise intertextuality detection.</p> <p>This pipeline classifies all possible pairs between query and source segments without a retrieval stage. It's suitable for smaller document collections or when you need exhaustive comparison without missing any potential matches.</p> Note <p>For large documents, consider using <code>ClassificationPipelineWithCandidategeneration</code> which first filters candidates using semantic similarity.</p> ATTRIBUTE DESCRIPTION <code>clf_model</code> <p>The transformer classification model.</p> <p> </p> <code>clf_tokenizer</code> <p>The tokenizer for the classification model.</p> <p> </p> <code>device</code> <p>The device used for computation ('cpu' or 'cuda').</p> <p> </p> Example <pre><code>from locisimiles.pipeline import ClassificationPipeline\nfrom locisimiles.document import Document\n\n# Load documents\nquery_doc = Document(\"hieronymus.csv\")\nsource_doc = Document(\"vergil.csv\")\n\n# Initialize pipeline\npipeline = ClassificationPipeline(\n    classification_name=\"julian-schelb/PhilBerta-class-latin-intertext-v1\",\n    device=\"cpu\",\n)\n\n# Classify all pairs (exhaustive)\nresults = pipeline.run(\n    query=query_doc,\n    source=source_doc,\n    batch_size=32,\n)\n\n# Filter results by probability threshold\nfor query_id, pairs in results.items():\n    matches = [(seg, prob) for seg, sim, prob in pairs if prob &gt; 0.7]\n    if matches:\n        print(f\"{query_id}: {len(matches)} matches\")\n</code></pre>"},{"location":"api/pipelines/#locisimiles.pipeline.classification.ClassificationPipeline.debug_input_sequence","title":"debug_input_sequence","text":"<pre><code>debug_input_sequence(\n    query_text: str, candidate_text: str, max_len: int = 512\n) -&gt; Dict[str, Any]\n</code></pre> <p>Debug method to inspect how a query-candidate pair is encoded.</p>"},{"location":"api/pipelines/#locisimiles.pipeline.classification.ClassificationPipeline.run","title":"run","text":"<pre><code>run(\n    *,\n    query: Document,\n    source: Document,\n    batch_size: int = 32,\n    **kwargs: Any,\n) -&gt; FullDict\n</code></pre> <p>Run classification on all query-source segment pairs. Returns a dictionary mapping query segment IDs to lists of (source segment, similarity_score=None, P(positive)) tuples.</p> <p>Note: Since there's no retrieval stage, similarity_score is set to None.</p>"},{"location":"api/pipelines/#classificationpipelinewithcandidategeneration","title":"ClassificationPipelineWithCandidategeneration","text":"<p>Two-stage pipeline: retrieval for candidate generation, then classification.</p>"},{"location":"api/pipelines/#locisimiles.pipeline.two_stage.ClassificationPipelineWithCandidategeneration","title":"locisimiles.pipeline.two_stage.ClassificationPipelineWithCandidategeneration","text":"<pre><code>ClassificationPipelineWithCandidategeneration(\n    *,\n    classification_name: str = \"julian-schelb/PhilBerta-class-latin-intertext-v1\",\n    embedding_model_name: str = \"julian-schelb/SPhilBerta-emb-lat-intertext-v1\",\n    device: str | int | None = None,\n    pos_class_idx: int = 1,\n)\n</code></pre> <p>A two-stage pipeline combining retrieval and classification for intertextuality detection.</p> This pipeline implements an efficient two-stage approach <ol> <li>Retrieval: Generate candidate segments using embedding similarity</li> <li>Classification: Classify the top-k candidates using a transformer model</li> </ol> <p>This approach is more efficient than exhaustive classification for large document collections, while maintaining high accuracy by using learned classification.</p> ATTRIBUTE DESCRIPTION <code>embedder</code> <p>The sentence transformer model for candidate generation.</p> <p> </p> <code>clf_model</code> <p>The transformer classification model.</p> <p> </p> <code>clf_tokenizer</code> <p>The tokenizer for the classification model.</p> <p> </p> <code>device</code> <p>The device used for computation ('cpu' or 'cuda').</p> <p> </p> Example <pre><code>from locisimiles.pipeline import ClassificationPipelineWithCandidategeneration\nfrom locisimiles.document import Document\n\n# Load documents\nquery_doc = Document(\"hieronymus.csv\")\nsource_doc = Document(\"vergil.csv\")\n\n# Initialize two-stage pipeline\npipeline = ClassificationPipelineWithCandidategeneration(\n    classification_name=\"julian-schelb/PhilBerta-class-latin-intertext-v1\",\n    embedding_model_name=\"julian-schelb/SPhilBerta-emb-lat-intertext-v1\",\n    device=\"cpu\",\n)\n\n# Run pipeline: retrieve top 10 candidates, then classify\nresults = pipeline.run(\n    query=query_doc,\n    source=source_doc,\n    top_k=10,\n)\n\n# Pretty print results\nfrom locisimiles.pipeline import pretty_print\npretty_print(results)\n</code></pre>"},{"location":"api/pipelines/#locisimiles.pipeline.two_stage.ClassificationPipelineWithCandidategeneration.debug_input_sequence","title":"debug_input_sequence","text":"<pre><code>debug_input_sequence(\n    query_text: str, candidate_text: str, max_len: int = 512\n) -&gt; Dict[str, Any]\n</code></pre> <p>Debug method to inspect how a query-candidate pair is encoded.</p> <p>Returns a dictionary with: - query: Original query text - candidate: Original candidate text - query_truncated: Truncated query text - candidate_truncated: Truncated candidate text - input_ids: Token IDs as list - input_text: Decoded text with special tokens visible - attention_mask: Attention mask as list</p>"},{"location":"api/pipelines/#locisimiles.pipeline.two_stage.ClassificationPipelineWithCandidategeneration.build_source_index","title":"build_source_index","text":"<pre><code>build_source_index(\n    source_segments: Sequence[TextSegment],\n    source_embeddings: ndarray,\n    collection_name: str = \"source_segments\",\n    batch_size: int = 5000,\n)\n</code></pre> <p>Create a Chroma collection from source_segments and their embeddings.</p>"},{"location":"api/pipelines/#locisimiles.pipeline.two_stage.ClassificationPipelineWithCandidategeneration.generate_candidates","title":"generate_candidates","text":"<pre><code>generate_candidates(\n    *,\n    query: Document,\n    source: Document,\n    top_k: int = 5,\n    query_prompt_name: str = \"query\",\n    source_prompt_name: str = \"match\",\n    **kwargs: Any,\n) -&gt; SimDict\n</code></pre> <p>Generate candidate segments from source based on similarity to query. Returns a dictionary mapping query segment IDs to lists of (source segment, similarity score) pairs.</p>"},{"location":"api/pipelines/#locisimiles.pipeline.two_stage.ClassificationPipelineWithCandidategeneration.check_candidates","title":"check_candidates","text":"<pre><code>check_candidates(\n    *,\n    query: Document,\n    source: Document,\n    candidates: SimDict | None = None,\n    batch_size: int = 32,\n    **kwargs: Any,\n) -&gt; FullDict\n</code></pre> <p>Classify candidates generated from source. If candidates is not provided, it will be generated using generate_candidates. Returns a dictionary mapping query segment IDs to lists of (source segment, similarity score, P(positive)) tuples.</p>"},{"location":"api/pipelines/#locisimiles.pipeline.two_stage.ClassificationPipelineWithCandidategeneration.run","title":"run","text":"<pre><code>run(\n    *,\n    query: Document,\n    source: Document,\n    top_k: int = 5,\n    query_prompt_name: str = \"query\",\n    source_prompt_name: str = \"match\",\n    **kwargs: Any,\n) -&gt; FullDict\n</code></pre> <p>Run the full pipeline: generate candidates and classify them. Returns a dictionary mapping query segment IDs to lists of (source segment, similarity score, P(positive)) tuples.</p>"},{"location":"api/pipelines/#rulebasedpipeline","title":"RuleBasedPipeline","text":"<p>Rule-based pipeline using lexical matching and linguistic filters.</p>"},{"location":"api/pipelines/#locisimiles.pipeline.rule_based.RuleBasedPipeline","title":"locisimiles.pipeline.rule_based.RuleBasedPipeline","text":"<pre><code>RuleBasedPipeline(\n    *,\n    min_shared_words: int = 2,\n    min_complura: int = 4,\n    max_distance: int = 3,\n    similarity_threshold: float = 0.3,\n    stopwords: Optional[Set[str]] = None,\n    use_htrg: bool = False,\n    use_similarity: bool = False,\n    pos_model: str = \"enelpol/evalatin2022-pos-open\",\n    spacy_model: str = \"la_core_web_lg\",\n    device: Optional[str] = None,\n)\n</code></pre> <p>A rule-based pipeline for detecting intertextuality in Latin texts.</p> <p>This pipeline uses lexical matching combined with various filters to identify potential quotations, allusions, and textual reuse between Latin documents.</p> Features <ul> <li>Orthographic normalization (v\u2192u, j\u2192i, prefix assimilation)</li> <li>Stopword filtering</li> <li>Distance criterion (shared words must be close together)</li> <li>Scissa filter (punctuation agreement)</li> <li>HTRG filter (Part-of-Speech agreement) - optional</li> <li>Similarity filter (embedding-based) - optional</li> </ul> Example <pre><code>pipeline = RuleBasedPipeline()\nresults = pipeline.run(query=query_doc, source=source_doc)\n</code></pre> <p>Initialize the rule-based pipeline.</p> PARAMETER DESCRIPTION <code>min_shared_words</code> <p>Minimum number of shared non-stopwords required.</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>min_complura</code> <p>Minimum adjacent tokens for complura detection.</p> <p> TYPE: <code>int</code> DEFAULT: <code>4</code> </p> <code>max_distance</code> <p>Maximum distance between shared words.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>similarity_threshold</code> <p>Threshold for semantic similarity filter.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.3</code> </p> <code>stopwords</code> <p>Set of stopwords to exclude. Uses defaults if None.</p> <p> TYPE: <code>Optional[Set[str]]</code> DEFAULT: <code>None</code> </p> <code>use_htrg</code> <p>Whether to apply HTRG (POS-based) filter. Requires torch.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>use_similarity</code> <p>Whether to apply similarity filter. Requires spacy.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>pos_model</code> <p>HuggingFace model name for POS tagging.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'enelpol/evalatin2022-pos-open'</code> </p> <code>spacy_model</code> <p>spaCy model name for embeddings.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'la_core_web_lg'</code> </p> <code>device</code> <p>Device for neural models ('cuda', 'cpu', or None for auto).</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p>"},{"location":"api/pipelines/#locisimiles.pipeline.rule_based.RuleBasedPipeline.run","title":"run","text":"<pre><code>run(\n    query: Document,\n    source: Document,\n    *,\n    top_k: Optional[int] = None,\n    query_genre: str = \"prose\",\n    source_genre: str = \"poetry\",\n    threshold: float = 0.5,\n) -&gt; FullDict\n</code></pre> <p>Run the rule-based pipeline on query and source documents.</p> PARAMETER DESCRIPTION <code>query</code> <p>Query document (text being analyzed for intertextuality).</p> <p> TYPE: <code>Document</code> </p> <code>source</code> <p>Source document (potential origin of quotations).</p> <p> TYPE: <code>Document</code> </p> <code>top_k</code> <p>Maximum matches per query (None = no limit). For API compatibility.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>query_genre</code> <p>Genre of query ('prose' or 'poetry').</p> <p> TYPE: <code>str</code> DEFAULT: <code>'prose'</code> </p> <code>source_genre</code> <p>Genre of source ('prose' or 'poetry').</p> <p> TYPE: <code>str</code> DEFAULT: <code>'poetry'</code> </p> <code>threshold</code> <p>Not used (included for API compatibility).</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.5</code> </p> RETURNS DESCRIPTION <code>FullDict</code> <p>FullDict mapping query segment IDs to lists of</p> <code>FullDict</code> <p>(source_segment, similarity, probability) tuples.</p>"},{"location":"api/pipelines/#locisimiles.pipeline.rule_based.RuleBasedPipeline.load_stopwords","title":"load_stopwords","text":"<pre><code>load_stopwords(filepath: Union[str, Path]) -&gt; None\n</code></pre> <p>Load stopwords from a file (one word per line).</p> PARAMETER DESCRIPTION <code>filepath</code> <p>Path to stopwords file.</p> <p> TYPE: <code>Union[str, Path]</code> </p>"},{"location":"api/pipelines/#type-definitions","title":"Type Definitions","text":"<p>Data classes for pipeline results.</p>"},{"location":"api/pipelines/#locisimiles.pipeline._types","title":"locisimiles.pipeline._types","text":"<p>Shared type definitions and utilities for pipeline modules.</p> <p>This module defines the common data structures used across all pipeline implementations for representing detection results.</p>"},{"location":"api/pipelines/#locisimiles.pipeline._types.ScoreT","title":"ScoreT  <code>module-attribute</code>","text":"<pre><code>ScoreT = float\n</code></pre> <p>Type alias for similarity or probability scores (float between 0 and 1).</p>"},{"location":"api/pipelines/#locisimiles.pipeline._types.SimPair","title":"SimPair  <code>module-attribute</code>","text":"<pre><code>SimPair = Tuple[TextSegment, ScoreT]\n</code></pre> <p>A tuple of (TextSegment, similarity_score) representing a candidate match.</p> <p>Used in retrieval-only pipelines where only similarity scores are computed.</p>"},{"location":"api/pipelines/#locisimiles.pipeline._types.FullPair","title":"FullPair  <code>module-attribute</code>","text":"<pre><code>FullPair = Tuple[TextSegment, ScoreT, ScoreT]\n</code></pre> <p>A tuple of (TextSegment, similarity_score, probability) for classified matches.</p> The three elements are <ul> <li>TextSegment: The matching source segment</li> <li>similarity_score: Cosine similarity from retrieval (may be None)</li> <li>probability: Classification probability P(positive)</li> </ul>"},{"location":"api/pipelines/#locisimiles.pipeline._types.SimDict","title":"SimDict  <code>module-attribute</code>","text":"<pre><code>SimDict = Dict[str, List[SimPair]]\n</code></pre> <p>Mapping from query segment IDs to lists of (segment, similarity) pairs.</p> <p>Used as intermediate result format in retrieval pipelines.</p>"},{"location":"api/pipelines/#locisimiles.pipeline._types.FullDict","title":"FullDict  <code>module-attribute</code>","text":"<pre><code>FullDict = Dict[str, List[FullPair]]\n</code></pre> <p>Mapping from query segment IDs to lists of (segment, similarity, probability) tuples.</p> <p>This is the standard output format for all pipelines, used by the evaluator.</p> Example <pre><code>results: FullDict = {\n    \"hier. adv. iovin. 1.41\": [\n        (TextSegment(...), 0.82, 0.95),  # High confidence match\n        (TextSegment(...), 0.65, 0.23),  # Low confidence\n    ],\n    \"hier. adv. iovin. 1.42\": [...],\n}\n</code></pre>"},{"location":"api/pipelines/#locisimiles.pipeline._types.pretty_print","title":"pretty_print","text":"<pre><code>pretty_print(results: FullDict) -&gt; None\n</code></pre> <p>Print pipeline results in a human-readable format.</p> <p>Displays each query segment and its candidate matches with similarity scores and classification probabilities.</p> PARAMETER DESCRIPTION <code>results</code> <p>Pipeline output in FullDict format.</p> <p> TYPE: <code>FullDict</code> </p> Example <pre><code>from locisimiles.pipeline import pretty_print\n\nresults = pipeline.run(query=query_doc, source=source_doc, top_k=5)\npretty_print(results)\n\n# Output:\n# \u25b6 Query segment 'hier. adv. iovin. 1.41':\n#   verg. aen. 1.1              sim=+0.823  P(pos)=0.951\n#   verg. aen. 2.45             sim=+0.654  P(pos)=0.234\n</code></pre>"}]}