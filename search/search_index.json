{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"LociSimiles","text":"<p>A Python package for extracting intertextualities in Latin literature using pre-trained language models.</p> <p>LociSimiles enables researchers to detect textual reuse, quotations, and allusions between Latin texts, from verbatim citations to subtle paraphrases and thematic echoes.</p>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<pre><code>pip install locisimiles\n</code></pre> <p>Or install with development dependencies:</p> <pre><code>pip install \"locisimiles[dev]\"\n</code></pre>"},{"location":"#basic-usage","title":"Basic Usage","text":"<pre><code>from locisimiles import Document, RetrievalPipeline\n\n# Load your documents\nsource = Document.from_csv(\"source_texts.csv\")\ntarget = Document.from_csv(\"target_texts.csv\")\n\n# Create retrieval pipeline\npipeline = RetrievalPipeline()\n\n# Find similar passages\nresults = pipeline.retrieve(source, target, top_k=5)\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>Getting Started - Installation and first steps</li> <li>Examples - Working examples and tutorials</li> <li>CLI Reference - Command-line interface documentation</li> <li>API Reference - Complete API documentation</li> <li>Development - Contributing and development setup</li> </ul>"},{"location":"#authors","title":"Authors","text":"<ul> <li>Julian Schelb - University of Konstanz</li> <li>Michael Wittweiler - University of Zurich</li> </ul>"},{"location":"#citation","title":"Citation","text":"<p>If you use LociSimiles in your research, please cite our paper:</p> <pre><code>@article{schelb2026locisimiles,\n  title={Loci Similes: A Benchmark for Extracting Intertextualities in Latin Literature},\n  author={Schelb, Julian and Wittweiler, Michael and Revellio, Marie and Feichtinger, Barbara and Spitz, Andreas},\n  journal={arXiv preprint arXiv:2601.07533},\n  year={2026}\n}\n</code></pre>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License.</p>"},{"location":"cli/","title":"CLI Reference","text":"<p>LociSimiles provides a command-line interface for common workflows.</p>"},{"location":"cli/#installation","title":"Installation","text":"<p>The CLI is installed automatically with the package:</p> <pre><code>pip install locisimiles\n</code></pre>"},{"location":"cli/#commands","title":"Commands","text":""},{"location":"cli/#locisimiles-run","title":"<code>locisimiles run</code>","text":"<p>Run the intertextual detection pipeline on source and target documents.</p> <pre><code>locisimiles run SOURCE TARGET [OPTIONS]\n</code></pre>"},{"location":"cli/#arguments","title":"Arguments","text":"Argument Description <code>SOURCE</code> Path to the source CSV file <code>TARGET</code> Path to the target CSV file"},{"location":"cli/#options","title":"Options","text":"Option Default Description <code>--output</code>, <code>-o</code> <code>results.csv</code> Output file path <code>--model</code>, <code>-m</code> <code>sentence-transformers/all-MiniLM-L6-v2</code> Model name or path <code>--top-k</code>, <code>-k</code> <code>10</code> Number of candidates to retrieve <code>--threshold</code>, <code>-t</code> <code>0.5</code> Classification threshold <code>--batch-size</code>, <code>-b</code> <code>32</code> Batch size for processing"},{"location":"cli/#examples","title":"Examples","text":"<p>Basic usage:</p> <pre><code>locisimiles run source.csv target.csv\n</code></pre> <p>With custom output and model:</p> <pre><code>locisimiles run source.csv target.csv \\\n    --output results.csv \\\n    --model bert-base-multilingual-cased \\\n    --top-k 20\n</code></pre>"},{"location":"cli/#locisimiles-evaluate","title":"<code>locisimiles evaluate</code>","text":"<p>Evaluate detection results against ground truth.</p> <pre><code>locisimiles evaluate PREDICTIONS GROUND_TRUTH [OPTIONS]\n</code></pre>"},{"location":"cli/#arguments_1","title":"Arguments","text":"Argument Description <code>PREDICTIONS</code> Path to predictions CSV file <code>GROUND_TRUTH</code> Path to ground truth CSV file"},{"location":"cli/#options_1","title":"Options","text":"Option Default Description <code>--output</code>, <code>-o</code> <code>None</code> Output file for metrics (prints to stdout if not specified)"},{"location":"cli/#examples_1","title":"Examples","text":"<pre><code>locisimiles evaluate results.csv ground_truth.csv\n</code></pre> <p>Save metrics to file:</p> <pre><code>locisimiles evaluate results.csv ground_truth.csv -o metrics.json\n</code></pre>"},{"location":"cli/#input-file-formats","title":"Input File Formats","text":""},{"location":"cli/#sourcetarget-csv","title":"Source/Target CSV","text":"<p>CSV files should contain at minimum an ID column and a text column:</p> <pre><code>id,text\n1,\"Arma virumque cano Troiae qui primus ab oris\"\n2,\"Italiam fato profugus Laviniaque venit\"\n</code></pre>"},{"location":"cli/#ground-truth-csv","title":"Ground Truth CSV","text":"<p>Ground truth files should contain query-reference pairs with labels:</p> <pre><code>query_id,reference_id,label\n1,42,1\n2,15,0\n</code></pre> <p>Where <code>label</code> is <code>1</code> for true matches and <code>0</code> for non-matches.</p>"},{"location":"cli/#output-format","title":"Output Format","text":"<p>The pipeline outputs a CSV with the following columns:</p> Column Description <code>query_id</code> ID of the source text segment <code>reference_id</code> ID of the matched target segment <code>score</code> Similarity/classification score <code>above_threshold</code> Whether the score exceeds the threshold"},{"location":"cli/#environment-variables","title":"Environment Variables","text":"Variable Description <code>LOCISIMILES_CACHE_DIR</code> Directory for model caching <code>LOCISIMILES_DEVICE</code> Device for computation (<code>cpu</code>, <code>cuda</code>, <code>mps</code>)"},{"location":"development/","title":"Development Guide","text":"<p>This guide covers setting up a development environment and contributing to LociSimiles.</p>"},{"location":"development/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10 or higher</li> <li>Git</li> <li>pip package manager</li> </ul>"},{"location":"development/#setup","title":"Setup","text":""},{"location":"development/#clone-the-repository","title":"Clone the Repository","text":"<pre><code>git clone https://github.com/julianschelb/locisimiles.git\ncd locisimiles\n</code></pre>"},{"location":"development/#create-virtual-environment","title":"Create Virtual Environment","text":"<pre><code>python -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n</code></pre>"},{"location":"development/#install-development-dependencies","title":"Install Development Dependencies","text":"<pre><code>pip install -e \".[dev]\"\n</code></pre> <p>This installs the package in editable mode along with development tools:</p> <ul> <li>pytest - Testing framework</li> <li>pytest-cov - Coverage reporting</li> <li>poethepoet - Task runner</li> <li>mkdocs - Documentation</li> <li>mkdocs-material - Documentation theme</li> </ul>"},{"location":"development/#running-tests","title":"Running Tests","text":""},{"location":"development/#using-poe-recommended","title":"Using Poe (Recommended)","text":"<pre><code># Run all tests\npoe test\n\n# Run tests with coverage report\npoe test-cov\n</code></pre>"},{"location":"development/#using-pytest-directly","title":"Using Pytest Directly","text":"<pre><code># Run all tests\npytest\n\n# Run with verbose output\npytest -v\n\n# Run specific test file\npytest tests/test_document.py\n\n# Run specific test\npytest tests/test_document.py::TestDocument::test_from_csv\n\n# Run with coverage\npytest --cov=locisimiles --cov-report=term-missing\n\n# Generate HTML coverage report\npytest --cov=locisimiles --cov-report=html\n</code></pre>"},{"location":"development/#documentation","title":"Documentation","text":""},{"location":"development/#serve-locally","title":"Serve Locally","text":"<pre><code>poe docs\n</code></pre> <p>This starts a local server at <code>http://127.0.0.1:8000</code> with live reload.</p>"},{"location":"development/#build-static-site","title":"Build Static Site","text":"<pre><code>poe docs-build\n</code></pre> <p>Output is written to the <code>site/</code> directory.</p>"},{"location":"development/#deploy-to-github-pages","title":"Deploy to GitHub Pages","text":"<pre><code>poe docs-deploy\n</code></pre> <p>This builds and pushes to the <code>gh-pages</code> branch.</p>"},{"location":"development/#project-structure","title":"Project Structure","text":"<pre><code>locisimiles/\n\u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 locisimiles/\n\u2502       \u251c\u2500\u2500 __init__.py          # Package exports\n\u2502       \u251c\u2500\u2500 cli.py               # Command-line interface\n\u2502       \u251c\u2500\u2500 document.py          # Document and TextSegment classes\n\u2502       \u251c\u2500\u2500 evaluator.py         # Evaluation metrics\n\u2502       \u2514\u2500\u2500 pipeline/\n\u2502           \u251c\u2500\u2500 __init__.py      # Pipeline exports\n\u2502           \u251c\u2500\u2500 _types.py        # Type definitions\n\u2502           \u251c\u2500\u2500 classification.py # Classification pipeline\n\u2502           \u251c\u2500\u2500 retrieval.py     # Retrieval pipeline\n\u2502           \u2514\u2500\u2500 two_stage.py     # Combined pipeline\n\u251c\u2500\u2500 tests/\n\u2502   \u251c\u2500\u2500 conftest.py              # Shared test fixtures\n\u2502   \u251c\u2500\u2500 test_document.py         # Document tests\n\u2502   \u251c\u2500\u2500 test_evaluator.py        # Evaluator tests\n\u2502   \u251c\u2500\u2500 test_cli.py              # CLI tests\n\u2502   \u251c\u2500\u2500 test_types.py            # Type definition tests\n\u2502   \u251c\u2500\u2500 test_retrieval.py        # Retrieval pipeline tests\n\u2502   \u251c\u2500\u2500 test_classification.py   # Classification pipeline tests\n\u2502   \u2514\u2500\u2500 test_two_stage.py        # Two-stage pipeline tests\n\u251c\u2500\u2500 docs/                        # Documentation source\n\u251c\u2500\u2500 examples/                    # Example scripts and notebooks\n\u251c\u2500\u2500 pyproject.toml               # Project configuration\n\u2514\u2500\u2500 mkdocs.yml                   # Documentation configuration\n</code></pre>"},{"location":"development/#code-style","title":"Code Style","text":""},{"location":"development/#general-guidelines","title":"General Guidelines","text":"<ul> <li>Follow PEP 8 style guidelines</li> <li>Use type hints for function signatures</li> <li>Write docstrings for public APIs</li> <li>Keep functions focused and small</li> </ul>"},{"location":"development/#example","title":"Example","text":"<pre><code>def compute_similarity(\n    text_a: str,\n    text_b: str,\n    model: Optional[str] = None\n) -&gt; float:\n    \"\"\"Compute semantic similarity between two texts.\n\n    Args:\n        text_a: First text string.\n        text_b: Second text string.\n        model: Optional model name. Defaults to MiniLM.\n\n    Returns:\n        Similarity score between 0 and 1.\n\n    Raises:\n        ValueError: If either text is empty.\n    \"\"\"\n    if not text_a or not text_b:\n        raise ValueError(\"Texts cannot be empty\")\n    ...\n</code></pre>"},{"location":"development/#testing-guidelines","title":"Testing Guidelines","text":""},{"location":"development/#writing-tests","title":"Writing Tests","text":"<ul> <li>Place tests in the <code>tests/</code> directory</li> <li>Name test files <code>test_&lt;module&gt;.py</code></li> <li>Name test classes <code>Test&lt;ClassName&gt;</code></li> <li>Name test methods <code>test_&lt;behavior&gt;</code></li> </ul>"},{"location":"development/#using-fixtures","title":"Using Fixtures","text":"<p>Shared fixtures are defined in <code>conftest.py</code>:</p> <pre><code>def test_document_loading(sample_csv_file):\n    \"\"\"Test loading document from CSV.\"\"\"\n    doc = Document.from_csv(sample_csv_file)\n    assert len(doc) &gt; 0\n</code></pre>"},{"location":"development/#mocking-external-dependencies","title":"Mocking External Dependencies","text":"<p>Use unittest.mock for ML models:</p> <pre><code>from unittest.mock import MagicMock, patch\n\ndef test_retrieval_with_mock(mock_embedder):\n    \"\"\"Test retrieval with mocked embedding model.\"\"\"\n    pipeline = RetrievalPipeline()\n    pipeline.model = mock_embedder\n    # Test without actual model loading\n</code></pre>"},{"location":"development/#contributing","title":"Contributing","text":""},{"location":"development/#workflow","title":"Workflow","text":"<ol> <li>Fork the repository</li> <li>Create a feature branch: <code>git checkout -b feature/my-feature</code></li> <li>Make changes and add tests</li> <li>Ensure tests pass: <code>poe test</code></li> <li>Commit changes: <code>git commit -m \"Add my feature\"</code></li> <li>Push to fork: <code>git push origin feature/my-feature</code></li> <li>Open a Pull Request</li> </ol>"},{"location":"development/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<ul> <li>Include tests for new functionality</li> <li>Update documentation as needed</li> <li>Keep commits focused and atomic</li> <li>Write clear commit messages</li> </ul>"},{"location":"development/#available-poe-tasks","title":"Available Poe Tasks","text":"Task Command Description <code>test</code> <code>poe test</code> Run all tests <code>test-cov</code> <code>poe test-cov</code> Run tests with coverage <code>docs</code> <code>poe docs</code> Serve documentation locally <code>docs-build</code> <code>poe docs-build</code> Build documentation <code>docs-deploy</code> <code>poe docs-deploy</code> Deploy to GitHub Pages"},{"location":"examples/","title":"Examples","text":"<p>This section provides working examples demonstrating LociSimiles usage.</p>"},{"location":"examples/#sample-data","title":"Sample Data","text":"<p>The examples use sample Latin texts:</p> <ul> <li>Hieronymus samples - Query texts from Jerome's writings</li> <li>Vergil samples - Source texts from Virgil's works</li> <li>Ground truth - Annotated intertextual links for evaluation</li> </ul>"},{"location":"examples/#quick-start-example","title":"Quick Start Example","text":"<pre><code>from locisimiles.evaluator import IntertextEvaluator\nfrom locisimiles.pipeline import (\n    ClassificationPipeline,\n    ClassificationPipelineWithCandidategeneration,\n    pretty_print,\n)\nfrom locisimiles.document import Document\n\n# Load example query and source documents\nquery_doc = Document(\"./hieronymus_samples.csv\", author=\"Hieronymus\")\nsource_doc = Document(\"./vergil_samples.csv\", author=\"Vergil\")\n\nprint(\"Loaded query and source documents:\")\nprint(f\"Query Document: {query_doc}\")\nprint(f\"Source Document: {source_doc}\")\nprint(\"=\" * 70)\n\n\n# Load the pipeline with pre-trained models\npipeline_two_stage = ClassificationPipelineWithCandidategeneration(\n    classification_name=\"julian-schelb/PhilBerta-class-latin-intertext-v1\",\n    embedding_model_name=\"julian-schelb/SPhilBerta-emb-lat-intertext-v1\",\n    device=\"mps\",\n)\n\n# Run the pipeline with the query and source documents\nresults_two_stage = pipeline_two_stage.run(\n    query=query_doc,    # Query document\n    source=source_doc,  # Source document\n    top_k=10            # Number of top similar candidates to classify\n)\nprint(\"\\nResults of the two-stage pipeline run:\")\npretty_print(results_two_stage)\n\nevaluator = IntertextEvaluator(\n    query_doc=query_doc,\n    source_doc=source_doc,\n    ground_truth_csv=\"./ground_truth.csv\",\n    pipeline=pipeline_two_stage,\n    top_k=10,\n    threshold=0.5,\n)\n\nprint(\"\\nSingle sentence:\\n\", evaluator.evaluate_single_query(\"hier. adv. iovin. 1.41\"))\nprint(\"\\nPer-sentence head:\\n\", evaluator.evaluate_all_queries().head(20))\nprint(\"\\nMacro scores:\\n\", evaluator.evaluate(average=\"macro\", with_match_only=True))\nprint(\"\\nMicro scores:\\n\", evaluator.evaluate(average=\"micro\", with_match_only=True))\n</code></pre>"},{"location":"examples/#jupyter-notebook","title":"Jupyter Notebook","text":"<p>For an interactive walkthrough, see the example notebook.</p> <p>The notebook covers:</p> <ol> <li>Loading Documents - Creating Document objects from CSV files</li> <li>Two-Stage Pipeline - Using retrieval + classification</li> <li>Finding Optimal Threshold - Automatic threshold tuning</li> <li>Evaluating Different K Values - Comparing top-k settings</li> <li>Classification-Only Pipeline - Exhaustive pairwise comparison</li> </ol>"},{"location":"examples/#two-stage-pipeline","title":"Two-Stage Pipeline","text":"<p>The recommended approach combines fast retrieval with accurate classification:</p> <pre><code>from locisimiles.pipeline import ClassificationPipelineWithCandidategeneration\nfrom locisimiles.document import Document\n\n# Load documents\nquery_doc = Document(\"./hieronymus_samples.csv\", author=\"Hieronymus\")\nsource_doc = Document(\"./vergil_samples.csv\", author=\"Vergil\")\n\n# Initialize pipeline with pre-trained models\npipeline = ClassificationPipelineWithCandidategeneration(\n    classification_name=\"julian-schelb/PhilBerta-class-latin-intertext-v1\",\n    embedding_model_name=\"julian-schelb/SPhilBerta-emb-lat-intertext-v1\",\n    device=\"cpu\",  # or \"cuda\", \"mps\"\n)\n\n# Run the pipeline\nresults = pipeline.run(\n    query=query_doc,\n    source=source_doc,\n    top_k=10  # Number of candidates per query\n)\n</code></pre>"},{"location":"examples/#evaluation","title":"Evaluation","text":"<p>Evaluate your results against ground truth annotations:</p> <pre><code>from locisimiles.evaluator import IntertextEvaluator\n\nevaluator = IntertextEvaluator(\n    query_doc=query_doc,\n    source_doc=source_doc,\n    ground_truth_csv=\"./ground_truth.csv\",\n    pipeline=pipeline,\n    top_k=10,\n    threshold=0.5,\n)\n\n# Evaluate a single query\nprint(evaluator.evaluate_single_query(\"hier. adv. iovin. 1.41\"))\n\n# Get metrics for all queries\nprint(evaluator.evaluate(average=\"macro\"))\nprint(evaluator.evaluate(average=\"micro\"))\n</code></pre>"},{"location":"examples/#finding-the-best-threshold","title":"Finding the Best Threshold","text":"<p>Automatically find the optimal probability threshold:</p> <pre><code>best_result, all_thresholds_df = evaluator.find_best_threshold(\n    metric=\"f1\",       # Optimize for F1 (or 'precision', 'recall', 'smr')\n    average=\"micro\",   # Use micro-averaging\n)\n\nprint(f\"Best threshold: {best_result['best_threshold']}\")\nprint(f\"Best F1 score: {best_result['best_f1']:.4f}\")\n</code></pre>"},{"location":"examples/#classification-only-pipeline","title":"Classification-Only Pipeline","text":"<p>For smaller datasets, use exhaustive pairwise comparison:</p> <pre><code>from locisimiles.pipeline import ClassificationPipeline\n\npipeline_clf = ClassificationPipeline(\n    classification_name=\"julian-schelb/PhilBerta-class-latin-intertext-v1\",\n    device=\"cpu\",\n)\n\nresults = pipeline_clf.run(\n    query=query_doc,\n    source=source_doc,\n    batch_size=32,\n)\n\n# Filter high-probability matches\nthreshold = 0.7\nfor query_id, pairs in results.items():\n    high_prob = [(seg, prob) for seg, sim, prob in pairs if prob &gt; threshold]\n    if high_prob:\n        print(f\"Query {query_id}:\")\n        for seg, prob in high_prob:\n            print(f\"  {seg.id}: P={prob:.3f}\")\n</code></pre>"},{"location":"examples/#running-the-examples","title":"Running the Examples","text":"<p>To run the examples locally:</p> <pre><code>cd examples\npip install -r requirements.txt\npython example.py\n</code></pre> <p>Or open <code>example.ipynb</code> in Jupyter for the interactive version.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This guide will help you get started with LociSimiles for finding intertextual links in Latin literature.</p>"},{"location":"getting-started/#installation","title":"Installation","text":""},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10 or higher</li> <li>pip package manager</li> </ul>"},{"location":"getting-started/#installing-from-pypi","title":"Installing from PyPI","text":"<pre><code>pip install locisimiles\n</code></pre>"},{"location":"getting-started/#installing-from-source","title":"Installing from Source","text":"<pre><code>git clone https://github.com/julianschelb/locisimiles.git\ncd locisimiles\npip install -e \".[dev]\"\n</code></pre>"},{"location":"getting-started/#basic-concepts","title":"Basic Concepts","text":""},{"location":"getting-started/#documents-and-segments","title":"Documents and Segments","text":"<p>LociSimiles works with Documents containing TextSegments. Each segment represents a unit of text (e.g., a verse, sentence, or passage).</p> <pre><code>from locisimiles import Document, TextSegment\n\n# Create segments manually\nsegments = [\n    TextSegment(id=\"1\", text=\"Arma virumque cano\"),\n    TextSegment(id=\"2\", text=\"Troiae qui primus ab oris\"),\n]\n\n# Create a document\ndoc = Document(segments=segments)\n</code></pre>"},{"location":"getting-started/#loading-from-csv","title":"Loading from CSV","text":"<p>Documents are typically loaded from CSV files:</p> <pre><code>doc = Document.from_csv(\"texts.csv\")\n</code></pre> <p>The CSV should have columns for <code>id</code> and <code>text</code> (column names are configurable).</p>"},{"location":"getting-started/#pipelines","title":"Pipelines","text":"<p>LociSimiles provides three main pipeline types:</p>"},{"location":"getting-started/#1-retrieval-pipeline","title":"1. Retrieval Pipeline","text":"<p>Uses semantic embeddings to find similar passages:</p> <pre><code>from locisimiles import RetrievalPipeline\n\npipeline = RetrievalPipeline(\n    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n)\n\nresults = pipeline.retrieve(source_doc, target_doc, top_k=10)\n</code></pre>"},{"location":"getting-started/#2-classification-pipeline","title":"2. Classification Pipeline","text":"<p>Uses a transformer model to classify text pairs:</p> <pre><code>from locisimiles import ClassificationPipeline\n\npipeline = ClassificationPipeline(\n    model_name=\"bert-base-uncased\"\n)\n\nresults = pipeline.classify(pairs)\n</code></pre>"},{"location":"getting-started/#3-two-stage-pipeline","title":"3. Two-Stage Pipeline","text":"<p>Combines retrieval and classification for best results:</p> <pre><code>from locisimiles import TwoStagePipeline\n\npipeline = TwoStagePipeline(\n    retrieval_model=\"sentence-transformers/all-MiniLM-L6-v2\",\n    classification_model=\"bert-base-uncased\"\n)\n\nresults = pipeline.run(source_doc, target_doc)\n</code></pre>"},{"location":"getting-started/#evaluation","title":"Evaluation","text":"<p>Use the <code>IntertextEvaluator</code> to assess detection quality:</p> <pre><code>from locisimiles import IntertextEvaluator\n\nevaluator = IntertextEvaluator(\n    predictions=predictions,\n    ground_truth=ground_truth\n)\n\nmetrics = evaluator.evaluate()\nprint(f\"Precision: {metrics['precision']:.3f}\")\nprint(f\"Recall: {metrics['recall']:.3f}\")\nprint(f\"F1: {metrics['f1']:.3f}\")\n</code></pre>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<ul> <li>See the CLI Reference for command-line usage</li> <li>Explore the API Reference for detailed documentation</li> <li>Check out the examples for complete workflows</li> </ul>"},{"location":"api/","title":"API Reference","text":"<p>This section provides detailed documentation for the LociSimiles Python API, auto-generated from source code docstrings.</p>"},{"location":"api/#core-modules","title":"Core Modules","text":""},{"location":"api/#document-module","title":"Document Module","text":"<p>The Document module provides classes for representing and loading text collections:</p> <ul> <li><code>TextSegment</code> - Individual text unit with ID and content</li> <li><code>Document</code> - Container for text segments</li> </ul>"},{"location":"api/#pipeline-module","title":"Pipeline Module","text":"<p>The Pipelines module provides the main processing pipelines:</p> <ul> <li><code>RetrievalPipeline</code> - Semantic similarity retrieval</li> <li><code>ClassificationPipeline</code> - Text pair classification</li> <li><code>ClassificationPipelineWithCandidategeneration</code> - Two-stage retrieval + classification</li> </ul>"},{"location":"api/#evaluator-module","title":"Evaluator Module","text":"<p>The Evaluator module provides tools for assessing detection quality:</p> <ul> <li><code>IntertextEvaluator</code> - Main evaluation class</li> </ul>"},{"location":"api/#quick-reference","title":"Quick Reference","text":""},{"location":"api/#loading-documents","title":"Loading Documents","text":"<pre><code>from locisimiles import Document\n\n# From CSV file\ndoc = Document.from_csv(\"texts.csv\")\n\n# From list of d# From list of d# From list of dre#ords([\n    {\"id\"    {\"id\"    {\"id\"    {\"id\"    {\"id\"    {\"id\"    {\"id\"    {\"id\"    {\"id\"    {\"id\"    {\"id\"    {\"id\"    {\"id\"    {\"id\"  om     {\"id\"    mpor    {\"id\"    {\"id\"pel    {\"id\"    {\"id\"    {\"id\"    {\"id\"    {\"id\"    {\"id\"    {\"id\"    {\"id\"    {\"id\"    {\"id\"    {\"ipip    {\"id\"    {\"id\"    {\"id\"    {\"id\"    {\"id\" 0.5)\n</code></pre>"},{"location":"api/#evaluating-results","title":"Evaluating Results","text":"<pre><code>from locisimiles import IntertextEvaluator\n\nevaluator = IntertextEvaluator(predictions, ground_truth)\nmetrics = evaluator.evaluate()\n</code></pre>"},{"location":"api/document/","title":"Document Module","text":"<p>Classes for representing and loading text collections.</p>"},{"location":"api/document/#textsegment","title":"TextSegment","text":"<p>An individual unit of text with an identifier.</p>"},{"location":"api/document/#locisimiles.document.TextSegment","title":"locisimiles.document.TextSegment","text":"<pre><code>TextSegment(\n    text: str,\n    seg_id: ID,\n    *,\n    row_id: int | None = None,\n    meta: Dict[str, Any] | None = None,\n)\n</code></pre> <p>Atomic unit of text inside a document.</p>"},{"location":"api/document/#document","title":"Document","text":"<p>A collection of text segments with loading utilities.</p>"},{"location":"api/document/#locisimiles.document.Document","title":"locisimiles.document.Document","text":"<pre><code>Document(\n    path: str | Path,\n    *,\n    author: str | None = None,\n    meta: Dict[str, Any] | None = None,\n    segment_delimiter: str = \"\\n\",\n)\n</code></pre> <p>Collection of text segments, representing a document.</p>"},{"location":"api/document/#locisimiles.document.Document.ids","title":"ids","text":"<pre><code>ids() -&gt; List[ID]\n</code></pre> <p>Return segment IDs in original order.</p>"},{"location":"api/document/#locisimiles.document.Document.get_text","title":"get_text","text":"<pre><code>get_text(seg_id: ID) -&gt; str\n</code></pre> <p>Return raw text of a segment.</p>"},{"location":"api/document/#locisimiles.document.Document.add_segment","title":"add_segment","text":"<pre><code>add_segment(\n    text: str,\n    seg_id: ID,\n    *,\n    row_id: int | None = None,\n    meta: Dict[str, Any] | None = None,\n) -&gt; None\n</code></pre> <p>Add a new text segment to the document.</p>"},{"location":"api/document/#locisimiles.document.Document.remove_segment","title":"remove_segment","text":"<pre><code>remove_segment(seg_id: ID) -&gt; None\n</code></pre> <p>Delete a segment if present.</p>"},{"location":"api/evaluator/","title":"Evaluator Module","text":"<p>Tools for assessing detection quality.</p>"},{"location":"api/evaluator/#intertextevaluator","title":"IntertextEvaluator","text":"<p>Evaluate detection results against ground truth annotations.</p> <p>Computes precision, recall, F1, and other metrics for intertextual link detection.</p>"},{"location":"api/evaluator/#locisimiles.evaluator.IntertextEvaluator","title":"locisimiles.evaluator.IntertextEvaluator","text":"<pre><code>IntertextEvaluator(\n    *,\n    query_doc: Document,\n    source_doc: Document,\n    ground_truth_csv: str | DataFrame,\n    pipeline: ClassificationPipelineWithCandidategeneration,\n    top_k: int = 5,\n    threshold: float | str = \"auto\",\n    auto_threshold_metric: str = \"smr\",\n)\n</code></pre> <p>Compute sentence- and document-level scores for intertextual link prediction.</p>"},{"location":"api/evaluator/#locisimiles.evaluator.IntertextEvaluator.evaluate_single_query","title":"evaluate_single_query","text":"<pre><code>evaluate_single_query(query_id: str) -&gt; Dict[str, float]\n</code></pre> <p>Compute metrics for one query sentence.</p>"},{"location":"api/evaluator/#locisimiles.evaluator.IntertextEvaluator.query_ids_with_match","title":"query_ids_with_match","text":"<pre><code>query_ids_with_match() -&gt; List[str]\n</code></pre> <p>Return query IDs that have ground truth labels.</p>"},{"location":"api/evaluator/#locisimiles.evaluator.IntertextEvaluator.evaluate_all_queries","title":"evaluate_all_queries","text":"<pre><code>evaluate_all_queries(\n    with_match_only: bool = False,\n) -&gt; DataFrame\n</code></pre> <p>Compute metrics for every query sentence (cached).</p>"},{"location":"api/evaluator/#locisimiles.evaluator.IntertextEvaluator.evaluate","title":"evaluate","text":"<pre><code>evaluate(\n    *, average: str = \"macro\", with_match_only: bool = False\n) -&gt; Dict[str, float]\n</code></pre> <p>Compute aggregated metrics across queries.</p> <ul> <li>Precision, Recall, F1, Accuracy: ALWAYS computed on queries with at least    one ground truth match (otherwise these metrics are meaningless).</li> <li>FPR, FNR, SMR: Computed on ALL queries by default (measures false alarms    on queries that shouldn't have matches). If with_match_only=True, these    are also restricted to queries with matches.</li> </ul>"},{"location":"api/evaluator/#locisimiles.evaluator.IntertextEvaluator.confusion_matrix","title":"confusion_matrix","text":"<pre><code>confusion_matrix(query_id: str) -&gt; ndarray\n</code></pre> <p>Return 2x2 confusion matrix [[TP,FP],[FN,TN]] for one query sentence.</p>"},{"location":"api/evaluator/#locisimiles.evaluator.IntertextEvaluator.find_best_threshold","title":"find_best_threshold","text":"<pre><code>find_best_threshold(\n    *,\n    metric: str = \"f1\",\n    thresholds: List[float] | None = None,\n    average: str = \"micro\",\n    with_match_only: bool = False,\n) -&gt; Tuple[Dict[str, float], DataFrame]\n</code></pre> <p>Find the optimal probability threshold based on the given metric.</p>"},{"location":"api/evaluator/#locisimiles.evaluator.IntertextEvaluator.evaluate_k_values","title":"evaluate_k_values","text":"<pre><code>evaluate_k_values(\n    *,\n    k_values: List[int] | None = None,\n    average: str = \"micro\",\n    with_match_only: bool = False,\n) -&gt; Dict[int, Dict[str, float]]\n</code></pre> <p>Evaluate metrics for different top_k values WITHOUT re-running the pipeline.</p>"},{"location":"api/pipelines/","title":"Pipelines Module","text":"<p>Processing pipelines for intertextual detection.</p>"},{"location":"api/pipelines/#retrievalpipeline","title":"RetrievalPipeline","text":"<p>Find similar passages using semantic embeddings.</p>"},{"location":"api/pipelines/#locisimiles.pipeline.retrieval.RetrievalPipeline","title":"locisimiles.pipeline.retrieval.RetrievalPipeline","text":"<pre><code>RetrievalPipeline(\n    *,\n    embedding_model_name: str = \"julian-schelb/SPhilBerta-emb-lat-intertext-v1\",\n    device: str | int | None = None,\n)\n</code></pre> <p>A retrieval-only pipeline for intertextuality detection.</p> <p>Uses embedding similarity to find candidate segments without a classification stage. Binary decisions are made based on rank (top-k) or similarity threshold.</p> <p>To maintain compatibility with the evaluator, results are returned in FullDict format where the \"probability\" field is set to 1.0 for positive predictions and 0.0 for negative predictions based on the decision criteria.</p>"},{"location":"api/pipelines/#locisimiles.pipeline.retrieval.RetrievalPipeline.build_source_index","title":"build_source_index","text":"<pre><code>build_source_index(\n    source_segments: Sequence[TextSegment],\n    source_embeddings: ndarray,\n    collection_name: str = \"source_segments\",\n    batch_size: int = 5000,\n)\n</code></pre> <p>Create a Chroma collection from source_segments and their embeddings.</p>"},{"location":"api/pipelines/#locisimiles.pipeline.retrieval.RetrievalPipeline.retrieve","title":"retrieve","text":"<pre><code>retrieve(\n    *,\n    query: Document,\n    source: Document,\n    top_k: int = 100,\n    query_prompt_name: str = \"query\",\n    source_prompt_name: str = \"match\",\n    **kwargs: Any,\n) -&gt; SimDict\n</code></pre> <p>Retrieve candidate segments from source based on similarity to query.</p> <p>Returns a dictionary mapping query segment IDs to lists of (source segment, similarity score) pairs, sorted by similarity (descending).</p>"},{"location":"api/pipelines/#locisimiles.pipeline.retrieval.RetrievalPipeline.run","title":"run","text":"<pre><code>run(\n    *,\n    query: Document,\n    source: Document,\n    top_k: int = 10,\n    similarity_threshold: float | None = None,\n    query_prompt_name: str = \"query\",\n    source_prompt_name: str = \"match\",\n    **kwargs: Any,\n) -&gt; FullDict\n</code></pre> <p>Run the retrieval pipeline and return results compatible with the evaluator.</p> <p>Binary decisions are made using one of two criteria: - top_k (default): The top-k ranked candidates per query are predicted    as positive (prob=1.0), all others as negative (prob=0.0). - similarity_threshold: If provided, candidates with similarity &gt;= threshold   are predicted as positive, regardless of rank.</p> PARAMETER DESCRIPTION <code>query</code> <p>Query document</p> <p> TYPE: <code>Document</code> </p> <code>source</code> <p>Source document  </p> <p> TYPE: <code>Document</code> </p> <code>top_k</code> <p>Number of top candidates to mark as positive (default: 10).    Used when similarity_threshold is None.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>similarity_threshold</code> <p>If provided, use this similarity cutoff instead    of top_k. Candidates with similarity &gt;= threshold are positive.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>query_prompt_name</code> <p>Prompt name for query embeddings</p> <p> TYPE: <code>str</code> DEFAULT: <code>'query'</code> </p> <code>source_prompt_name</code> <p>Prompt name for source embeddings</p> <p> TYPE: <code>str</code> DEFAULT: <code>'match'</code> </p> RETURNS DESCRIPTION <code>FullDict</code> <p>FullDict mapping query IDs to lists of (segment, similarity, probability)</p> <code>FullDict</code> <p>tuples. Probability is 1.0 for positive predictions, 0.0 for negative.</p>"},{"location":"api/pipelines/#classificationpipeline","title":"ClassificationPipeline","text":"<p>Classify text pairs using transformer models (exhaustive comparison).</p>"},{"location":"api/pipelines/#locisimiles.pipeline.classification.ClassificationPipeline","title":"locisimiles.pipeline.classification.ClassificationPipeline","text":"<pre><code>ClassificationPipeline(\n    *,\n    classification_name: str = \"julian-schelb/PhilBerta-class-latin-intertext-v1\",\n    device: str | int | None = None,\n    pos_class_idx: int = 1,\n)\n</code></pre> <p>A simpler pipeline for intertextuality classification without candidate generation. It classifies all possible pairs between query and source segments without a retrieval stage. Suitable for smaller document pairs or when exhaustive comparison is needed.</p>"},{"location":"api/pipelines/#locisimiles.pipeline.classification.ClassificationPipeline.debug_input_sequence","title":"debug_input_sequence","text":"<pre><code>debug_input_sequence(\n    query_text: str, candidate_text: str, max_len: int = 512\n) -&gt; Dict[str, Any]\n</code></pre> <p>Debug method to inspect how a query-candidate pair is encoded.</p>"},{"location":"api/pipelines/#locisimiles.pipeline.classification.ClassificationPipeline.run","title":"run","text":"<pre><code>run(\n    *,\n    query: Document,\n    source: Document,\n    batch_size: int = 32,\n    **kwargs: Any,\n) -&gt; FullDict\n</code></pre> <p>Run classification on all query-source segment pairs. Returns a dictionary mapping query segment IDs to lists of (source segment, similarity_score=None, P(positive)) tuples.</p> <p>Note: Since there's no retrieval stage, similarity_score is set to None.</p>"},{"location":"api/pipelines/#classificationpipelinewithcandidategeneration","title":"ClassificationPipelineWithCandidategeneration","text":"<p>Two-stage pipeline: retrieval for candidate generation, then classification.</p>"},{"location":"api/pipelines/#locisimiles.pipeline.two_stage.ClassificationPipelineWithCandidategeneration","title":"locisimiles.pipeline.two_stage.ClassificationPipelineWithCandidategeneration","text":"<pre><code>ClassificationPipelineWithCandidategeneration(\n    *,\n    classification_name: str = \"julian-schelb/PhilBerta-class-latin-intertext-v1\",\n    embedding_model_name: str = \"julian-schelb/SPhilBerta-emb-lat-intertext-v1\",\n    device: str | int | None = None,\n    pos_class_idx: int = 1,\n)\n</code></pre> <p>A pipeline for intertextuality classification with candidate generation. It first generates candidate segments from a source document based on similarity to a query segment, and then classifies these candidates as intertextual or not using a pre-trained model.</p>"},{"location":"api/pipelines/#locisimiles.pipeline.two_stage.ClassificationPipelineWithCandidategeneration.debug_input_sequence","title":"debug_input_sequence","text":"<pre><code>debug_input_sequence(\n    query_text: str, candidate_text: str, max_len: int = 512\n) -&gt; Dict[str, Any]\n</code></pre> <p>Debug method to inspect how a query-candidate pair is encoded.</p> <p>Returns a dictionary with: - query: Original query text - candidate: Original candidate text - query_truncated: Truncated query text - candidate_truncated: Truncated candidate text - input_ids: Token IDs as list - input_text: Decoded text with special tokens visible - attention_mask: Attention mask as list</p>"},{"location":"api/pipelines/#locisimiles.pipeline.two_stage.ClassificationPipelineWithCandidategeneration.build_source_index","title":"build_source_index","text":"<pre><code>build_source_index(\n    source_segments: Sequence[TextSegment],\n    source_embeddings: ndarray,\n    collection_name: str = \"source_segments\",\n    batch_size: int = 5000,\n)\n</code></pre> <p>Create a Chroma collection from source_segments and their embeddings.</p>"},{"location":"api/pipelines/#locisimiles.pipeline.two_stage.ClassificationPipelineWithCandidategeneration.generate_candidates","title":"generate_candidates","text":"<pre><code>generate_candidates(\n    *,\n    query: Document,\n    source: Document,\n    top_k: int = 5,\n    query_prompt_name: str = \"query\",\n    source_prompt_name: str = \"match\",\n    **kwargs: Any,\n) -&gt; SimDict\n</code></pre> <p>Generate candidate segments from source based on similarity to query. Returns a dictionary mapping query segment IDs to lists of (source segment, similarity score) pairs.</p>"},{"location":"api/pipelines/#locisimiles.pipeline.two_stage.ClassificationPipelineWithCandidategeneration.check_candidates","title":"check_candidates","text":"<pre><code>check_candidates(\n    *,\n    query: Document,\n    source: Document,\n    candidates: SimDict | None = None,\n    batch_size: int = 32,\n    **kwargs: Any,\n) -&gt; FullDict\n</code></pre> <p>Classify candidates generated from source. If candidates is not provided, it will be generated using generate_candidates. Returns a dictionary mapping query segment IDs to lists of (source segment, similarity score, P(positive)) tuples.</p>"},{"location":"api/pipelines/#locisimiles.pipeline.two_stage.ClassificationPipelineWithCandidategeneration.run","title":"run","text":"<pre><code>run(\n    *,\n    query: Document,\n    source: Document,\n    top_k: int = 5,\n    query_prompt_name: str = \"query\",\n    source_prompt_name: str = \"match\",\n    **kwargs: Any,\n) -&gt; FullDict\n</code></pre> <p>Run the full pipeline: generate candidates and classify them. Returns a dictionary mapping query segment IDs to lists of (source segment, similarity score, P(positive)) tuples.</p>"},{"location":"api/pipelines/#type-definitions","title":"Type Definitions","text":"<p>Data classes for pipeline results.</p>"},{"location":"api/pipelines/#locisimiles.pipeline._types","title":"locisimiles.pipeline._types","text":"<p>Shared type definitions and utilities for pipeline modules.</p>"},{"location":"api/pipelines/#locisimiles.pipeline._types.pretty_print","title":"pretty_print","text":"<pre><code>pretty_print(results: FullDict) -&gt; None\n</code></pre> <p>Human-friendly dump of run() output.</p>"}]}